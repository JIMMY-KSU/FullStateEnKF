{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 794,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6778.1363\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import sympy as sym\n",
    "from scipy.integrate import ode\n",
    "from scipy.io import loadmat\n",
    "import scipy.io\n",
    "import scipy\n",
    "from scipy import stats\n",
    "import pickle\n",
    "import copy\n",
    "import filter_functions\n",
    "from sympy.utilities.lambdify import lambdify\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.debugger import Tracer\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12.0, 4.0)\n",
    "plt.rcParams['xtick.labelsize'] = 14\n",
    "plt.rcParams['ytick.labelsize'] = 14\n",
    "np.set_printoptions(precision=15)\n",
    "sym.init_printing()\n",
    "from IPython.display import display\n",
    "\n",
    "#MSIS: https://github.com/DeepHorizons/Python-NRLMSISE-00\n",
    "#import time\n",
    "from nrlmsise_00_header import *\n",
    "from nrlmsise_00 import *\n",
    "#SUBROUTINE GTD7D -- d[5] is the \"effective total mass density\n",
    "#for drag\" and is the sum of the mass densities of all species\n",
    "#in this model, INCLUDING anomalous oxygen.\n",
    "\n",
    "#define constants\n",
    "r_earth_const = 6378136.3 * 1e-3 #km\n",
    "omega_const = 7.2921158553e-5 #rad/s, angular velocity of earth\n",
    "J_2_const = .00108262617385222\n",
    "J_3_const = -.00000253241051856772\n",
    "mu_earth = 3.986004415e14 * 1e-9 #km^3/s^2\n",
    "\n",
    "\n",
    "#Drag:\n",
    "A_const = 0.9551567 * 1e-6 #km^2; cross-sectional area of satellite\n",
    "m_const = 10 #kg; mass of satellite\n",
    "C_D_const = 2.0\n",
    "theta_dot_const = 7.2921158553e-5 #rad/sec\n",
    "print(400 + r_earth_const)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 795,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86, 6)\n",
      "(86, 7)\n",
      "114 20520.0\n",
      "20700.0\n",
      "Initial measurement index:  115.0\n",
      "Initial simulation index:  114\n",
      "399.999999942\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stop_time = 36180 \n",
    "\n",
    "num_of_objects = 5\n",
    "\n",
    "meas_type = 3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if meas_type == 1:\n",
    "    meas_file = open('Data Files/meas_range_rangeRate.pkl', 'rb')\n",
    "elif meas_type == 2:\n",
    "    meas_file = open('Data Files/meas_az_el.pkl', 'rb')\n",
    "elif meas_type == 3:\n",
    "    meas_file = open('Data Files/meas_az_el_range_MultiObj_geoMag.pkl', 'rb') #_10s_all_3stat.pkl\n",
    "    \n",
    "    \n",
    "    \n",
    "#Date of Simulation Details:\n",
    "#June 24th, 2017 at 6am (this is the date & time at the beginning of the simulation/orbit)\n",
    "year_init = 2017\n",
    "month_init = 6\n",
    "day_of_month_init = 24\n",
    "day_of_year_init = 175\n",
    "hour_init = 6\n",
    "boulder_UT_offset = 6 #Boulder time + 6 hours = UT time\n",
    "hour_init_UT = hour_init + boulder_UT_offset\n",
    "    \n",
    "    \n",
    "\n",
    "#Canbera Station (DSS 34)\n",
    "lat_dss34 = math.radians(-35.398333)\n",
    "lon_dss34 = math.radians(148.981944)\n",
    "alt_dss34 = 691.75 * 1e-3 #km\n",
    "\n",
    "r_ecef_dss34 = filter_functions.topo2ecef(lat_dss34, lon_dss34, alt_dss34, r_earth_const)\n",
    "#print(r_ecef_dss34)\n",
    "\n",
    "#Madrid Station (DSS 65) -- correct position of Madrid Station\n",
    "lat_dss65 = math.radians(40.427222)\n",
    "lon_dss65 = math.radians(355.749444)\n",
    "alt_dss65 = 834.539 * 1e-3 #km\n",
    "\n",
    "r_ecef_dss65 = filter_functions.topo2ecef(lat_dss65, lon_dss65, alt_dss65, r_earth_const)\n",
    "#print(r_ecef_dss65)\n",
    "\n",
    "#Goldstone Station (DSS 13) \n",
    "lat_dss13 = math.radians(35.247164)\n",
    "lon_dss13 = math.radians(200.205)\n",
    "alt_dss13 = 1071.14904 * 1e-3 #km\n",
    "\n",
    "r_ecef_dss13 = filter_functions.topo2ecef(lat_dss13, lon_dss13, alt_dss13, r_earth_const)\n",
    "#print(r_ecef_dss13)\n",
    "\n",
    "#Diego Garcia, British Indian Ocean Territory 7.41173°S 72.45222°E., Space Fence (Dedicated Sensor\n",
    "lat_diego = math.radians(-7.41173)\n",
    "lon_diego = math.radians(72.45222)\n",
    "alt_diego = 0 * 1e-3 #km, \"sea level\"\n",
    "\n",
    "r_ecef_diego = filter_functions.topo2ecef(lat_diego, lon_diego, alt_diego, r_earth_const)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# read python dict containing measurements\n",
    "mydict2 = pickle.load(meas_file)\n",
    "meas_file.close()\n",
    "measurement_array = mydict2['measurement_array']\n",
    "truth_xyz = mydict2['true_state']\n",
    "true_state_all_times = mydict2['true_state_all_times'] #includes truth before & after measurements occur\n",
    "lat_lst_meas_array = mydict2['lat_lst_array']\n",
    "print(np.shape(measurement_array))\n",
    "print(np.shape(truth_xyz))\n",
    "\n",
    "\n",
    "truth_xyz[:,:6] = truth_xyz[:,:6] * 1e-3 #convert to km\n",
    "truth_xyz[:,-1] = truth_xyz[:,-1] * 1e9 #prep truth for filter used density magnitude/units\n",
    "\n",
    "true_state_all_times[:,2:8] = true_state_all_times[:,2:8] * 1e-3 #convert to km\n",
    "true_state_all_times[:,-1] = true_state_all_times[:,-1] * 1e9 #prep truth for filter used density magnitude/units\n",
    "\n",
    "measurement_array[:, -1] = measurement_array[:, -1] * 1e-3 #convert to km\n",
    "\n",
    "\n",
    "\n",
    "#calculate data time step using truth at all times\n",
    "time_step = true_state_all_times[1,0] - true_state_all_times[0,0] #seconds\n",
    "\n",
    "\n",
    "#specify initial index of filter simulation: anywhere from 1 (\"time_step - 1\") before to the first index\n",
    "#initial_sim_index = int(measurement_array[0,0]/time_step - 3) #initialize for time step before first measurement \n",
    "initial_sim_index = int(measurement_array[0,0]/time_step - 1) #initialize for time step before first measurement\n",
    "initial_sim_time = initial_sim_index * time_step\n",
    "print(initial_sim_index, initial_sim_time)\n",
    "\n",
    "\n",
    "\n",
    "print(measurement_array[0,0])\n",
    "print('Initial measurement index: ', measurement_array[0,0]/time_step)\n",
    "print('Initial simulation index: ', initial_sim_index)\n",
    "#print(measurement_array[:50,:3])\n",
    "print(np.linalg.norm(true_state_all_times[-1, 2:5]) - r_earth_const)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAC0AAAAPBAMAAACCUFuUAAAAMFBMVEX///8AAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAv3aB7AAAAD3RSTlMAVO8Qq5l2zWaJRCK7\n3TJS0mj0AAAACXBIWXMAAA7EAAAOxAGVKw4bAAAA4ElEQVQYGWNgAAHmtKkLGJgjOx+AORCaUZmB\ngZWBwZGBXYD5NkgcQpuEfGZgkGBg2M7QxMAwHSQOpdmA4nMYGGwZjjAwyBsAxaE0SHz9YYYZDF+A\ntABQHEqDxFn+dzYwfwWKJwDdAKVB4gzzvxtw/2RgeD+BgQFGg8QZve5rcQPVgcWhNEi8hYHtkzEW\nc7SBSgWA9slD7QXRQPXMH4BWN5xhYOg3ANoFpUHmANXzbAD6JxwoDPIXiAaJLzdgaGPgFWDWY7Av\ngNAMrE7fPRj4XKcCw21aygMGXgcIDdKIDQAAFqJLZ4k0aDwAAAAASUVORK5CYII=\n",
      "text/latex": [
       "$$180.0$$"
      ],
      "text/plain": [
       "180.0"
      ]
     },
     "execution_count": 796,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Vector of State/density Ensembles: (450, 37, 73)\n",
      "0.00351022831235\n",
      "0.00350241656975\n",
      "0.00470466076647\n"
     ]
    }
   ],
   "source": [
    "#read in files necessary for ensemble & density est. portion\n",
    "\n",
    "\n",
    "# read python dict containing densities\n",
    "ensemble_file = open('Data Files/ensemble_density_grids_moreRealistic.pkl', 'rb')\n",
    "mydict2 = pickle.load(ensemble_file)\n",
    "ensemble_file.close()\n",
    "\n",
    "#shape: ensembles (# of combos) by lat by lon\n",
    "ensembles_of_density_grid = mydict2['ensemble_of_density_grids'] \n",
    "print('Shape of Vector of State/density Ensembles:', np.shape(ensembles_of_density_grid))\n",
    "(num_of_ensembles, num_of_lat, num_of_lon) = np.shape(ensembles_of_density_grid)\n",
    "\n",
    "#convert from kg/m**3 to kg/km**3 -> 1/(1e-3)**3 = 1/(1e-9) = 1e9\n",
    "ensembles_of_density_grid = ensembles_of_density_grid * 1e9\n",
    "\n",
    "latitude_grid = mydict2['latitudes'] \n",
    "longitude_grid = mydict2['longitudes'] \n",
    "lat_res = latitude_grid[1] - latitude_grid[0] #spacing between each latitude \"tick\"\n",
    "lon_res = longitude_grid[1] - longitude_grid[0] #spacing between each longitude \"tick\"\n",
    "\n",
    "\n",
    "\n",
    "#add noise w/ standard deviation = 1e-4 (used as initialization of density covariance, as well)\n",
    "\n",
    "print(np.sum(ensembles_of_density_grid[:, 0, 0])/num_of_ensembles)\n",
    "ensemble_noise = np.random.randn(num_of_ensembles, num_of_lat, num_of_lon) * 1e-4 \n",
    "ensembles_of_density_grid = ensembles_of_density_grid + ensemble_noise \n",
    "print(np.sum(ensembles_of_density_grid[:, 0, 0])/num_of_ensembles) \n",
    "\n",
    "print(np.mean(ensembles_of_density_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Propogate reference trajectory and S.T.M.\n",
    "def orbitpropogator_EnKF(t, X_vector, density):\n",
    "    \n",
    "    ensemble_member = X_vector\n",
    "\n",
    "    #find X acceleration via the F(X) lambdified equation\n",
    "    state_acc = X_dot_sol_fcn(ensemble_member[0], ensemble_member[1], ensemble_member[2], \\\n",
    "                                  ensemble_member[3], ensemble_member[4], ensemble_member[5], density)\n",
    "        \n",
    "    dx = state_acc.flatten()\n",
    "    return dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 799,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#EnKF specific functionality\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def gen_ensemble(X_0, Bsqrt_cov, ensemble_size):\n",
    "    \n",
    "    X_0 = X_0.reshape(len(X_0), 1)\n",
    "    \n",
    "    ensemble = np.zeros((len(X_0), ensemble_size))\n",
    "    \n",
    "    for ii in range(ensemble_size):\n",
    "        \n",
    "        member = X_0 + np.dot(Bsqrt_cov, np.random.randn(len(X_0), 1))\n",
    "\n",
    "        ensemble[:, ii] = member.reshape(len(X_0))\n",
    "    \n",
    "    return ensemble\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 800,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#two body motion force\n",
    "# **Setup force equations/acceleration/U\n",
    "\n",
    "#Force equations with J_2\n",
    "x, y, z, J_2, r_earth, mu, r, J_3 = sym.symbols('x y z J_2 r_earth mu r J_3')\n",
    "\n",
    "'''\n",
    "two_body_J2_string = 'mu/r * ( 1 - J_2*(r_earth/r)**2 * (3/2 * (z/r)**2 - 1/2) )' #potential\n",
    "two_body_J2 = sym.sympify(two_body_J2_string)\n",
    "two_body_J2 = two_body_J2.subs([(r, sym.sqrt(x**2+y**2+z**2))])\n",
    "two_body_J2_acc_x = two_body_J2.diff(x)\n",
    "two_body_J2_acc_y = two_body_J2.diff(y)\n",
    "two_body_J2_acc_z = two_body_J2.diff(z)\n",
    "\n",
    "\n",
    "two_body_J2_acc_x = two_body_J2_acc_x.subs([(r_earth, r_earth_const), (mu, mu_earth), \\\n",
    "                              (J_2, J_2_const)])\n",
    "two_body_J2_acc_y = two_body_J2_acc_y.subs([(r_earth, r_earth_const), (mu, mu_earth), \\\n",
    "                              (J_2, J_2_const)])\n",
    "two_body_J2_acc_z = two_body_J2_acc_z.subs([(r_earth, r_earth_const), (mu, mu_earth), \\\n",
    "                              (J_2, J_2_const)])\n",
    "#print('2 body & J2', two_body_J2_acc_x)\n",
    "\n",
    "x_acc = two_body_J2_acc_x\n",
    "y_acc = two_body_J2_acc_y\n",
    "z_acc = two_body_J2_acc_z\n",
    "'''\n",
    "\n",
    "x_acc = '(-mu/r**3) * x'\n",
    "y_acc = '(-mu/r**3) * y'\n",
    "z_acc = '(-mu/r**3) * z'\n",
    "\n",
    "x_acc = sym.sympify(x_acc)\n",
    "y_acc = sym.sympify(y_acc)\n",
    "z_acc = sym.sympify(z_acc)\n",
    "\n",
    "\n",
    "x_acc = x_acc.subs([(r, sym.sqrt(x**2+y**2+z**2)), (mu, mu_earth)])\n",
    "y_acc = y_acc.subs([(r, sym.sqrt(x**2+y**2+z**2)), (mu, mu_earth)])\n",
    "z_acc = z_acc.subs([(r, sym.sqrt(x**2+y**2+z**2)), (mu, mu_earth)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 801,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Add drag to J_2 force equations\n",
    "\n",
    "x_dot, y_dot, z_dot, x_double_dot, y_double_dot, z_double_dot = \\\n",
    "    sym.symbols('x_dot, y_dot, z_dot, x_double_dot, y_double_dot, z_double_dot')\n",
    "    \n",
    "C_D, A, m, density, theta_dot, val, val_dot = \\\n",
    "    sym.symbols('C_D A m density theta_dot val, val_dot')\n",
    "\n",
    "\n",
    "\n",
    "drag_str_x = ('-(1/2)*C_D*(A/m)*density*'   \n",
    "        '(  ( 1/(sqrt((x_dot+theta_dot*y)**2 + (y_dot-theta_dot*x)**2 +z_dot**2)) ) *(x_dot+theta_dot*y))')\n",
    "\n",
    "drag_str_y = ('-(1/2)*C_D*(A/m)*density*'   \n",
    "        '(  ( 1/(sqrt((x_dot+theta_dot*y)**2 + (y_dot-theta_dot*x)**2 +z_dot**2)) ) *(y_dot-theta_dot*x))')\n",
    "\n",
    "drag_str_z = ('-(1/2)*C_D*(A/m)*density*'   \n",
    "        '(  ( 1/(sqrt((x_dot+theta_dot*y)**2 + (y_dot-theta_dot*x)**2 +z_dot**2)) ) *(z_dot))')\n",
    "\n",
    "\n",
    "drag_symp_x = sym.sympify(drag_str_x)\n",
    "drag_symp_y = sym.sympify(drag_str_y)\n",
    "drag_symp_z = sym.sympify(drag_str_z)\n",
    "\n",
    "drag_symp_x = drag_symp_x.subs([(A, A_const), (m, m_const), (C_D, C_D_const),\\\n",
    "                        (theta_dot, theta_dot_const)])\n",
    "drag_symp_y = drag_symp_y.subs([(A, A_const), (m, m_const), (C_D, C_D_const),\\\n",
    "                        (theta_dot, theta_dot_const)])\n",
    "drag_symp_z = drag_symp_z.subs([(A, A_const), (m, m_const), (C_D, C_D_const),\\\n",
    "                        (theta_dot, theta_dot_const)])\n",
    "\n",
    "\n",
    "drag_symp_x = drag_symp_x.subs([(r, sym.sqrt(x**2+y**2+z**2))])\n",
    "x_acc = x_acc + drag_symp_x\n",
    "\n",
    "drag_symp_y = drag_symp_y.subs([(r, sym.sqrt(x**2+y**2+z**2))])\n",
    "y_acc = y_acc + drag_symp_y\n",
    "\n",
    "drag_symp_z = drag_symp_z.subs([(r, sym.sqrt(x**2+y**2+z**2))])\n",
    "z_acc = z_acc + drag_symp_z\n",
    "\n",
    "\n",
    "x_acc_sol_fcn = lambdify((x, y, z, x_dot, y_dot, z_dot, density), x_acc)\n",
    "y_acc_sol_fcn = lambdify((x, y, z, x_dot, y_dot, z_dot, density), y_acc)\n",
    "z_acc_sol_fcn = lambdify((x, y, z, x_dot, y_dot, z_dot, density), z_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if (meas_type == 1) or (meas_type == 3):\n",
    "    \n",
    "    x_s, y_s, z_s, x_sf, y_sf, z_sf, theta, theta_dot, t, x_dot, y_dot, z_dot = \\\n",
    "        sym.symbols('x_s, y_s, z_s, x_sf, y_sf, z_sf, theta, theta_dot, t, x_dot, y_dot, z_dot')\n",
    "\n",
    "    #define symbolic rho equation\n",
    "    rho = ('sqrt((x - x_s)**2 + (y - y_s)**2 + (z - z_s)**2)')\n",
    "    rho = sym.sympify(rho)\n",
    "    #sub rotation equation of ecef for eci\n",
    "    rho = rho.subs(x_s, x_sf*sym.cos(omega_const*t) - y_sf*sym.sin(omega_const*t))\n",
    "    rho = rho.subs(y_s, x_sf*sym.sin(omega_const*t) + y_sf*sym.cos(omega_const*t))\n",
    "    rho = rho.subs(z_s, z_sf)\n",
    "\n",
    "    #define symbolic rho dot equation\n",
    "    rho_dot = ('(x*x_dot + y*y_dot + z*z_dot - (x_dot*x_s+y_dot*y_s)*cos(theta) + \\\n",
    "               theta_dot*(x*x_s + y*y_s)*sin(theta) + (x_dot*y_s - y_dot*x_s)*sin(theta) +\\\n",
    "               theta_dot*(x*y_s - y*x_s)*cos(theta) - z_dot*z_s)/ rho')\n",
    "    rho_dot = sym.sympify(rho_dot)\n",
    "    #execute substitutions for rho_dot\n",
    "    rho_dot = rho_dot.subs(x_s, x_sf) \n",
    "    rho_dot = rho_dot.subs(y_s, y_sf) \n",
    "    rho_dot = rho_dot.subs(z_s, z_sf) \n",
    "    rho_dot = rho_dot.subs('rho', rho)\n",
    "    rho_dot = rho_dot.subs(theta, omega_const*t)    \n",
    "    rho_dot = rho_dot.subs(theta_dot, omega_const)\n",
    "\n",
    "    rho_fcn = lambdify(((x, y, z, x_sf, y_sf, z_sf, t)), rho)\n",
    "    rho_dot_fcn = lambdify(((x, y, z, x_dot, y_dot, z_dot, x_sf, y_sf, z_sf, t)), rho_dot)\n",
    "\n",
    "\n",
    "if (meas_type == 2) or (meas_type == 3):\n",
    "    \n",
    "    #x_sf, etc. is the sensor pos in ecef\n",
    "    #x, y, z is the satellite eci\n",
    "    x_s, y_s, z_s, x_sf, y_sf, z_sf, theta, x, y, z, t = \\\n",
    "        sym.symbols('x_s, y_s, z_s, x_sf, y_sf, z_sf, theta, x, y, z, t')\n",
    "    x_L, y_L, z_L, X_L_norm, x_range, y_range, z_range, lon, lat, \\\n",
    "        x_sat_ecef, y_sat_ecef, z_sat_ecef, sen_ecef_norm, omega  = \\\n",
    "        sym.symbols('x_L, y_L, z_L, X_L_norm, x_range, y_range, z_range, lon, lat, \\\n",
    "        x_sat_ecef, y_sat_ecef, z_sat_ecef, sen_ecef_norm, omega')\n",
    "        \n",
    "\n",
    "    #define symbolic rho equation\n",
    "    azimuth = ('atan2(x_L, y_L)') #step 4\n",
    "    azimuth = sym.sympify(azimuth)\n",
    "    \n",
    "    elevation = ('asin(z_L/X_L_norm)') #step 4\n",
    "    elevation = sym.sympify(elevation)\n",
    "    elevation = elevation.subs(X_L_norm, sym.sqrt(x_L**2 + y_L**2 + z_L**2))\n",
    "    \n",
    "    #step 3\n",
    "    azimuth = azimuth.subs([(x_L, -x_range*sym.sin(lon) + y_range*sym.cos(lon)), \\\n",
    "            (y_L, -x_range*sym.sin(lat)*sym.cos(lon) - y_range*sym.sin(lat)*sym.sin(lon) + z_range*sym.cos(lat))])\n",
    "    elevation = elevation.subs([(x_L, -x_range*sym.sin(lon) + y_range*sym.cos(lon)), \\\n",
    "            (y_L, -x_range*sym.sin(lat)*sym.cos(lon) - y_range*sym.sin(lat)*sym.sin(lon) + z_range*sym.cos(lat)), \\\n",
    "            (z_L, x_range*sym.cos(lat)*sym.cos(lon) + y_range*sym.cos(lat)*sym.sin(lon) + z_range*sym.sin(lat))])\n",
    "    \n",
    "    #step 2\n",
    "    azimuth = azimuth.subs([(x_range, x_sat_ecef - x_sf), (y_range, y_sat_ecef - y_sf), \\\n",
    "            (z_range, z_sat_ecef - z_sf), (lat, sym.asin(z_sf/sen_ecef_norm)), (lon, sym.atan2(y_sf, x_sf))])\n",
    "    elevation = elevation.subs([(x_range, x_sat_ecef - x_sf), (y_range, y_sat_ecef - y_sf), \\\n",
    "            (z_range, z_sat_ecef - z_sf), (lat, sym.asin(z_sf/sen_ecef_norm)), (lon, sym.atan2(y_sf, x_sf))])\n",
    "    \n",
    "    #step 1\n",
    "    azimuth = azimuth.subs([(x_sat_ecef, x*sym.cos(theta) + y*sym.sin(theta)), \\\n",
    "                        (y_sat_ecef, -x*sym.sin(theta) + y*sym.cos(theta)), (z_sat_ecef, z), \\\n",
    "                        (sen_ecef_norm, sym.sqrt(x_sf**2 + y_sf**2 + z_sf**2))])\n",
    "    elevation = elevation.subs([(x_sat_ecef, x*sym.cos(theta) + y*sym.sin(theta)), \\\n",
    "                        (y_sat_ecef, -x*sym.sin(theta) + y*sym.cos(theta)), (z_sat_ecef, z), \\\n",
    "                        (sen_ecef_norm, sym.sqrt(x_sf**2 + y_sf**2 + z_sf**2))])\n",
    "    \n",
    "    \n",
    "    azimuth = azimuth.subs([(theta, omega_const*t), (omega, omega_const)])\n",
    "    elevation = elevation.subs([(theta, omega_const*t), (omega, omega_const)])\n",
    "    \n",
    "    azimuth_fcn = lambdify(((x, y, z, x_sf, y_sf, z_sf, t)), azimuth)\n",
    "    elevation_fcn = lambdify(((x, y, z, x_sf, y_sf, z_sf, t)), elevation)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 803,
   "metadata": {},
   "outputs": [],
   "source": [
    "#State and A matrix\n",
    "\n",
    "\n",
    "#define the symbolic state matrix\n",
    "X = sym.Matrix([x, y, z, x_dot, y_dot, z_dot])\n",
    "X_dot = sym.Matrix([x_dot, y_dot, z_dot, x_acc, y_acc, z_acc])\n",
    "    \n",
    "\n",
    "#partial of the force model (x dot) WRT the state vector\n",
    "A_mat = X_dot.jacobian(X)\n",
    "#print(A_mat)\n",
    "\n",
    "A_sol_fcn = lambdify((x, y, z, x_dot, y_dot, z_dot, density), A_mat)\n",
    "#print(A_sol_fcn(1,2,3,4,5,6,7,8))\n",
    "\n",
    "#print(X_dot)\n",
    "X_dot_sol_fcn = lambdify((x, y, z, x_dot, y_dot, z_dot, density), X_dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define observation model (G) and H_tilde\n",
    "\n",
    "\n",
    "if meas_type == 1:\n",
    "    G = sym.Matrix([rho, rho_dot])\n",
    "    \n",
    "    \n",
    "elif meas_type == 2:\n",
    "    G = sym.Matrix([azimuth, elevation])\n",
    "    \n",
    "elif meas_type == 3:\n",
    "    G = sym.Matrix([azimuth, elevation, rho])\n",
    "\n",
    "#print(G)\n",
    "G_sol_fcn = lambdify((x, y, z, x_dot, y_dot, z_dot, x_sf, y_sf, z_sf, t), G)\n",
    "\n",
    "#partial derivitive of observation model WRT the state vector\n",
    "X_full = X = sym.Matrix([x, y, z, x_dot, y_dot, z_dot, density])\n",
    "H_tilde = G.jacobian(X_full)\n",
    "#print(H_tilde[:,-1])\n",
    "H_tilde_sol_fcn = lambdify((x, y, z, x_dot, y_dot, z_dot, x_sf, y_sf, z_sf, t), H_tilde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def calc_P_TU(X_mean, ensemble):\n",
    "    \n",
    "    dimension = len(X_mean)\n",
    "    \n",
    "    P_bar_sum = np.zeros((dimension, dimension))\n",
    "    \n",
    "    for ii in range(num_of_ensembles):\n",
    "\n",
    "        X = ensemble[:, ii].reshape(dimension, 1)\n",
    "        diff_X = X - X_mean\n",
    "        P_bar_sum = P_bar_sum + np.dot(diff_X, diff_X.T)\n",
    "\n",
    "    P_bar = P_bar_sum/(num_of_ensembles-1)\n",
    "\n",
    "    return P_bar\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20520.0\n",
      "20520.0\n",
      "20520.0\n",
      "20520.0\n",
      "20520.0\n",
      "(30, 450)\n",
      "[  1.291429687067983e+03   5.893494158448985e+03   3.089356323718909e+03\n",
      "  -7.316221163381272e+00   4.191185560028785e-01   2.259006505945974e+00\n",
      "   5.687360557365937e+03  -1.728511250918340e+03  -3.257215182313016e+03\n",
      "   2.353430530247371e+00   7.294763086940775e+00   2.378317440977042e-01\n",
      "  -6.772740907936626e+03   9.457964405709244e+00   2.678128414205885e+02\n",
      "  -8.763989682898038e-02  -7.413075043154821e+00  -1.960493789816318e+00\n",
      "   6.422705760491270e+03   1.790572752892109e+03  -1.219520542452833e+03\n",
      "  -1.168313655351251e+00   6.658027158950876e+00   3.621371323001144e+00\n",
      "  -5.103631139819520e+03  -4.430438016344372e+03   5.152631191266021e+02\n",
      "   3.306422963141880e+00  -4.426902955250112e+00  -5.317480635615175e+00]\n"
     ]
    }
   ],
   "source": [
    "#define reference state at epoch, covariance at epoch, and R (using measurement noise)\n",
    "\n",
    "pos_perturbation = 100 * 1e-3 #km\n",
    "vel_perturbation = .1 * 1e-3 #km/s\n",
    "\n",
    "density_dimension = num_of_lat * num_of_lon\n",
    "prob_dimension = 6 * num_of_objects\n",
    "fullState_dimension = prob_dimension + density_dimension\n",
    "\n",
    "single_obj_fullState_dim = 6 + density_dimension\n",
    "\n",
    "\n",
    "\n",
    "X_ref = np.zeros((num_of_objects, 6))\n",
    "P_bar_0 = np.zeros((num_of_objects, 7, 7))\n",
    "X_RV_ensemble = np.zeros(())\n",
    "\n",
    "\n",
    "\n",
    "for ii in range(num_of_objects):\n",
    "\n",
    "    \n",
    "    indices = np.where(true_state_all_times[:, 1] == (ii+1))[0] #find indices of truth that pertain to this object\n",
    "    initial_RV = true_state_all_times[np.r_[indices],:][initial_sim_index] #select specificed initial index for sim\n",
    "    print(initial_RV[0])\n",
    "\n",
    "    X_ref[ii,:] = np.array([initial_RV[2] + pos_perturbation, initial_RV[3] + pos_perturbation, initial_RV[4] \\\n",
    "                + pos_perturbation, initial_RV[5] + vel_perturbation, initial_RV[6] + vel_perturbation, \\\n",
    "                initial_RV[7] + vel_perturbation])\n",
    "    \n",
    "    \n",
    "X_ref = X_ref.reshape(prob_dimension)\n",
    "\n",
    "\n",
    "P_sigma_pos = 100 * 1e-3 #km\n",
    "P_sigma_vel = .1 * 1e-3 #km/s\n",
    "P_bar_0_singleObj = [P_sigma_pos**2, P_sigma_pos**2, P_sigma_pos**2, \\\n",
    "                   P_sigma_vel**2, P_sigma_vel**2, P_sigma_vel**2]\n",
    "\n",
    "P_bar_0_multiObj = np.tile(P_bar_0_singleObj, num_of_objects)\n",
    "P_bar_0_multiObj = np.append(P_bar_0_multiObj, (1e-4)**2) #add density at the end\n",
    "P_bar_0 = np.diag(P_bar_0_multiObj)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "(L, V) = np.linalg.eig(P_bar_0[:prob_dimension, :prob_dimension]) #eigenvalues, eigenvectors\n",
    "Bsqrt_cov = V * np.sqrt(L)\n",
    "X_RV_ensemble = gen_ensemble(X_ref, Bsqrt_cov, num_of_ensembles)\n",
    "print(np.shape(X_RV_ensemble))\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "#for process noise\n",
    "snc_sigma = 1e-5\n",
    "\n",
    "       \n",
    "                  \n",
    "#Define Measurement Noise\n",
    "if meas_type == 1:\n",
    "    sigma_rho = .1 * 1e-3 #km\n",
    "    sigma_rho_dot = .01 * 1e-3 #km/s\n",
    "    W = np.array([[1/sigma_rho**2, 0], [0, 1/sigma_rho_dot**2]])\n",
    "    R = np.linalg.inv(W)\n",
    "    \n",
    "elif meas_type == 2:\n",
    "    arcsec_per_rad = 206264.806247 #arcsec/rad\n",
    "    noise_rad = (1/arcsec_per_rad)  * 5  #5 arcsec noise (suggested by Moriba for a telescope)\n",
    "    sigma_az = noise_rad\n",
    "    sigma_el = noise_rad\n",
    "    W = np.array([[1/sigma_az**2, 0], [0, 1/sigma_el**2]])\n",
    "    R = np.linalg.inv(W)\n",
    "    \n",
    "elif meas_type == 3:\n",
    "    arcsec_per_rad = 206264.806247 #arcsec/rad\n",
    "    noise_rad = (1/arcsec_per_rad)  * .15  #5 arcsec noise (suggested by Moriba for a telescope)\n",
    "    sigma_az = noise_rad #noise_rad\n",
    "    sigma_el = noise_rad #noise_rad\n",
    "    sigma_rho = .1 * 1e-3 #km\n",
    "    W = np.array([[1/sigma_az**2, 0, 0], [0, 1/sigma_el**2, 0], [0, 0, 1/sigma_rho**2]])\n",
    "    R = np.linalg.inv(W)\n",
    "    \n",
    "    meas_indices = np.array([0, 1, 2])\n",
    "\n",
    "\n",
    "#reduced rank variable for number of eigvals\n",
    "q = 39\n",
    "\n",
    "    \n",
    "print(X_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nindex_firstMeas = int(27910/10)\\n\\nindices = np.where(true_state_all_times[:,1] == 5)[0]  \\ntrue_state_thisObj = true_state_all_times[np.r_[indices,:]][index_firstMeas]\\ntrue_dens = true_state_thisObj[-1]\\nprint(true_dens)\\nprint(truth_xyz[0,-1])\\n\\n\\nobj_index_begin = 4*6\\nobj_index_end = (4+1)*6\\n(lat_grid_ticks, lst_grid_ticks) = filter_functions.calc_lat_lst_indices(27910,                         truth_xyz[0,:3], day_of_month_init, hour_init_UT, month_init,                                        year_init, omega_const, r_earth_const, lat_res, lon_res)\\n\\nprint(lat_grid_ticks, lst_grid_ticks)\\nlat_grid_ticks = 24\\nlst_grid_ticks = 38\\nprint('est:', np.degrees(latitude_grid[lat_grid_ticks]), np.degrees(longitude_grid[lst_grid_ticks]))\\nprint('true:', np.degrees(lat_lst_meas_array[0, :]))\\n\\n\\n\\nest_dens = np.mean(ensembles_of_density_grid.reshape(num_of_ensembles, num_of_lat, num_of_lon)[:, 24, 38])\\nprint(np.mean(est_dens))\\n\\nest_dens = ensembles_of_density_grid.reshape(num_of_ensembles, num_of_lat, num_of_lon)                                                                      [:, lat_grid_ticks+1, lst_grid_ticks]\\nprint(np.mean(est_dens))\\nest_dens = ensembles_of_density_grid.reshape(num_of_ensembles, num_of_lat, num_of_lon)                                                                      [:, lat_grid_ticks, lst_grid_ticks+1]\\nprint(np.mean(est_dens))\\n\""
      ]
     },
     "execution_count": 807,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prnt(h)\n",
    "\"\"\"\n",
    "\n",
    "index_firstMeas = int(27910/10)\n",
    "\n",
    "indices = np.where(true_state_all_times[:,1] == 5)[0]  \n",
    "true_state_thisObj = true_state_all_times[np.r_[indices,:]][index_firstMeas]\n",
    "true_dens = true_state_thisObj[-1]\n",
    "print(true_dens)\n",
    "print(truth_xyz[0,-1])\n",
    "\n",
    "\n",
    "obj_index_begin = 4*6\n",
    "obj_index_end = (4+1)*6\n",
    "(lat_grid_ticks, lst_grid_ticks) = filter_functions.calc_lat_lst_indices(27910, \\\n",
    "                        truth_xyz[0,:3], day_of_month_init, hour_init_UT, month_init, \\\n",
    "                                       year_init, omega_const, r_earth_const, lat_res, lon_res)\n",
    "\n",
    "print(lat_grid_ticks, lst_grid_ticks)\n",
    "lat_grid_ticks = 24\n",
    "lst_grid_ticks = 38\n",
    "print('est:', np.degrees(latitude_grid[lat_grid_ticks]), np.degrees(longitude_grid[lst_grid_ticks]))\n",
    "print('true:', np.degrees(lat_lst_meas_array[0, :]))\n",
    "\n",
    "\n",
    "\n",
    "est_dens = np.mean(ensembles_of_density_grid.reshape(num_of_ensembles, num_of_lat, num_of_lon)[:, 24, 38])\n",
    "print(np.mean(est_dens))\n",
    "\n",
    "est_dens = ensembles_of_density_grid.reshape(num_of_ensembles, num_of_lat, num_of_lon)\\\n",
    "                                                                      [:, lat_grid_ticks+1, lst_grid_ticks]\n",
    "print(np.mean(est_dens))\n",
    "est_dens = ensembles_of_density_grid.reshape(num_of_ensembles, num_of_lat, num_of_lon)\\\n",
    "                                                                      [:, lat_grid_ticks, lst_grid_ticks+1]\n",
    "print(np.mean(est_dens))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nest_dens = ensembles_of_density_grid.reshape(num_of_ensembles, num_of_lat, num_of_lon)                                                                      [:, lat_grid_ticks, lst_grid_ticks]\\nprint(np.mean(est_dens))\\n\\n\\n\\n\\ndens_ens = ensembles_of_density_grid.reshape(num_of_ensembles, density_dimension).T\\nX_ensemble = np.zeros((fullState_dimension, num_of_ensembles))\\nX_ensemble[:prob_dimension, :] = X_RV_ensemble\\nX_ensemble[prob_dimension:, :] = dens_ens\\n\\n\\n\\nest_dens = X_ensemble[prob_dimension:, :].T.reshape(num_of_ensembles, num_of_lat, num_of_lon)                                                                      [:, lat_grid_ticks, lst_grid_ticks]\\nprint(np.mean(est_dens))\\n\\n\\nX_mean = np.sum(X_ensemble, axis=1).reshape(fullState_dimension,1)/num_of_ensembles\\nprint(X_mean[prob_dimension:].T.reshape(num_of_lat, num_of_lon)[lat_grid_ticks, lst_grid_ticks])\\n'"
      ]
     },
     "execution_count": 808,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "est_dens = ensembles_of_density_grid.reshape(num_of_ensembles, num_of_lat, num_of_lon)\\\n",
    "                                                                      [:, lat_grid_ticks, lst_grid_ticks]\n",
    "print(np.mean(est_dens))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dens_ens = ensembles_of_density_grid.reshape(num_of_ensembles, density_dimension).T\n",
    "X_ensemble = np.zeros((fullState_dimension, num_of_ensembles))\n",
    "X_ensemble[:prob_dimension, :] = X_RV_ensemble\n",
    "X_ensemble[prob_dimension:, :] = dens_ens\n",
    "\n",
    "\n",
    "\n",
    "est_dens = X_ensemble[prob_dimension:, :].T.reshape(num_of_ensembles, num_of_lat, num_of_lon)\\\n",
    "                                                                      [:, lat_grid_ticks, lst_grid_ticks]\n",
    "print(np.mean(est_dens))\n",
    "\n",
    "\n",
    "X_mean = np.sum(X_ensemble, axis=1).reshape(fullState_dimension,1)/num_of_ensembles\n",
    "print(X_mean[prob_dimension:].T.reshape(num_of_lat, num_of_lon)[lat_grid_ticks, lst_grid_ticks])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00796263841797505\n",
      "175\n"
     ]
    }
   ],
   "source": [
    "day_of_year_nom = 175 #June 24th\n",
    "f107_nom = 180\n",
    "ap_nom = 10\n",
    "\n",
    "\n",
    "lon = 0 #dependent only on lst\n",
    "\n",
    "\n",
    "Output = nrlmsise_output()\n",
    "Input = nrlmsise_input()\n",
    "flags = nrlmsise_flags()\n",
    "aph = ap_array()\n",
    "\n",
    "for i in range(7):\n",
    "    aph.a[i]=100\n",
    "flags.switches[0] = 1\n",
    "for i in range(1, 24):\n",
    "    flags.switches[i]=1\n",
    "\n",
    "Input.doy = day_of_year_init\n",
    "Input.year = 0 #/* without effect */\n",
    "Input.sec = 0\n",
    "Input.alt = 398.722 #400 #convert to km\n",
    "Input.g_lat = 29.498 #30#math.degrees(lat_lst_meas_array[0, 0])\n",
    "Input.g_long = 0\n",
    "Input.lst = 12.820 #(190/360) * 24 #lat_lst_meas_array[0, 1]/(2*math.pi) * 24\n",
    "Input.f107A = f107_nom #/* 81 day average of F10.7 flux (centered on doy) */\n",
    "Input.f107 = f107_nom #/* daily F10.7 flux for previous day */\n",
    "Input.ap = ap_nom \n",
    "\n",
    "gtd7d(Input, flags, Output)\n",
    "\n",
    "density = Output.d[5] #total mass density (grams/m^3, m^3 b/c switches[0] = 1)\n",
    "print(density*1e9)\n",
    "print(day_of_year_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAH8AAAAPBAMAAAA/sQ3hAAAAMFBMVEX///8AAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAv3aB7AAAAD3RSTlMAVO8Qq5l2zWaJMt0i\nu0SCRuA9AAAACXBIWXMAAA7EAAAOxAGVKw4bAAACNElEQVQoFaWTTWsTURSGn+k0aT4mHxvBXcQu\nBK0llIpIFo2VgijYgAZxoWQhFdw0gvsORYpdWcRqo5TMP1AXLgShAyJSFFrRbTUI4rLGqLG0mp57\n7tQ/4N3MMznPvPfcj4AZ7qX5wD7vhXjVjQCvsZw3v8g4tvQQVhvvInKnFlvWUQJnEAby7gvjjvje\nEKMk//IK57etcYNS6FUo+ErE4KR1lBi90IFZeGACxmGCU3CQ6TzbtpY4QLIeb5N7osR+eG0dJYhL\nwAcoFCWgC02OwGTwueXKi6llyjid7DBjdSWWoWQdJSv9gE1ZsvsHpoOZogRI+7IEE5Ark21LeMG3\ntPmeJdRRUsn9KQE1kQ5JBy15rkg7pQUbMFYh+0vCn2OpvzfviyJORDKLtwWPxZf+mZFqVt5Xn0mI\n6WBtgdQW3qPzEdHsSkUdSyZAOtCAgZrTzEO6LEZyOAqomAC421pTcs6tSMU4EUnAvyUwd70ZQFUE\nOYqidhAtgb5BS7eIf5cWxInItCmbWJCZzZiUzyrwFj7l9zbRaXshmbZsotNmSLqVijiWVNqAO5Jq\nxk3Yhxf2insBmXUSnVxHApTcb9DvG+elJQ2Qi3TRfD0bpnZIVYiHR2WbQ63J9YnV+9ZJ7yiZeTOh\nOkp2o9J597DcFK4Vj/u8aSye5TaOnJ1ZHk8ZCRI+hZoSX4vMWUeJ2Hj3NO79q4HZ12T1ihxkr7dN\ntnq5ZWucmDoDX6ofsZSakD+TOkqm8f8auxcV2XsLu5E0AAAAAElFTkSuQmCC\n",
      "text/latex": [
       "$$10.9231030828$$"
      ],
      "text/plain": [
       "10.9231030828"
      ]
     },
     "execution_count": 810,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lat_lst_meas_array[0, 1]/(2*math.pi) * 24\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#KALMAN\n",
    "\n",
    "def execute_enkf(obs_data, X_RV_ensemble, P, R, X_density_ensemble,\\\n",
    "                         prob_dimension, density_dimension, initial_sim_time, den_std, stop_index):\n",
    "    \n",
    "    #initializations\n",
    "    Q_ECI = np.zeros((6,6)) #*******\n",
    "    \n",
    "    num_of_meas = np.shape(R)[0]\n",
    "    X_7elem_mean_updated = np.sum(X_RV_ensemble, axis=1)/num_of_ensembles\n",
    "\n",
    "    pre_fit_list = np.zeros((stop_index, num_of_meas))\n",
    "    prefit_bounds_list = np.zeros((stop_index, num_of_meas))\n",
    "    post_fit_list = np.zeros((stop_index, num_of_meas))\n",
    "    \n",
    "    xpos_info_array = np.zeros((stop_index))\n",
    "\n",
    "    P_list = np.zeros((stop_index, prob_dimension+1, prob_dimension+1))\n",
    "    #P_full_list = np.zeros((21, fullState_dimension, fullState_dimension))\n",
    "    X_7elem_mean_updated_list = np.zeros((stop_index, prob_dimension+1))\n",
    "    X_7elem_mean_TU_list = np.zeros(((TU_array_length), prob_dimension+num_of_objects))\n",
    "\n",
    "    \n",
    "    density_MSIS_array = np.zeros(stop_index)\n",
    "    #est_density_grid_array = np.zeros((stop_index, num_of_lat, num_of_lon))\n",
    "\n",
    "    \n",
    "    X_distribution = np.zeros((prob_dimension, num_of_ensembles, stop_index))\n",
    "    density_distribution = np.zeros((num_of_ensembles, stop_index))\n",
    "    lat_lst_array = np.zeros((2, stop_index))\n",
    "    X_ensemble = np.zeros((fullState_dimension, num_of_ensembles))\n",
    "\n",
    "    \n",
    "    #for Reduced Rank\n",
    "    dens_ens = X_density_ensemble.reshape(num_of_ensembles, density_dimension).T\n",
    "    X_ensemble = np.zeros((fullState_dimension, num_of_ensembles))\n",
    "    X_ensemble[:prob_dimension, :] = X_RV_ensemble\n",
    "    X_ensemble[prob_dimension:, :] = dens_ens\n",
    "\n",
    "    #mean of X state & density state over ensemble\n",
    "    X_mean = np.sum(X_ensemble, axis=1).reshape(fullState_dimension,1)/num_of_ensembles\n",
    "    P = calc_P_TU(X_mean, X_ensemble)\n",
    "    \n",
    "    eigvals, eigvecs = np.linalg.eig(P)\n",
    "    eigvec_truncated = eigvecs[:, :q]\n",
    "    eigvals_truncated = eigvals[:q]\n",
    "    L = np.dot(eigvec_truncated, np.diag(np.sqrt(eigvals_truncated)))\n",
    "    L = np.real(L)\n",
    "    \n",
    "    FQ = np.zeros((fullState_dimension, fullState_dimension))\n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    \n",
    "    t_init = initial_sim_time\n",
    "    time = t_init + time_step\n",
    "    obsnum = 0\n",
    "\n",
    "    \n",
    "    \n",
    "    while time < obs_data[stop_index,0]:\n",
    "    #for obsnum in range(len(obs_data[:stop_index])):\n",
    "    \n",
    "            \n",
    "        print(time)\n",
    "           \n",
    "         \n",
    "        #reduced rank\n",
    "        epsilon = .7 #.95 #number close to 1\n",
    "        X_mean_wSigma_array = np.tile(X_mean, q) + epsilon*L\n",
    "    \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        result = X_RV_ensemble\n",
    "        \n",
    "        #if (obsnum != 0): still need to propagate if obsnum == 0, b/c initializations occur at t=0 not 1st obs\n",
    "        for obj in range(num_of_objects):\n",
    "            \n",
    "            obj_index_begin = obj*6\n",
    "            obj_index_end = (obj+1)*6\n",
    "            \n",
    "            #determine density to be used in propagation\n",
    "            (lat_grid_ticks, lst_grid_ticks) = filter_functions.calc_lat_lst_indices(t_init, \\\n",
    "                            X_mean[obj_index_begin:(obj_index_begin+3),0], day_of_month_init, hour_init_UT, month_init, \\\n",
    "                                   year_init, omega_const, r_earth_const, lat_res, lon_res)\n",
    "            densities = X_density_ensemble[:, lat_grid_ticks, lst_grid_ticks]\n",
    "            #print(np.mean(densities))\n",
    "\n",
    "\n",
    "            for ii in range(num_of_ensembles):\n",
    "                #set the initial values for the propogator:\n",
    "                y0 = X_RV_ensemble[obj_index_begin:obj_index_end, ii]\n",
    "                density = densities[ii] * 1e-9\n",
    "            \n",
    "                integrator = ode(orbitpropogator_EnKF)\n",
    "                integrator.set_integrator('dopri5', nsteps=1e6, rtol=1e-12, atol=1e-12)\n",
    "                integrator.set_f_params(density)\n",
    "                integrator.set_initial_value(y0, t_init)\n",
    "                integrator.integrate(time)\n",
    "                result[obj_index_begin:obj_index_end, ii] = integrator.y\n",
    "        \n",
    "        X_RV_ensemble = result.reshape(prob_dimension, num_of_ensembles)\n",
    "        \n",
    "        \n",
    "        #create array that holds X state ensemble & density state ensemble combined\n",
    "        density_state = X_density_ensemble[:, lat_grid_ticks, lst_grid_ticks]\n",
    "        X_ensemble[:prob_dimension, :] = X_RV_ensemble\n",
    "        X_ensemble[prob_dimension:, :] = X_density_ensemble.reshape(num_of_ensembles, density_dimension).T \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        #reduced rank\n",
    "        result = X_mean_wSigma_array[:prob_dimension,:]\n",
    "        result_mean = X_mean[:prob_dimension]\n",
    "        \n",
    "        #if (obsnum != 0): still need to propagate if obsnum == 0, b/c initializations occur at t=0 not 1st obs\n",
    "        for obj in range(num_of_objects):\n",
    "            \n",
    "            obj_index_begin = obj*6\n",
    "            obj_index_end = (obj+1)*6\n",
    "            \n",
    "            #determine density to be used in propagation\n",
    "            (lat_grid_ticks, lst_grid_ticks) = filter_functions.calc_lat_lst_indices(t_init, \\\n",
    "                            X_mean[obj_index_begin:(obj_index_begin+3),0], day_of_month_init, hour_init_UT, month_init, \\\n",
    "                                   year_init, omega_const, r_earth_const, lat_res, lon_res)\n",
    "            densities = X_density_ensemble[:, lat_grid_ticks, lst_grid_ticks]\n",
    "\n",
    "            \n",
    "            for ii in range(q):\n",
    "                y0 = X_mean_wSigma_array[obj_index_begin:obj_index_end, ii]\n",
    "                density = densities[ii] * 1e-9\n",
    "            \n",
    "                integrator = ode(orbitpropogator_EnKF)\n",
    "                integrator.set_integrator('dopri5', nsteps=1e6, rtol=1e-12, atol=1e-12)\n",
    "                integrator.set_f_params(density)\n",
    "                integrator.set_initial_value(y0, t_init)\n",
    "                integrator.integrate(time)\n",
    "                result[obj_index_begin:obj_index_end, ii] = integrator.y\n",
    "                \n",
    "            \n",
    "            y0 = X_mean[obj_index_begin:obj_index_end]\n",
    "            density = np.mean(densities[ii]) * 1e-9\n",
    "\n",
    "            integrator = ode(orbitpropogator_EnKF)\n",
    "            integrator.set_integrator('dopri5', nsteps=1e6, rtol=1e-12, atol=1e-12)\n",
    "            integrator.set_f_params(density)\n",
    "            integrator.set_initial_value(y0, t_init)\n",
    "            integrator.integrate(time)\n",
    "            result_mean[obj_index_begin:obj_index_end] = integrator.y    \n",
    "            \n",
    "        \n",
    "        X_mean_wSigma_array_TU = np.zeros((fullState_dimension, q))\n",
    "        X_mean_wSigma_array_TU[:prob_dimension,:] = result.reshape(prob_dimension, q)\n",
    "        X_mean_wSigma_array_TU[prob_dimension:,:] = X_mean_wSigma_array[prob_dimension:,:]\n",
    "        \n",
    "        X_mean_TU = np.zeros((fullState_dimension, 1))\n",
    "        X_mean_TU[:prob_dimension, 0] = result_mean.reshape(prob_dimension)\n",
    "        X_mean_TU[prob_dimension:, 0] = X_mean[prob_dimension:].reshape(density_dimension)\n",
    "\n",
    "        L = (1/epsilon) * (X_mean_wSigma_array_TU - np.tile(X_mean_TU, q))\n",
    "        \n",
    "        \n",
    "        #Use SNC P_bar if delta t is less than 100 seconds---------------------------\n",
    "        if (obs_data[obsnum, 0] != obs_data[obsnum-1, 0]) and (obsnum != 0):\n",
    "            delta_t = prop_step\n",
    "            if snc_flag:\n",
    "                #build SNC STM\n",
    "                idenity_6_3 = np.array([(delta_t/2)*np.eye(3), np.eye(3)]).reshape(6, 3)\n",
    "                snc_stm = delta_t*idenity_6_3\n",
    "                #Covariance with SNC: Q Matrix\n",
    "                Q = np.identity(3)*(snc_sigma)**2\n",
    "\n",
    "                #Q_entire = np.zeros((fullState_dimension, fullState_dimension))\n",
    "                #Q_entire[:prob_dimension, :prob_dimension] = np.dot(snc_stm, np.dot(Q, snc_stm.T))\n",
    "                #instead:\n",
    "                \n",
    "                #FQ[:prob_dimension, :prob_dimension] = np.dot(snc_stm, np.dot(Q, snc_stm.T))\n",
    "                FQ[prob_dimension:, prob_dimension:] = FQ[prob_dimension:, prob_dimension:] +\\\n",
    "                                    np.diag(np.ones((density_dimension))) * den_std\n",
    "                \n",
    "                #build density portion of Q\n",
    "                #Q_entire[prob_dimension:, prob_dimension:] = np.eye(density_dimension, density_dimension) * 1e-9\n",
    "                \n",
    "                #Eq. 24:\n",
    "                #L_tilde = np.zeros((fullState_dimension, q+fullState_dimension))\n",
    "                #L_tilde[:, :q] = L\n",
    "                #L_tilde[:, q:] = FQ #Q_entire\n",
    "                \n",
    "                #Eq. 25:\n",
    "                #eigvals, X = np.linalg.eig(np.dot(L_tilde.T, L_tilde)) \n",
    "                #L = np.dot(L_tilde, X)[:, :q]\n",
    "                \n",
    "\n",
    "        \n",
    "        #mean of X state & density state over ensemble, reduced rank, *re-definition of X_mean on purpose*\n",
    "        if snc_flag:\n",
    "            ensemble_noise = np.random.randn(num_of_lat * num_of_lon, num_of_ensembles) * den_std #1e-8 #1e-9 #1e-10#\n",
    "            X_ensemble[prob_dimension:, :] = X_ensemble[prob_dimension:, :] + ensemble_noise\n",
    "        X_mean = np.sum(X_ensemble, axis=1).reshape(fullState_dimension,1)/num_of_ensembles #Eq. 16\n",
    "        E = X_ensemble - np.tile(X_mean, num_of_ensembles) #Eq. 17\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #save propagation results; will be over-written w/ MU values if measurement at this time\n",
    "        X_7elem_mean_TU = np.zeros((prob_dimension+num_of_objects))\n",
    "        X_7elem_mean_TU[:prob_dimension] = \\\n",
    "                            np.sum(X_ensemble[:prob_dimension,:], axis=1)/num_of_ensembles\n",
    "            \n",
    "        #for TU density loop through objects, so calc indices for each object before grabbing its mean density\n",
    "        for jj in range(num_of_objects): \n",
    "            ID = jj+1\n",
    "            obj_index_begin = (ID-1)*6\n",
    "            (lat_grid_ticks, lst_grid_ticks) = filter_functions.calc_lat_lst_indices(time, \\\n",
    "                        X_mean[obj_index_begin:(obj_index_begin+3),0], day_of_month_init, hour_init_UT, month_init, \\\n",
    "                                       year_init, omega_const, r_earth_const, lat_res, lon_res)\n",
    "            X_7elem_mean_TU[prob_dimension+jj] = \\\n",
    "                                np.sum(X_density_ensemble[:, lat_grid_ticks, lst_grid_ticks])/num_of_ensembles\n",
    "            X_7elem_mean_TU_list[counter, :] = X_7elem_mean_TU\n",
    "\n",
    "       \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        #-------------------Measurement Update--------------------------------------------------------\n",
    "        \n",
    "        \n",
    "        indices_4_timeStep = np.where(obs_data[:,0] == time)[0]\n",
    "        for jj in range(len(indices_4_timeStep)):\n",
    "\n",
    "            obsnum = indices_4_timeStep[jj]\n",
    "            \n",
    "            \n",
    "            if jj == 0: #first measurement at this time\n",
    "                \n",
    "                #Eq. 24:\n",
    "                L_tilde = np.zeros((fullState_dimension, q+fullState_dimension))\n",
    "                L_tilde[:, :q] = L\n",
    "                L_tilde[:, q:] = FQ #Q_entire\n",
    "                \n",
    "                #Eq. 25:\n",
    "                eigvals, X = np.linalg.eig(np.dot(L_tilde.T, L_tilde)) \n",
    "                L = np.dot(L_tilde, X)[:, :q]\n",
    "                \n",
    "                FQ = np.zeros((fullState_dimension, fullState_dimension))\n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "            #based on object ID, calculate indices in the state for this update\n",
    "            ID = int(obs_data[obsnum, 2])\n",
    "            obj_index_begin = (ID-1)*6\n",
    "            obj_index_end = ID*6\n",
    "            print('obs#', obsnum, 'obj', ID)\n",
    "            \n",
    "            \n",
    "            #determine density to be used in propagation\n",
    "            (lat_grid_ticks, lst_grid_ticks) = filter_functions.calc_lat_lst_indices(t_init, \\\n",
    "                            X_mean[obj_index_begin:(obj_index_begin+3),0], day_of_month_init, hour_init_UT, month_init, \\\n",
    "                                   year_init, omega_const, r_earth_const, lat_res, lon_res)\n",
    "            \n",
    "\n",
    "        \n",
    "            #determine station coordinates for observation eq.\n",
    "            if int(obs_data[obsnum, 1]) == 1:\n",
    "                #print('1')\n",
    "                station_index = 0\n",
    "                X_s = r_ecef_dss34\n",
    "            if int(obs_data[obsnum, 1]) == 2:\n",
    "                #print('2')\n",
    "                station_index = 1\n",
    "                X_s = r_ecef_dss65\n",
    "            if int(obs_data[obsnum, 1]) == 3:\n",
    "                #print('amos')\n",
    "                station_index = 3\n",
    "                X_s = r_ecef_diego\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #assuming Eq. 31 is supposed to be ensemble - x_mean, not just ensemble\n",
    "            #below is the alternative to Eq. 32 (determined by Penny & I)\n",
    "            EtransposeE = np.dot(E.T, E)\n",
    "            eigvals, X = np.linalg.eig(EtransposeE)\n",
    "            E_perp = np.dot(E, X)[:,q:] #Eq. 32 alternative\n",
    "\n",
    "            E_doublePrime = np.dot(E, X)[:,:q] #Eq. 35 alternative\n",
    "\n",
    "            lambdaa = .1 #.2\n",
    "            P = lambdaa * np.dot(L, L.T) + \\\n",
    "                    ((1-lambdaa)/(num_of_ensembles-1)) * np.dot(E_doublePrime, E_doublePrime.T) \\\n",
    "                                    + (1/(num_of_ensembles-1)) * np.dot(E_perp, E_perp.T)  #Eq. 37\n",
    "\n",
    "\n",
    "            \n",
    "            H = np.zeros((num_of_meas, fullState_dimension))\n",
    "            indices = np.r_[obj_index_begin:obj_index_end, prob_dimension:prob_dimension+1]\n",
    "            H[:num_of_meas, indices] = H_tilde_sol_fcn(*X_mean[obj_index_begin:obj_index_end,0], *X_s, time)\n",
    "\n",
    "            part1 = np.dot(P, H.T)\n",
    "            part2 = np.dot(H, np.dot(P, H.T))\n",
    "            K = np.dot(part1, np.linalg.inv(part2 + R)) #Eq. 34\n",
    "\n",
    "\n",
    "\n",
    "            #computations needed for Eq. 20\n",
    "            Hx_array = np.zeros((num_of_meas, num_of_ensembles))\n",
    "            Hx = np.zeros(3)\n",
    "            for ii in range(num_of_ensembles):\n",
    "\n",
    "                X = X_RV_ensemble[obj_index_begin:obj_index_end, ii]\n",
    "                Hx = G_sol_fcn(*X, *X_s, time)\n",
    "\n",
    "                if (meas_type != 1) and (Hx[0] < 0): #if az less than 0, add 2*pi\n",
    "                    Hx[0] = Hx[0] + 2*math.pi\n",
    "\n",
    "                Hx_array[:, ii] = Hx.reshape(num_of_meas)\n",
    "\n",
    "            \n",
    "            #get actual observation\n",
    "            y_observed = obs_data[obsnum, 3:(3+num_of_meas)].reshape(num_of_meas,1)\n",
    "\n",
    "            #Eq. 20:\n",
    "            X_ensemble_updated = np.zeros((fullState_dimension, num_of_ensembles))\n",
    "            correction_array = np.zeros((num_of_ensembles, fullState_dimension))\n",
    "            for ii in range(num_of_ensembles):\n",
    "\n",
    "                e = np.sqrt(np.diag(R)).reshape(num_of_meas, 1) * np.random.randn(num_of_meas, 1)\n",
    "                \n",
    "                correction = \\\n",
    "                np.dot(K, (y_observed + e - Hx_array[:, ii].reshape(num_of_meas, 1))).reshape(fullState_dimension)\n",
    "                correction_array[ii] = correction\n",
    "                \n",
    "                correction[:obj_index_begin] = np.zeros((obj_index_begin))\n",
    "                correction[obj_index_end:prob_dimension] = np.zeros((prob_dimension - obj_index_end))\n",
    "                \n",
    "\n",
    "                X_member_updated = X_ensemble[:, ii].reshape(fullState_dimension, 1) + \\\n",
    "                                                                        correction.reshape(fullState_dimension, 1)\n",
    "\n",
    "                X_ensemble_updated[:, ii] = X_member_updated.reshape(fullState_dimension)\n",
    "\n",
    "\n",
    "\n",
    "            #update overall density grid ensemble with this lat/lon's updated density values \n",
    "            X_density_ensemble = copy.deepcopy(X_ensemble_updated[prob_dimension:,:].T.reshape(num_of_ensembles, \\\n",
    "                                                                                            num_of_lat, num_of_lon)) \n",
    "            X_ensemble = copy.deepcopy(X_ensemble_updated)\n",
    "            #est_density_grid_array[obsnum] = np.mean(X_density_ensemble, axis=0)\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "            #SRF Update equations for updating L for next obs time\n",
    "            #Eq. 28, using X_mean b/c paper shows mean being propagated then used, not\n",
    "            #ensemble being propagated and then the mean calculated and used\n",
    "            Hx = G_sol_fcn(*X_mean[obj_index_begin:obj_index_end,0], *X_s, time)\n",
    "            X_mean_MU = X_mean + np.dot(K, y - Hx) #not used? Eq. 28\n",
    "\n",
    "\n",
    "            #Eq. 29:\n",
    "            H = np.zeros((num_of_meas, fullState_dimension))\n",
    "            indices = np.r_[obj_index_begin:obj_index_end, prob_dimension:prob_dimension+1]\n",
    "            H[:num_of_meas, indices] = H_tilde_sol_fcn(*X_mean[obj_index_begin:obj_index_end,0], *X_s, time)\n",
    "\n",
    "            L_tilde = np.zeros((fullState_dimension, q+num_of_meas))\n",
    "            part1 = np.eye(fullState_dimension, fullState_dimension) - np.dot(K, H)\n",
    "            L_tilde[:, :q] = np.dot(part1, L)\n",
    "            L_tilde[:, q:] = np.dot(K, np.sqrt(R)) #Eq. 29\n",
    "\n",
    "            #Eq. 30\n",
    "            eigvals, X = np.linalg.eig(np.dot(L_tilde.T, L_tilde))\n",
    "            L = np.dot(L_tilde, X)[:, :q]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #save values\n",
    "            X_7elem_mean_updated = np.zeros((prob_dimension+1))\n",
    "            X_7elem_mean_updated[:prob_dimension] = \\\n",
    "                                np.sum(X_ensemble_updated[:prob_dimension,:], axis=1)/num_of_ensembles\n",
    "            X_7elem_mean_updated[prob_dimension:] = np.sum(X_density_ensemble[:, lat_grid_ticks, lst_grid_ticks])\\\n",
    "                                                                                                /num_of_ensembles\n",
    "            X_7elem_mean_updated_list[obsnum, :] = X_7elem_mean_updated\n",
    "\n",
    "            #save values for analysis of distributions and such\n",
    "            density_distribution[:, obsnum] = X_density_ensemble[:, lat_grid_ticks, lst_grid_ticks] \n",
    "            X_distribution[:, :, obsnum] = X_ensemble_updated[:prob_dimension, :]\n",
    "            lat_lst_array[:, obsnum] = \\\n",
    "                np.array([latitude_grid[lat_grid_ticks], longitude_grid[lst_grid_ticks]]).reshape(2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            P_list[obsnum, :prob_dimension, :prob_dimension] = P[:prob_dimension, :prob_dimension]\n",
    "            P_list[obsnum, -1, -1] = \\\n",
    "                        P[0,prob_dimension:].reshape(num_of_lat, num_of_lon)[lat_grid_ticks, lst_grid_ticks]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #-------------RMS / Residuals-------------\n",
    "            Hx = G_sol_fcn(*X_7elem_mean_updated[obj_index_begin:obj_index_end], *X_s, time)\n",
    "            if (meas_type != 1) and (Hx[0] < 0): #if az less than 0, add 2*pi\n",
    "                Hx[0] = Hx[0] + 2*math.pi\n",
    "            post_fit_resid = y_observed - Hx.reshape(num_of_meas, 1)\n",
    "            post_fit_list[obsnum, :] = post_fit_resid.reshape(num_of_meas) \n",
    "\n",
    "\n",
    "            #pre-fit residual calculation\n",
    "            #determine observation as evaluated in observation equation with ref. \n",
    "            y_G_ref = G_sol_fcn(*X_mean[obj_index_begin:obj_index_end,0], *X_s, time)\n",
    "            if (meas_type != 1) and (y_G_ref[0] < 0): #if az less than 0, add 2*pi\n",
    "                    y_G_ref[0] = y_G_ref[0] + 2*math.pi\n",
    "                    \n",
    "            \n",
    "            #calculate information matrix for this set of measurements (at this time & for this object)\n",
    "            H_info = H_tilde_sol_fcn(*X_7elem_mean_updated[obj_index_begin:obj_index_end], *X_s, time)\n",
    "            info_mat = np.dot(H_info.T, np.dot(np.linalg.inv(R), H_info))\n",
    "            info = info_mat[0,0]\n",
    "            xpos_info_array[obsnum] =  info  \n",
    "\n",
    "                \n",
    "\n",
    "            #get actual observation\n",
    "            y_observed = obs_data[obsnum, 3:(3+num_of_meas)].reshape(num_of_meas,1)\n",
    "\n",
    "            #calculate pre-fit\n",
    "            prefit_bounds = np.sqrt(np.dot(H, np.dot(P, H.T)) + R)\n",
    "            prefit_bounds_list[obsnum, :] = np.diag(prefit_bounds)\n",
    "            prefit = y_observed - y_G_ref\n",
    "            pre_fit_list[obsnum, :] = prefit.reshape(num_of_meas)\n",
    "            \n",
    "            #Tracer() ()\n",
    "            #if (np.any(prefit[:2]) > np.radians(.1)) or (prefit[-1] > .1):\n",
    "                #den_std = 1e-1\n",
    "                #print(time, np.degrees(prefit[:2]))\n",
    "\n",
    "\n",
    "            #Prep for next time step\n",
    "            X_RV_ensemble = X_ensemble_updated[:prob_dimension, :]\n",
    "            \n",
    "            X_mean = np.sum(X_ensemble, axis=1).reshape(fullState_dimension,1)/num_of_ensembles\n",
    "            \n",
    "            \n",
    "            #save true MSIS density for this time step (which is density at \n",
    "            #beginning of time used to prop for this step)\n",
    "            density_MSIS = \\\n",
    "                filter_functions.calc_MSIS_density(time, X_7elem_mean_updated, day_of_year_init, day_of_month_init, \\\n",
    "                                                    hour_init_UT, month_init, year_init, omega_const, r_earth_const)\n",
    "            density_MSIS = density_MSIS * 1e9  #convert from kg/m**3 to kg/km**3\n",
    "            density_MSIS_array[obsnum] = density_MSIS\n",
    "\n",
    "            obsnum = obsnum + 1 #just to ensure that obsnum is not still zero for second time\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        t_init = t_init + time_step\n",
    "        time = t_init + time_step\n",
    "        counter = counter + 1\n",
    "\n",
    "    \n",
    "    return (X_7elem_mean_updated_list, X_7elem_mean_TU_list, P_list, pre_fit_list, prefit_bounds_list, post_fit_list, \\\n",
    "            density_MSIS_array, X_distribution, density_distribution, lat_lst_array, X_density_ensemble, X_ensemble,\\\n",
    "               xpos_info_array)\n",
    "            \n",
    "            #, est_density_grid_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20700.0\n",
      "obs# 0 obj 1\n",
      "20880.0\n",
      "21060.0\n",
      "21240.0\n",
      "21420.0\n",
      "21600.0\n",
      "21780.0\n"
     ]
    }
   ],
   "source": [
    "#Call EnKF\n",
    "\n",
    "snc_flag = False #***** \n",
    "\n",
    "\n",
    "stop_index = np.where(measurement_array[:,0] >= stop_time)[0][0]\n",
    "den_std = 1e-2\n",
    "\n",
    "\n",
    "TU_array_length = int(measurement_array[stop_index,0]/time_step - initial_sim_index) - 1\n",
    "\n",
    "\n",
    "\n",
    "(X_mean_updated_list_EnKF, X_mean_TU_list_EnKF, P_list_EnKF, pre_fit_list, prefit_bounds_list, post_fit_list_EnKF, \\\n",
    " density_MSIS_array, X_distribution, density_distribution, lat_lst_array, density_state_ensemble, \\\n",
    "                     X_ensemble, xpos_info_array) = \\\n",
    "            execute_enkf(measurement_array, X_RV_ensemble, P_bar_0, R, ensembles_of_density_grid,\\\n",
    "                         prob_dimension, density_dimension, initial_sim_time, den_std, stop_index)\n",
    "    \n",
    "#, est_density_grid_array) = \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print(measurement_array[:100,0])\n",
    "TU_array_length\n",
    "print(np.shape(true_state_all_times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save Results\n",
    "\n",
    "\n",
    "\n",
    "saveFig_bool = True\n",
    "\n",
    "save_results = False\n",
    "\n",
    "save_density_grid = False #save time series of density grid, True if saved the est. density grid within EnKF\n",
    "\n",
    "#\"\"\"\n",
    "\n",
    "if ((save_density_grid == False) and (save_results)):\n",
    "\n",
    "    #generate density grid for final time in order to compare entire final density grid estimate to truth\n",
    "    alt = np.linalg.norm(X_mean_updated_list_EnKF[-1,:3]) - r_earth_const\n",
    "\n",
    "    final_density_grid_truth = filter_functions.gen_one_ensemble(latitude_grid, longitude_grid, alt,\\\n",
    "                                                        day_of_year_init, measurement_array[stop_index-1,0]) #final time\n",
    "   \n",
    "    mydict = {'X_mean_updated_list_EnKF': X_mean_updated_list_EnKF, 'P_list_EnKF': P_list_EnKF, \\\n",
    "          'pre_fit_list': pre_fit_list, 'prefit_bounds_list': prefit_bounds_list, \\\n",
    "              'post_fit_list_EnKF': post_fit_list_EnKF, 'density_MSIS_array': density_MSIS_array, \\\n",
    "          'X_distribution': X_distribution, \n",
    "          'density_distribution': density_distribution, 'lat_lst_array': lat_lst_array,\n",
    "         'final_density_ensemble_est': density_state_ensemble, 'final_X_ensemble': X_ensemble,\\\n",
    "        'true_density_array': true_density_array, \\\n",
    "          'final_density_grid_truth': final_density_grid_truth*1e9, 'measurement_array': measurement_array, \\\n",
    "             'truth_xyz': truth_xyz, 'lat_lst_meas_array': lat_lst_meas_array}\n",
    "\n",
    "    \n",
    "    \n",
    "    output = open('Data Files/Results.pkl', 'wb')\n",
    "    pickle.dump(mydict, output)\n",
    "    output.close()\n",
    "\n",
    "\n",
    "    #MATLAB file\n",
    "\n",
    "    filename = 'Data Files/Results.mat'\n",
    "\n",
    "    import scipy.io\n",
    "    scipy.io.savemat(filename, mdict={'X_mean_updated_list_EnKF': X_mean_updated_list_EnKF, 'P_list_EnKF': P_list_EnKF, \\\n",
    "              'post_fit_list_EnKF': post_fit_list_EnKF, 'density_MSIS_array': density_MSIS_array, \\\n",
    "              'X_distribution': X_distribution, \n",
    "              'density_distribution': density_distribution, 'lat_lst_array': lat_lst_array,\n",
    "             'final_density_ensemble_est': density_state_ensemble, 'final_X_ensemble': X_ensemble,\n",
    "            'true_density_array': true_density_array, \n",
    "            'final_density_grid_truth': final_density_grid_truth*1e9, 'measurement_array': measurement_array, \\\n",
    "             'truth_xyz': truth_xyz, 'lat_lst_meas_array': lat_lst_meas_array})\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "if (save_density_grid and save_results):\n",
    "\n",
    "    final_density_grid_truth_timeSeries = np.zeros((stop_index, num_of_lat, num_of_lon))    \n",
    "\n",
    "    for ii in range(stop_index):\n",
    "\n",
    "        #generate density grid for final time in order to compare entire final density grid estimate to truth\n",
    "        alt = np.linalg.norm(X_mean_updated_list_EnKF[ii,:3]) - r_earth_const\n",
    "        final_density_grid_truth_timeSeries[ii] = filter_functions.gen_one_ensemble(latitude_grid, longitude_grid, alt,\\\n",
    "                                                    day_of_year_init, measurement_array[ii,0]) #final time\n",
    "\n",
    "    #PKL file\n",
    "\n",
    "    mydict = {'X_mean_updated_list_EnKF': X_mean_updated_list_EnKF, 'P_list_EnKF': P_list_EnKF, \\\n",
    "              'post_fit_list_EnKF': post_fit_list_EnKF, 'density_MSIS_array': density_MSIS_array, \\\n",
    "              'X_distribution': X_distribution, \n",
    "              'density_distribution': density_distribution, 'lat_lst_array': lat_lst_array,\n",
    "             'final_density_ensemble_est': density_state_ensemble, 'final_X_ensemble': X_ensemble,\\\n",
    "            'true_density_array': true_density_array, \\\n",
    "              'final_density_grid_truth_timeSeries': final_density_grid_truth_timeSeries*1e9,\n",
    "             'est_density_grid_array': est_density_grid_array, 'measurement_array': measurement_array, \\\n",
    "             'truth_xyz': truth_xyz, 'lat_lst_meas_array': lat_lst_meas_array}\n",
    "\n",
    "    output = open('Figures/Results.pkl', 'wb')\n",
    "    pickle.dump(mydict, output)\n",
    "    output.close()\n",
    "\n",
    "\n",
    "\n",
    "    #MATLAB file\n",
    "\n",
    "    filename = 'Data Files/Results.mat'\n",
    "\n",
    "    import scipy.io\n",
    "    scipy.io.savemat(filename, mdict={'X_mean_updated_list_EnKF': X_mean_updated_list_EnKF, 'P_list_EnKF': P_list_EnKF, \\\n",
    "              'post_fit_list_EnKF': post_fit_list_EnKF, 'density_MSIS_array': density_MSIS_array, \\\n",
    "              'X_distribution': X_distribution, \n",
    "              'density_distribution': density_distribution, 'lat_lst_array': lat_lst_array,\n",
    "             'final_density_ensemble_est': density_state_ensemble, 'final_X_ensemble': X_ensemble,\n",
    "            'true_density_array': true_density_array, \n",
    "            'final_density_grid_truth_timeSeries': final_density_grid_truth_timeSeries*1e9,\n",
    "            'est_density_grid_array': est_density_grid_array, 'measurement_array': measurement_array, \\\n",
    "             'truth_xyz': truth_xyz, 'lat_lst_meas_array': lat_lst_meas_array})\n",
    "\n",
    "    #\"\"\"\n",
    "#'P_full_list': P_full_list,  \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read Data (from saved run) in to Generate Results\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# read python dict containing densities\n",
    "results_file = open('Figures/12Periods_trulyObs_0RAAN/Results.pkl', 'rb') #Results_1Period/Results_558.pkl\n",
    "mydict = pickle.load(results_file)\n",
    "results_file.close()\n",
    "\n",
    "X_mean_updated_list_EnKF = mydict['X_mean_updated_list_EnKF'] \n",
    "P_list_EnKF = mydict['P_list_EnKF'] \n",
    "post_fit_list_EnKF = mydict['post_fit_list_EnKF'] \n",
    "density_MSIS_array = mydict['density_MSIS_array'] \n",
    "X_distribution = mydict['X_distribution'] \n",
    "density_distribution = mydict['density_distribution'] \n",
    "lat_lst_array = mydict['lat_lst_array']\n",
    "final_density_ensemble_est = mydict['final_density_ensemble_est']\n",
    "final_X_ensemble = mydict['final_X_ensemble']\n",
    "true_density_array = mydict['true_density_array']\n",
    "final_density_grid_truth_timeSeries = mydict['final_density_grid_truth_timeSeries']\n",
    "est_density_grid_array = mydict['est_density_grid_array']\n",
    "\n",
    "                \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "times = measurement_array[:stop_index, 0]/(60)\n",
    "\n",
    "time_str = 'Time (minutes)'\n",
    "\n",
    "time_repeated_ensemble = np.repeat(times[:stop_index], num_of_ensembles)\n",
    "time_repeated_X = np.repeat(times[:stop_index], num_of_ensembles)\n",
    "\n",
    "#\"\"\"\n",
    "#calculate the normed density distribution by subtracting the mean of the ensemble from each ensemble   \n",
    "#density_distribution shape = num_of_ensembles x stop_index\n",
    "density_distribution_mean = np.mean(density_distribution[:,:stop_index], axis=0).reshape(1, stop_index)\n",
    "\n",
    "density_distribution_mean_tiled = np.tile(density_distribution_mean, (1, num_of_ensembles))\n",
    "density_distribution_mean_tiled = density_distribution_mean_tiled.reshape(num_of_ensembles, stop_index)\n",
    "\n",
    "density_distribution_normed = density_distribution[:, :stop_index] - density_distribution_mean_tiled\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.semilogy(time_repeated_ensemble, density_distribution_normed.T.flatten())\n",
    "#plt.scatter(time_repeated_ensemble, density_distribution_normed.T.flatten())\n",
    "plt.ylabel(r'Distribution $(kg/km^3)$', fontsize=18)\n",
    "plt.xlabel(time_str, fontsize=18)\n",
    "#plt.ylim(-.5e-3, .5e-3)\n",
    "plt.title('Density Ensemble Distribution (normed by mean)', fontsize=18)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#calculate the normed density distribution by subtracting the truth of the ensemble from each ensemble   \n",
    "#density_distribution shape = num_of_ensembles x stop_index\n",
    "true_density_array = truth_xyz[:,-1]\n",
    "density_distribution_truth = true_density_array[:stop_index].reshape(1, stop_index)\n",
    "density_distribution_normed = density_distribution[:, :stop_index] - \\\n",
    "                    np.tile(density_distribution_truth, (num_of_ensembles, 1))\n",
    "\n",
    "fig_dens_dist, ax = plt.subplots()\n",
    "ax.semilogy(time_repeated_ensemble, density_distribution_normed.T.flatten())\n",
    "#plt.scatter(time_repeated_ensemble, density_distribution_normed.T.flatten())\n",
    "plt.ylabel(r'Distribution $(kg/km^3)$', fontsize=18)\n",
    "plt.xlabel(time_str, fontsize=18)\n",
    "#plt.ylim(-1e-3, 1e-3)\n",
    "plt.title('Density Ensemble Distribution (normed by truth)', fontsize=18)\n",
    "plt.show()\n",
    "#\"\"\"\n",
    "\n",
    "#calculate the normed X pos distribution by subtracting the mean of the ensemble from each ensemble   \n",
    "#X_distribution shape = 6 x num_of_ensembles x stop_index\n",
    "X_distribution_Xpos = X_distribution[0,:,:stop_index]\n",
    "X_distribution_Xpos_mean = np.mean(X_distribution_Xpos, axis=0).reshape(1, stop_index)\n",
    "X_distribution_Xpos_mean_tiled = np.tile(X_distribution_Xpos_mean, (1, num_of_ensembles))\n",
    "X_distribution_Xpos_mean_tiled = X_distribution_Xpos_mean_tiled.reshape(num_of_ensembles, stop_index)\n",
    "X_distribution_Xpos_diff = X_distribution_Xpos - X_distribution_Xpos_mean_tiled\n",
    "\n",
    "fig_X_dist = plt.figure()\n",
    "plt.scatter(time_repeated_X, X_distribution_Xpos_diff.T.flatten())\n",
    "plt.ylabel(r'Distribution (km)', fontsize=18)\n",
    "plt.xlabel(time_str, fontsize=18)\n",
    "#plt.ylim(-1e-4,1e-4)\n",
    "plt.title('State X Position Ensemble Distribution (normed by mean)', fontsize=18)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "''' not normed by correct truth because truth time series is for different objects\n",
    "#calculate the normed X pos distribution by subtracting the truth of the ensemble from each ensemble   \n",
    "#X_distribution shape = 6 x num_of_ensembles x stop_index\n",
    "X_distribution_Xpos_truth = truth_xyz[:stop_index, 0].reshape(1, stop_index)\n",
    "X_distribution_Xpos_truth_normed = X_distribution_Xpos - np.tile(X_distribution_Xpos_truth, (num_of_ensembles, 1))\n",
    "\n",
    "fig_X_dist = plt.figure()\n",
    "plt.scatter(time_repeated_ensemble, X_distribution_Xpos_truth_normed.T.flatten())\n",
    "plt.ylabel(r'Distribution (km)', fontsize=18)\n",
    "plt.xlabel(time_str, fontsize=18)\n",
    "#plt.ylim(-1e-4, 1e-4)\n",
    "plt.title('State X Position Ensemble Distribution (normed by truth)', fontsize=18)\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "\n",
    "if saveFig_bool:\n",
    "    fig_dens_dist.savefig('Figures/dens_dist.png')\n",
    "    fig_X_dist.savefig('Figures/fig_X_dist.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "times = measurement_array[:stop_index, 0]/(60)\n",
    "plt.scatter(times, xpos_info_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "diff = np.absolute(X_mean_updated_list_EnKF[:stop_index, -1] - \\\n",
    "                                               true_density_array[:stop_index])\n",
    "#plt.plot(times, diff)\n",
    "#print(X_mean_updated_list_EnKF[:stop_index, -1])\n",
    "#true_density_array[:stop_index]\n",
    "\n",
    "stop = 15\n",
    "#plt.scatter(measurement_array[:stop, 2], perc_error[:stop])\n",
    "#plt.scatter(measurement_array[:stop_index, 1], perc_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "time_str = 'Time (minutes)'\n",
    "\n",
    "\n",
    "times = measurement_array[:stop_index, 0]/(60)\n",
    "\n",
    "time_repeated_ensemble = np.repeat(times[:stop_index], num_of_ensembles)\n",
    "time_repeated_X = np.repeat(times[:stop_index], num_of_ensembles)\n",
    "\n",
    "\n",
    "indices_1 = np.where(measurement_array[:stop_index, 1] == 1)[0]\n",
    "indices_2 = np.where(measurement_array[:stop_index, 1] == 2)[0]\n",
    "indices_3 = np.where(measurement_array[:stop_index, 1] == 3)[0]\n",
    "indices_4 = np.where(measurement_array[:stop_index, 1] == 4)[0]\n",
    "\n",
    "\n",
    "#sort by object so that results show all results for one object and then\n",
    "indices = np.argsort(measurement_array[:stop_index,2], axis=0) \n",
    "\n",
    "\n",
    "fig_lat = plt.figure()\n",
    "plt.scatter(times, np.degrees(lat_lst_meas_array[:stop_index, 0]), s=70, c='b',marker='+')\n",
    "plt.scatter(times, np.degrees(lat_lst_array[0,:stop_index]), s=70, c='c',marker='+')\n",
    "plt.ylabel('Degrees', fontsize=18)\n",
    "plt.xlabel(time_str, fontsize=18)\n",
    "plt.title('Latitude', fontsize=18)\n",
    "legend_names = ['Truth', 'EnKF']\n",
    "plt.legend(legend_names, fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "#print(np.degrees(lat_lst_meas_array[:stop_index, 0] - lat_lst_array[0,:stop_index]))\n",
    "\n",
    "\n",
    "\n",
    "fig_LST = plt.figure()\n",
    "plt.scatter(times, np.degrees(lat_lst_meas_array[:stop_index, 1]), s=70, c='b', marker='+')\n",
    "plt.scatter(times, np.degrees(lat_lst_array[1,:stop_index]), s=70, c='c', marker='+')\n",
    "plt.ylabel('Degrees', fontsize=18)\n",
    "plt.xlabel(time_str, fontsize=18)\n",
    "plt.title('LST', fontsize=18)\n",
    "legend_names = ['Truth', 'EnKF']\n",
    "plt.legend(legend_names, fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "#print(np.degrees(lat_lst_meas_array[:stop_index, 1][indices] - lat_lst_array[1,:stop_index][indices]))\n",
    "\n",
    "\n",
    "fig_density_comparison = plt.figure()\n",
    "plt.scatter(times, density_MSIS_array[:stop_index], s=70, c='c', marker='+')\n",
    "plt.scatter(times, true_density_array[:stop_index], s=70, c='b', marker='+')\n",
    "#plt.ylim([-1e-11,1e-11])\n",
    "plt.ylabel(r'$kg/km^3$', fontsize=18)\n",
    "plt.xlabel(time_str, fontsize=18)\n",
    "plt.ylim([0,2e-2])\n",
    "plt.title('MSIS Filter Density v. True Density', fontsize=18)\n",
    "legend_names = ['MSIS', 'Meas Truth']\n",
    "plt.legend(legend_names, fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig_density_comparison = plt.figure()\n",
    "plt.scatter(times, density_MSIS_array[:stop_index], s=70, c='c', marker='+')\n",
    "plt.scatter(times, X_mean_updated_list_EnKF[:stop_index, -1], s=70, c='b', marker='+')\n",
    "#plt.ylim([-1e-11,1e-11])\n",
    "plt.ylabel(r'$kg/km^3$', fontsize=18)\n",
    "plt.xlabel(time_str, fontsize=18)\n",
    "plt.ylim([0,2e-2])\n",
    "plt.title('Estimated v. MSIS Filter Density', fontsize=18)\n",
    "legend_names = ['MSIS', 'Estimated']\n",
    "plt.legend(legend_names, fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig_density_comparison = plt.figure()\n",
    "plt.scatter(times, true_density_array[:stop_index], s=70, c='b', marker='+')\n",
    "plt.scatter(times, X_mean_updated_list_EnKF[:stop_index, -1], s=70, c='c', marker='+')\n",
    "#plt.ylim([-1e-11,1e-11])\n",
    "plt.ylabel(r'$kg/km^3$', fontsize=18)\n",
    "plt.xlabel(time_str, fontsize=18)\n",
    "plt.ylim([0,2e-2])\n",
    "plt.title('Estimated v. True Density', fontsize=18)\n",
    "legend_names = ['True', 'Estimated']\n",
    "plt.legend(legend_names, fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "perc_error = 100 * np.absolute(X_mean_updated_list_EnKF[:stop_index, -1] - \\\n",
    "                                               true_density_array[:stop_index])/true_density_array[:stop_index]\n",
    "\n",
    "fig_percent = plt.figure()\n",
    "plt.scatter(times[indices_1], perc_error[indices_1], s=50, c='m', marker='s')\n",
    "plt.scatter(times[indices_2], perc_error[indices_2], s=50, c='g', marker='^')\n",
    "plt.scatter(times[indices_3], perc_error[indices_3], s=50, c='r', marker='D')\n",
    "plt.scatter(times[indices_4], perc_error[indices_4], s=50, c='k', marker='o')\n",
    "#plt.ylim([0,200])\n",
    "plt.ylabel('Percent', fontsize=18)\n",
    "legend_names = ['Station 1', 'Station 2', 'Station 3']\n",
    "plt.legend(legend_names, fontsize=10)\n",
    "plt.xlabel(time_str, fontsize=18)\n",
    "plt.title('Percent Error of Estimated Density', fontsize=18)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "mean = np.mean(X_mean_updated_list_EnKF[:stop_index, -1])\n",
    "est_density_array_normalized = X_mean_updated_list_EnKF[:stop_index, -1] - mean\n",
    "\n",
    "fig_est = plt.figure()\n",
    "plt.scatter(times, est_density_array_normalized[:stop_index], s=70, c='b', marker='+')\n",
    "plt.ylabel(r'$kg/km^3$', fontsize=18)\n",
    "plt.xlabel(time_str, fontsize=18)\n",
    "plt.title('Normalized Estimated Density', fontsize=18)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "if saveFig_bool:\n",
    "    fig_lat.savefig('Figures/latitude.png')\n",
    "    fig_LST.savefig('Figures/LST.png')\n",
    "    fig_density_comparison.savefig('Figures/density_comparison.png')\n",
    "    fig_percent.savefig('Figures/percent.png')\n",
    "    fig_est.savefig('Figures/est_density.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Rank Histograms\n",
    "\n",
    "\n",
    "#Density\n",
    "\n",
    "rank_array = np.zeros((stop_index, 1))\n",
    "\n",
    "for ii in range(stop_index):\n",
    "    \n",
    "    new_array = np.append(density_distribution[:, ii], true_density_array[ii])\n",
    "    \n",
    "    rank = scipy.stats.rankdata(new_array, method='min')[-1]\n",
    "    \n",
    "    rank_array[ii] = rank\n",
    "    \n",
    "indices = np.where(rank_array == 1)[0]\n",
    "indices1 = np.where(rank_array == 450)[0]\n",
    "print(times[indices1])\n",
    "print(len(indices))\n",
    "\n",
    "\n",
    "\n",
    "fig_rank_hist = plt.figure()\n",
    "plt.hist(rank_array, bins=451)\n",
    "plt.ylabel('Frequency', fontsize=18)\n",
    "plt.xlabel('Rank', fontsize=18)\n",
    "plt.title('Density Ensemble Rank Histogram', fontsize=18)\n",
    "plt.xlim([0,451])\n",
    "plt.show()\n",
    "\n",
    "fig_rank_hist.savefig('Figures/rank_hist.png')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rank_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_display_results_pre(pre_fit_list, prefit_bounds_list, measurement_array, R, meas_type, stop_index, \\\n",
    "                             saveFig_bool, time_str):\n",
    "    \n",
    "    \n",
    "    rms_1 = 'Range ='\n",
    "    unit_1 = 'km'\n",
    "    ylabel_1 = 'Range Residuals (km)'\n",
    "    title_1 = 'Range pre-fit Residuals'\n",
    "    save_fig_1 = 'prefit_range.png'\n",
    "    rms_2 = 'Range Rate ='\n",
    "    unit_2 = 'km/s'\n",
    "    ylabel_2 = 'Range Rate Residuals (km/s)'\n",
    "    title_2 = 'Range Rate pre-fit Residuals'\n",
    "    save_fig_2 = 'prefit_rangeRate.png'\n",
    "    pre_fit_list_new = copy.deepcopy(pre_fit_list)\n",
    "         \n",
    "    if (meas_type == 2) or (meas_type == 3):\n",
    "        rms_1 = 'Azimuth ='\n",
    "        unit_1 = 'degrees'\n",
    "        rms_2 = 'Elevation ='\n",
    "        unit_2 = 'degrees'\n",
    "        pre_fit_list_new[:, 0] = np.degrees(pre_fit_list[:, 0])\n",
    "        pre_fit_list_new[:, 1] = np.degrees(pre_fit_list[:, 1])\n",
    "        ylabel_1 = 'Azimuth Residuals (degrees)'\n",
    "        title_1 = 'Azimuth pre-fit Residuals'\n",
    "        save_fig_1 = 'prefit_az.png'\n",
    "        ylabel_2 = 'Elevation Residuals (degrees)'\n",
    "        title_2 = 'Elevation pre-fit Residuals'\n",
    "        save_fig_2 = 'prefit_el_rate.png'\n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "    times = measurement_array[:stop_index,0]/(60)\n",
    "    \n",
    "    indices_1 = np.where(measurement_array[:stop_index, 1] == 1)[0]\n",
    "    indices_2 = np.where(measurement_array[:stop_index, 1] == 2)[0]\n",
    "    indices_3 = np.where(measurement_array[:stop_index, 1] == 3)[0]\n",
    "\n",
    "    \n",
    "    \n",
    "    #pre-fit\n",
    "    print('pre-fit RMS:')\n",
    "    pre_fit_1_list_4RMS = pre_fit_list_new[:, 0]\n",
    "    prefit_1_rms = np.sqrt(np.mean(np.square(pre_fit_1_list_4RMS)))\n",
    "    print(rms_1, \"%.4f\" % prefit_1_rms, unit_1)\n",
    "\n",
    "    pre_fit_2_list_4RMS = pre_fit_list_new[:, 1]\n",
    "    prefit_2_rms = np.sqrt(np.mean(np.square(pre_fit_2_list_4RMS)))\n",
    "    print(rms_2, \"%.4f\" % prefit_2_rms, unit_2)\n",
    "    \n",
    "    if meas_type == 3:\n",
    "        pre_fit_3_list_4RMS = pre_fit_list_new[:, 2]\n",
    "        prefit_3_rms = np.sqrt(np.mean(np.square(pre_fit_3_list_4RMS)))\n",
    "        print('Range =', \"%.3f\" % prefit_3_rms, 'km')\n",
    "    \n",
    "    \n",
    "    covar_env_upper1 = np.degrees(abs(prefit_bounds_list[:stop_index, 0]))*3\n",
    "    covar_env_upper2 = np.degrees(abs(prefit_bounds_list[:stop_index, 1]))*3\n",
    "\n",
    "   \n",
    "    #pre-fit Residuals\n",
    "    fig_preFit_az = plt.figure()\n",
    "    plt.plot(times, covar_env_upper1, label='_nolegend_', c='g')\n",
    "    plt.plot(times, -covar_env_upper1, label='_nolegend_', c='g')\n",
    "    plt.scatter(times[indices_1], pre_fit_list_new[indices_1, 0], s=50, c='m', marker='s')\n",
    "    plt.scatter(times[indices_2], pre_fit_list_new[indices_2, 0], s=50, c='g', marker='^')\n",
    "    plt.scatter(times[indices_3], pre_fit_list_new[indices_3, 0], s=50, c='r', marker='D')\n",
    "    plt.scatter(times[indices_4], pre_fit_list_new[indices_4, 0], s=50, c='k', marker='o')\n",
    "    plt.ylabel(ylabel_1, fontsize=18)\n",
    "    plt.xlabel(time_str, fontsize=18)\n",
    "    plt.title(title_1, fontsize=18)\n",
    "    legend_names = ['Station 1', 'Station 2', 'Station 3']\n",
    "    plt.legend(legend_names, fontsize=10)\n",
    "    plt.xlim([times[0] - 5, times[-1] + 5])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    #fig.savefig(save_fig_1)\n",
    "\n",
    "    fig_preFit_el = plt.figure()\n",
    "    plt.plot(times, covar_env_upper2, label='_nolegend_', c='g')\n",
    "    plt.plot(times, -covar_env_upper2, label='_nolegend_', c='g')\n",
    "    plt.scatter(times[indices_1], pre_fit_list_new[indices_1, 1], s=50, c='m', marker='s')\n",
    "    plt.scatter(times[indices_2], pre_fit_list_new[indices_2, 1], s=50, c='g', marker='^')\n",
    "    plt.scatter(times[indices_3], pre_fit_list_new[indices_3, 1], s=50, c='r', marker='D')\n",
    "    plt.scatter(times[indices_4], pre_fit_list_new[indices_4, 1], s=50, c='k', marker='o')\n",
    "    plt.ylabel(ylabel_2, fontsize=18)\n",
    "    plt.xlabel(time_str, fontsize=18)\n",
    "    plt.title(title_2, fontsize=18)\n",
    "    plt.legend(legend_names, fontsize=10)\n",
    "    #plt.ylim([-.01,.01])\n",
    "    plt.xlim([times[0] - 5, times[-1] + 5])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    #fig.savefig(save_fig_2)\n",
    "    \n",
    "    if meas_type == 3:\n",
    "        \n",
    "        covar_env_upper3 = abs(prefit_bounds_list[:stop_index, 2])*3\n",
    "        \n",
    "        fig_preFit_range = plt.figure()\n",
    "        plt.plot(times, covar_env_upper3, label='_nolegend_', c='g')\n",
    "        plt.plot(times, -covar_env_upper3, label='_nolegend_', c='g')\n",
    "        plt.scatter(times[indices_1], pre_fit_list_new[indices_1, 2], s=50, c='m', marker='s')\n",
    "        plt.scatter(times[indices_2], pre_fit_list_new[indices_2, 2], s=50, c='g', marker='^')\n",
    "        plt.scatter(times[indices_3], pre_fit_list_new[indices_3, 2], s=50, c='r', marker='D')\n",
    "        plt.scatter(times[indices_4], pre_fit_list_new[indices_4, 2], s=50, c='k', marker='o')\n",
    "        plt.ylabel('Range Residuals (km)', fontsize=18)\n",
    "        plt.xlabel(time_str, fontsize=18)\n",
    "        plt.title('Range pre-fit Residuals', fontsize=18)\n",
    "        plt.legend(legend_names, fontsize=10)\n",
    "        #plt.ylim([-.01,.01])\n",
    "        plt.xlim([times[0] - 5, times[-1] + 5])\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        #fig.savefig('prefit_range.png')\n",
    "    \n",
    "    if saveFig_bool:\n",
    "        fig_preFit_az.savefig('Figures/preFit_az.png')\n",
    "        fig_preFit_el.savefig('Figures/preFit_el.png')\n",
    "        fig_preFit_range.savefig('Figures/preFit_range.png')\n",
    "        \n",
    "        \n",
    "def calc_display_results(post_fit_list, measurement_array, R, meas_type, stop_index, saveFig_bool, time_str):\n",
    "    \n",
    "    \n",
    "    rms_1 = 'Range ='\n",
    "    unit_1 = 'km'\n",
    "    ylabel_1 = 'Range Residuals (km)'\n",
    "    title_1 = 'Range Post-fit Residuals'\n",
    "    save_fig_1 = 'postfit_range.png'\n",
    "    rms_2 = 'Range Rate ='\n",
    "    unit_2 = 'km/s'\n",
    "    ylabel_2 = 'Range Rate Residuals (km/s)'\n",
    "    title_2 = 'Range Rate Post-fit Residuals'\n",
    "    save_fig_2 = 'postfit_rangeRate.png'\n",
    "    post_fit_list_new = copy.deepcopy(post_fit_list)\n",
    "         \n",
    "    if (meas_type == 2) or (meas_type == 3):\n",
    "        rms_1 = 'Azimuth ='\n",
    "        unit_1 = 'degrees'\n",
    "        rms_2 = 'Elevation ='\n",
    "        unit_2 = 'degrees'\n",
    "        post_fit_list_new[:, 0] = np.degrees(post_fit_list[:, 0])\n",
    "        post_fit_list_new[:, 1] = np.degrees(post_fit_list[:, 1])\n",
    "        ylabel_1 = 'Azimuth Residuals (degrees)'\n",
    "        title_1 = 'Azimuth Post-fit Residuals'\n",
    "        save_fig_1 = 'postfit_az.png'\n",
    "        ylabel_2 = 'Elevation Residuals (degrees)'\n",
    "        title_2 = 'Elevation Post-fit Residuals'\n",
    "        save_fig_2 = 'postfit_el_rate.png'\n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "    times = measurement_array[:stop_index,0]/(60)\n",
    "    \n",
    "    indices_1 = np.where(measurement_array[:stop_index, 1] == 1)[0]\n",
    "    indices_2 = np.where(measurement_array[:stop_index, 1] == 2)[0]\n",
    "    indices_3 = np.where(measurement_array[:stop_index, 1] == 3)[0]\n",
    "    indices_4 = np.where(measurement_array[:stop_index, 1] == 4)[0]\n",
    "\n",
    "    \n",
    "    \n",
    "    #Post-fit\n",
    "    print('Post-fit RMS:')\n",
    "    post_fit_1_list_4RMS = post_fit_list_new[:, 0]\n",
    "    postfit_1_rms = np.sqrt(np.mean(np.square(post_fit_1_list_4RMS)))\n",
    "    print(rms_1, \"%.4f\" % postfit_1_rms, unit_1)\n",
    "\n",
    "    post_fit_2_list_4RMS = post_fit_list_new[:, 1]\n",
    "    postfit_2_rms = np.sqrt(np.mean(np.square(post_fit_2_list_4RMS)))\n",
    "    print(rms_2, \"%.4f\" % postfit_2_rms, unit_2)\n",
    "    \n",
    "    if meas_type == 3:\n",
    "        post_fit_3_list_4RMS = post_fit_list_new[:, 2]\n",
    "        postfit_3_rms = np.sqrt(np.mean(np.square(post_fit_3_list_4RMS)))\n",
    "        print('Range =', \"%.5f\" % postfit_3_rms, 'km')\n",
    "    \n",
    "    \n",
    "    covar_env_upper1 = np.ones((stop_index)) * np.degrees(np.sqrt(abs(R[0, 0])))*3\n",
    "    covar_env_upper2 = np.ones((stop_index)) * np.degrees(np.sqrt(abs(R[1, 1])))*3\n",
    "\n",
    "   \n",
    "    #Post-fit Residuals\n",
    "    fig_postFit_az = plt.figure()\n",
    "    plt.plot(times, covar_env_upper1, label='_nolegend_', c='g')\n",
    "    plt.plot(times, -covar_env_upper1, label='_nolegend_', c='g')\n",
    "    plt.scatter(times[indices_1], post_fit_list_new[indices_1, 0], s=50, c='m', marker='s')\n",
    "    plt.scatter(times[indices_2], post_fit_list_new[indices_2, 0], s=50, c='g', marker='^')\n",
    "    plt.scatter(times[indices_3], post_fit_list_new[indices_3, 0], s=50, c='r', marker='D')\n",
    "    plt.scatter(times[indices_4], post_fit_list_new[indices_4, 0], s=50, c='k', marker='o')\n",
    "    plt.ylabel(ylabel_1, fontsize=18)\n",
    "    plt.xlabel(time_str, fontsize=18)\n",
    "    plt.title(title_1, fontsize=18)\n",
    "    legend_names = ['Station 1', 'Station 2', 'Station 3']\n",
    "    plt.legend(legend_names, fontsize=10)\n",
    "    plt.xlim([times[0] - 5, times[-1] + 5])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    #fig.savefig(save_fig_1)\n",
    "\n",
    "    fig_postFit_el = plt.figure()\n",
    "    plt.plot(times, covar_env_upper2, label='_nolegend_', c='g')\n",
    "    plt.plot(times, -covar_env_upper2, label='_nolegend_', c='g')\n",
    "    plt.scatter(times[indices_1], post_fit_list_new[indices_1, 1], s=50, c='m', marker='s')\n",
    "    plt.scatter(times[indices_2], post_fit_list_new[indices_2, 1], s=50, c='g', marker='^')\n",
    "    plt.scatter(times[indices_3], post_fit_list_new[indices_3, 1], s=50, c='r', marker='D')\n",
    "    plt.scatter(times[indices_4], post_fit_list_new[indices_4, 1], s=50, c='k', marker='o')\n",
    "    plt.ylabel(ylabel_2, fontsize=18)\n",
    "    plt.xlabel(time_str, fontsize=18)\n",
    "    plt.title(title_2, fontsize=18)\n",
    "    plt.legend(legend_names, fontsize=10)\n",
    "    #plt.ylim([-.01,.01])\n",
    "    plt.xlim([times[0] - 5, times[-1] + 5])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    #fig.savefig(save_fig_2)\n",
    "    \n",
    "    if meas_type == 3:\n",
    "        \n",
    "        covar_env_upper3 = np.ones((stop_index)) * np.sqrt(abs(R[2, 2]))*3\n",
    "        \n",
    "        fig_postFit_range = plt.figure()\n",
    "        plt.plot(times, covar_env_upper3, label='_nolegend_', c='g')\n",
    "        plt.plot(times, -covar_env_upper3, label='_nolegend_', c='g')\n",
    "        plt.scatter(times[indices_1], post_fit_list_new[indices_1, 2], s=50, c='m', marker='s')\n",
    "        plt.scatter(times[indices_2], post_fit_list_new[indices_2, 2], s=50, c='g', marker='^')\n",
    "        plt.scatter(times[indices_3], post_fit_list_new[indices_3, 2], s=50, c='r', marker='D')\n",
    "        plt.scatter(times[indices_4], post_fit_list_new[indices_4, 2], s=50, c='k', marker='o')\n",
    "        plt.ylabel('Range Residuals (km)', fontsize=18)\n",
    "        plt.xlabel(time_str, fontsize=18)\n",
    "        plt.title('Range Post-fit Residuals', fontsize=18)\n",
    "        legend_names = ['Station 1', 'Station 2']\n",
    "        plt.legend(legend_names, fontsize=10)\n",
    "        #plt.ylim([-.01,.01])\n",
    "        plt.xlim([times[0] - 5, times[-1] + 5])\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        #fig.savefig('postfit_range.png')\n",
    "    \n",
    "    if saveFig_bool:\n",
    "        fig_postFit_az.savefig('Figures/postFit_az.png')\n",
    "        fig_postFit_el.savefig('Figures/postFit_el.png')\n",
    "        fig_postFit_range.savefig('Figures/postFit_range.png')\n",
    "        \n",
    "    \n",
    "    \n",
    "def plot_error_covar_xref(P_list, x_ref_updated_list, x_ref_TU_list, obs_data_truth, density_truth, density_range, x_range, y_range, \\\n",
    "                          z_range, xv_range, yv_range, zv_range, measurement_array, time, stop_index, \\\n",
    "                          saveFig_bool, time_str, title_str, num_of_objects):\n",
    "    \n",
    "    \n",
    "    times = time[:stop_index]/(60)\n",
    "    \n",
    "    initial_filter_index = initial_sim_index + 1\n",
    "    \n",
    "\n",
    "\n",
    "    indices_1 = np.where(measurement_array[:stop_index, 1] == 1)[0]\n",
    "    indices_2 = np.where(measurement_array[:stop_index, 1] == 2)[0]\n",
    "    indices_3 = np.where(measurement_array[:stop_index, 1] == 3)[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    density_covar_env_upper = np.sqrt(abs(P_list[:stop_index, -1, -1]))*3\n",
    "    density_covar_env_lower = -density_covar_env_upper\n",
    "    density_error = x_ref_updated_list[:stop_index,-1] - density_truth[:stop_index]\n",
    "        \n",
    "    \n",
    "    #error_pos_norm = np.sqrt(x_error**2 + y_error**2 + z_error**2)\n",
    "    error_density_rms_3D = np.sqrt(np.mean(np.square(density_error)))\n",
    "    print('Density RMS =', \"%.5f\" % error_density_rms_3D, r'$(kg/km^3)$')\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    for ii in range(num_of_objects):\n",
    "        \n",
    "        \n",
    "        \n",
    "        #based on object ID, calculate indices in the state for this update\n",
    "        ID = ii + 1\n",
    "        obj_index_begin = (ID-1)*6\n",
    "        obj_index_end = ID*6 - 1\n",
    "        object_num = ID\n",
    "        \n",
    "        \n",
    "        x_pos_index = obj_index_begin\n",
    "        y_pos_index = obj_index_begin + 1\n",
    "        z_pos_index = obj_index_begin + 2\n",
    "        x_vel_index = obj_index_begin + 3 \n",
    "        y_vel_index = obj_index_begin + 4\n",
    "        z_vel_index = obj_index_begin + 5\n",
    "       \n",
    "        \n",
    "        \n",
    "        \n",
    "        #prep all times (including no-measurement times) results\n",
    "        ind = np.where(true_state_all_times[:,1] == ID)\n",
    "        true_all_sim_times = true_state_all_times[ind][(initial_filter_index):(initial_filter_index+TU_array_length),:]\n",
    "\n",
    "        true_dens = true_all_sim_times[:,-1]\n",
    "\n",
    "        times_TU = true_all_sim_times[:,0]/60\n",
    "\n",
    "        \n",
    "        density_error_TU = x_ref_TU_list[:,(prob_dimension+ii)] - true_dens\n",
    "        \n",
    "        indices_1 = np.where(measurement_array[:stop_index, 1] == 1)[0]\n",
    "        indices_2 = np.where(measurement_array[:stop_index, 1] == 2)[0]\n",
    "        indices_3 = np.where(measurement_array[:stop_index, 1] == 3)[0]\n",
    "        \n",
    "\n",
    "        #Density\n",
    "        fig_dens = plt.figure()\n",
    "        plt.scatter(times_TU, density_error_TU, label='_nolegend_', s=20, c='k', marker='.')\n",
    "        plt.plot(times, density_covar_env_upper, label='_nolegend_', c='g')\n",
    "        plt.plot(times, density_covar_env_lower, label='_nolegend_', c='g')\n",
    "        plt.scatter(times[indices_1], density_error[indices_1], s=50, c='m', marker='s')\n",
    "        plt.scatter(times[indices_2], density_error[indices_2], s=50, c='g', marker='^')\n",
    "        plt.scatter(times[indices_3], density_error[indices_3], s=50, c='r', marker='D')\n",
    "        plt.scatter(times[indices_4], density_error[indices_4], s=50, c='k', marker='o')\n",
    "        plt.ylabel(r'$(kg/km^3)$', fontsize=18)\n",
    "        plt.xlabel(time_str, fontsize=18)\n",
    "        legend_names = ['Station 1', 'Station 2', 'Station 3']\n",
    "        plt.legend(legend_names, fontsize=10)\n",
    "        plt.title('Obj ' + str(object_num) + ' ' + title_str + ' Density Error & Covariance Envelope', fontsize=18)\n",
    "        plt.ylim([-density_range,density_range])\n",
    "        plt.xlim([times[0] - 5, times[-1] + 5])\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        \n",
    "        #find this object's results\n",
    "        indices = np.where(true_state_all_times[:,1] == ID)[0]  \n",
    "        true_state_all_times_thisObj = true_state_all_times[np.r_[indices,:]]\n",
    "        \n",
    "        true_all_sim_times_thisObj = true_state_all_times_thisObj\\\n",
    "                            [(initial_filter_index):(initial_filter_index+TU_array_length),:]\n",
    "        \n",
    "        times_all = true_state_all_times_thisObj[(initial_filter_index):(initial_filter_index+TU_array_length),0]/60\n",
    "        \n",
    "\n",
    "        x_error_all = x_ref_TU_list[:,x_pos_index] - true_all_sim_times_thisObj[:,2]\n",
    "        y_error_all = x_ref_TU_list[:,y_pos_index] - true_all_sim_times_thisObj[:,3]\n",
    "        z_error_all = x_ref_TU_list[:,z_pos_index] - true_all_sim_times_thisObj[:,4]\n",
    "        x_vel_error_all = x_ref_TU_list[:,x_vel_index] - true_all_sim_times_thisObj[:,5]\n",
    "        y_vel_error_all = x_ref_TU_list[:,y_vel_index] - true_all_sim_times_thisObj[:,6]\n",
    "        z_vel_error_all = x_ref_TU_list[:,z_vel_index] - true_all_sim_times_thisObj[:,7]\n",
    "        \n",
    "        \n",
    "        \n",
    "        indices_obj = np.where(measurement_array[:stop_index, 2] == ID)[0]\n",
    "        \n",
    "        if len(indices_obj) != 0:\n",
    "        \n",
    "            indices_1 = np.where(measurement_array[:stop_index, 1][np.r_[indices_obj]] == 1)[0]\n",
    "            indices_2 = np.where(measurement_array[:stop_index, 1][np.r_[indices_obj]] == 2)[0]\n",
    "            indices_3 = np.where(measurement_array[:stop_index, 1][np.r_[indices_obj]] == 3)[0]\n",
    "\n",
    "            times_obj = times[:stop_index][np.r_[indices_obj]]\n",
    "\n",
    "\n",
    "            #Compare to the Truth Data : Estimation Errors------\n",
    "\n",
    "            x_covar_env_upper = np.sqrt(abs(P_list[:stop_index, x_pos_index, x_pos_index]))*3\n",
    "            x_covar_env_lower = -x_covar_env_upper\n",
    "            x_error = x_ref_updated_list[:stop_index,x_pos_index][np.r_[indices_obj]] - \\\n",
    "                                obs_data_truth[:stop_index, 0][np.r_[indices_obj]]\n",
    "        \n",
    "\n",
    "            y_covar_env_upper = np.sqrt(abs(P_list[:stop_index, y_pos_index, y_pos_index]))*3\n",
    "            y_covar_env_lower = -y_covar_env_upper\n",
    "            y_error = x_ref_updated_list[:stop_index,y_pos_index][np.r_[indices_obj]] - \\\n",
    "                                 obs_data_truth[:stop_index, 1][np.r_[indices_obj]]\n",
    "\n",
    "            z_covar_env_upper = np.sqrt(abs(P_list[:stop_index, z_pos_index, z_pos_index]))*3\n",
    "            z_covar_env_lower = -z_covar_env_upper\n",
    "            z_error = x_ref_updated_list[:stop_index,z_pos_index][np.r_[indices_obj]] - \\\n",
    "                                 obs_data_truth[:stop_index, 2][np.r_[indices_obj]]\n",
    "\n",
    "            \n",
    "\n",
    "            print('~~Obj ' + str(object_num) + ' Results~~')\n",
    "            print('Position RMS:')\n",
    "            error_x_pos_rms_3D = np.sqrt(np.mean(np.square(x_error)))\n",
    "            print('X =', \"%.4f\" % error_x_pos_rms_3D, 'km')\n",
    "\n",
    "            error_y_pos_rms_3D = np.sqrt(np.mean(np.square(y_error)))\n",
    "            print('Y =', \"%.4f\" % error_y_pos_rms_3D, 'km')\n",
    "\n",
    "            error_z_pos_rms_3D = np.sqrt(np.mean(np.square(z_error)))\n",
    "            print('Z =', \"%.4f\" % error_z_pos_rms_3D, 'km')\n",
    "\n",
    "            pos_rms = np.sqrt(error_x_pos_rms_3D**2 + error_y_pos_rms_3D**2 + error_z_pos_rms_3D**2)\n",
    "            print('Overall =', \"%.4f\" % pos_rms, 'km')\n",
    "\n",
    "            #x Velocity\n",
    "            x_dot_covar_env_upper = np.sqrt(abs(P_list[:stop_index, x_vel_index, x_vel_index]))*3\n",
    "            x_dot_covar_env_lower = -x_dot_covar_env_upper\n",
    "            x_vel_error = x_ref_updated_list[:stop_index,x_vel_index][np.r_[indices_obj]]\\\n",
    "                                        - obs_data_truth[:stop_index, 3][np.r_[indices_obj]]\n",
    "\n",
    "            #y Velocity\n",
    "            y_dot_covar_env_upper = np.sqrt(abs(P_list[:stop_index, y_vel_index, y_vel_index]))*3\n",
    "            y_dot_covar_env_lower = -y_dot_covar_env_upper\n",
    "            y_vel_error = x_ref_updated_list[:stop_index,y_vel_index][np.r_[indices_obj]]\\\n",
    "                                        - obs_data_truth[:stop_index, 4][np.r_[indices_obj]]        \n",
    "\n",
    "            #z Velocity\n",
    "            z_dot_covar_env_upper = np.sqrt(abs(P_list[:stop_index, z_vel_index, z_vel_index]))*3\n",
    "            z_dot_covar_env_lower = -z_dot_covar_env_upper\n",
    "            z_vel_error = x_ref_updated_list[:stop_index,z_vel_index][np.r_[indices_obj]]\\\n",
    "                                        - obs_data_truth[:stop_index, 5][np.r_[indices_obj]]       \n",
    "\n",
    "            print('\\nVelocity RMS:')\n",
    "            error_x_vel_rms_3D = np.sqrt(np.mean(np.square(x_vel_error)))\n",
    "            print('X =', \"%.6f\" % error_x_vel_rms_3D, 'km/second')\n",
    "\n",
    "            error_y_vel_rms_3D = np.sqrt(np.mean(np.square(y_vel_error)))\n",
    "            print('Y =', \"%.6f\" % error_y_vel_rms_3D, 'km/second')\n",
    "\n",
    "            error_z_vel_rms_3D = np.sqrt(np.mean(np.square(z_vel_error)))\n",
    "            print('Z =', \"%.6f\" % error_z_vel_rms_3D, 'km/second')\n",
    "\n",
    "            vel_rms = np.sqrt(error_x_vel_rms_3D**2 + error_y_vel_rms_3D**2 + error_z_vel_rms_3D**2)\n",
    "            print('Overall =', \"%.6f\" % vel_rms, 'km/s')\n",
    "\n",
    "\n",
    "\n",
    "            #x Position\n",
    "            fig_xpos = plt.figure()\n",
    "            plt.scatter(times_all, x_error_all, label='_nolegend_', s=50, c='k', marker='.')\n",
    "            plt.plot(times, x_covar_env_upper, label='_nolegend_', c='g')\n",
    "            plt.plot(times, x_covar_env_lower, label='_nolegend_', c='g')\n",
    "            plt.scatter(times_obj[indices_1], x_error[indices_1], s=50, c='m', marker='s')\n",
    "            plt.scatter(times_obj[indices_2], x_error[indices_2], s=50, c='g', marker='^')\n",
    "            plt.scatter(times_obj[indices_3], x_error[indices_3], s=50, c='r', marker='D')\n",
    "            plt.scatter(times_obj[indices_4], x_error[indices_4], s=50, c='k', marker='o')\n",
    "            plt.ylabel('km', fontsize=18)\n",
    "            plt.xlabel(time_str, fontsize=18)\n",
    "            plt.legend(legend_names, fontsize=10)\n",
    "            plt.title('Obj ' + str(object_num) + ' ' + title_str + ' X Position Error & Covariance Envelope', fontsize=18)\n",
    "            plt.ylim([-x_range,x_range])\n",
    "            plt.xlim([times[0] - 5, times[-1] + 5])\n",
    "            plt.show()\n",
    "\n",
    "            #y Position \n",
    "            fig_ypos = plt.figure()\n",
    "            plt.scatter(times_all, y_error_all, label='_nolegend_', s=50, c='k', marker='.')\n",
    "            plt.plot(times, y_covar_env_upper, label='_nolegend_', c='g')\n",
    "            plt.plot(times, y_covar_env_lower, label='_nolegend_', c='g')\n",
    "            plt.scatter(times_obj[indices_1], y_error[indices_1], s=50, c='m', marker='s')\n",
    "            plt.scatter(times_obj[indices_2], y_error[indices_2], s=50, c='g', marker='^')\n",
    "            plt.scatter(times_obj[indices_3], y_error[indices_3], s=50, c='r', marker='D')\n",
    "            plt.scatter(times_obj[indices_4], y_error[indices_4], s=50, c='k', marker='o')\n",
    "            plt.ylabel('km', fontsize=18)\n",
    "            plt.xlabel(time_str, fontsize=18)\n",
    "            plt.legend(legend_names, fontsize=10)\n",
    "            plt.title('Obj ' + str(object_num) + ' ' + title_str + ' Y Position Error & Covariance Envelope', \\\n",
    "                      fontsize=18)\n",
    "            plt.ylim([-y_range,y_range])\n",
    "            plt.xlim([times[0] - 5, times[-1] + 5])\n",
    "            plt.show()\n",
    "            #fig.savefig('y_pos_error.png')\n",
    "\n",
    "            #z Position\n",
    "            fig_zpos = plt.figure()\n",
    "            plt.scatter(times_all, z_error_all, label='_nolegend_', s=50, c='k', marker='.')\n",
    "            plt.plot(times, z_covar_env_upper, label='_nolegend_', c='g')\n",
    "            plt.plot(times, z_covar_env_lower, label='_nolegend_', c='g')\n",
    "            plt.scatter(times_obj[indices_1], z_error[indices_1], s=50, c='m', marker='s')\n",
    "            plt.scatter(times_obj[indices_2], z_error[indices_2], s=50, c='g', marker='^')\n",
    "            plt.scatter(times_obj[indices_3], z_error[indices_3], s=50, c='r', marker='D')\n",
    "            plt.scatter(times_obj[indices_4], z_error[indices_4], s=50, c='k', marker='o')\n",
    "            plt.ylabel('km', fontsize=18)\n",
    "            plt.xlabel(time_str, fontsize=18)\n",
    "            plt.legend(legend_names, fontsize=10)\n",
    "            plt.title('Obj ' + str(object_num) + ' ' + title_str + ' Z Position Error & Covariance Envelope', \\\n",
    "                      fontsize=18)\n",
    "            plt.ylim([-z_range,z_range])\n",
    "            plt.xlim([times[0] - 5, times[-1] + 5])\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "\n",
    "            fig_xvel = plt.figure()\n",
    "            plt.scatter(times_all, x_vel_error_all, label='_nolegend_', s=50, c='k', marker='.')\n",
    "            plt.plot(times, x_dot_covar_env_upper, label='_nolegend_', c='g')\n",
    "            plt.plot(times, x_dot_covar_env_lower, label='_nolegend_', c='g')\n",
    "            plt.scatter(times_obj[indices_1], x_vel_error[indices_1], s=50, c='m', marker='s')\n",
    "            plt.scatter(times_obj[indices_2], x_vel_error[indices_2], s=50, c='g', marker='^')\n",
    "            plt.scatter(times_obj[indices_3], x_vel_error[indices_3], s=50, c='r', marker='D')\n",
    "            plt.scatter(times_obj[indices_4], x_vel_error[indices_4], s=50, c='k', marker='o')\n",
    "            plt.ylabel('km/second', fontsize=18)\n",
    "            plt.xlabel(time_str, fontsize=18)\n",
    "            plt.legend(legend_names, fontsize=10)\n",
    "            plt.title('Obj ' + str(object_num) + ' ' + title_str + ' X Velocity Error & Covariance Envelope', \\\n",
    "                      fontsize=18)\n",
    "            plt.ylim([-xv_range,xv_range])\n",
    "            plt.xlim([times[0] - 5, times[-1] + 5])\n",
    "            plt.show()\n",
    "            #fig.savefig('x_vel_error.png')\n",
    "\n",
    "\n",
    "\n",
    "            fig_yvel = plt.figure()\n",
    "            plt.scatter(times_all, y_vel_error_all, label='_nolegend_', s=50, c='k', marker='.')\n",
    "            plt.plot(times, y_dot_covar_env_upper, label='_nolegend_', c='g')\n",
    "            plt.plot(times, y_dot_covar_env_lower, label='_nolegend_', c='g')\n",
    "            plt.scatter(times_obj[indices_1], y_vel_error[indices_1], s=50, c='m', marker='s')\n",
    "            plt.scatter(times_obj[indices_2], y_vel_error[indices_2], s=50, c='g', marker='^')\n",
    "            plt.scatter(times_obj[indices_3], y_vel_error[indices_3], s=50, c='r', marker='D')\n",
    "            plt.scatter(times_obj[indices_4], y_vel_error[indices_4], s=50, c='k', marker='o')\n",
    "            plt.ylabel('km/second', fontsize=18)\n",
    "            plt.xlabel(time_str, fontsize=18)\n",
    "            plt.legend(legend_names, fontsize=10)\n",
    "            plt.title('Obj ' + str(object_num) + ' ' + title_str + ' Y Velocity Error & Covariance Envelope', \\\n",
    "                      fontsize=18)\n",
    "            plt.ylim([-yv_range,yv_range])\n",
    "            plt.xlim([times[0] - 5, times[-1] + 5])\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "\n",
    "            fig_zvel = plt.figure()\n",
    "            plt.scatter(times_all, z_vel_error_all, label='_nolegend_', s=50, c='k', marker='.')\n",
    "            plt.plot(times, z_dot_covar_env_upper, label='_nolegend_', c='g')\n",
    "            plt.plot(times, z_dot_covar_env_lower, label='_nolegend_', c='g') \n",
    "            plt.scatter(times_obj[indices_1], z_vel_error[indices_1], s=50, c='m', marker='s')\n",
    "            plt.scatter(times_obj[indices_2], z_vel_error[indices_2], s=50, c='g', marker='^')\n",
    "            plt.scatter(times_obj[indices_3], z_vel_error[indices_3], s=50, c='r', marker='D')\n",
    "            plt.scatter(times_obj[indices_4], z_vel_error[indices_4], s=50, c='k', marker='o')\n",
    "            plt.ylabel('km/second', fontsize=18)\n",
    "            plt.xlabel(time_str, fontsize=18)\n",
    "            plt.legend(legend_names, fontsize=10)\n",
    "            plt.title('Obj ' + str(object_num) + ' ' + title_str + ' Z Velocity Error & Covariance Envelope', \\\n",
    "                      fontsize=18)\n",
    "            plt.ylim([-zv_range,zv_range])\n",
    "            plt.xlim([times[0] - 5, times[-1] + 5])\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "\n",
    "            if saveFig_bool:\n",
    "\n",
    "                fig_xpos.savefig('Figures/x_pos_error_obj' + str(object_num) + '.png')\n",
    "                fig_ypos.savefig('Figures/y_pos_error_obj' + str(object_num) + '.png')\n",
    "                fig_zpos.savefig('Figures/z_pos_error_obj' + str(object_num) + '.png')\n",
    "                fig_xvel.savefig('Figures/x_vel_error_obj' + str(object_num) + '.png')\n",
    "                fig_yvel.savefig('Figures/y_vel_error_obj' + str(object_num) + '.png')\n",
    "                fig_zvel.savefig('Figures/z_vel_error_obj' + str(object_num) + '.png')\n",
    "                fig_dens.savefig('Figures/dens_error' + str(object_num) + '.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#Generate Plots for Analysis\n",
    "\n",
    "x_range = .05\n",
    "y_range = .5\n",
    "z_range = .5\n",
    "\n",
    "xv_range = 1e-3\n",
    "yv_range = 1e-3\n",
    "zv_range = 1e-3\n",
    "\n",
    "density_range = 2e-2\n",
    "\n",
    "times = measurement_array[:, 0]\n",
    "\n",
    "title_str = 'Reduced Rank EnKF'\n",
    "\n",
    "\n",
    "filter_functions.calc_display_results_pre(pre_fit_list, prefit_bounds_list, measurement_array, R,\\\n",
    "                                          meas_type, stop_index, saveFig_bool, time_str)\n",
    "\n",
    "\n",
    "filter_functions.calc_display_results(post_fit_list_EnKF, measurement_array, R, meas_type, stop_index, saveFig_bool,\\\n",
    "                                     time_str)\n",
    "\n",
    "\n",
    "plot_error_covar_xref(P_list_EnKF, X_mean_updated_list_EnKF, X_mean_TU_list_EnKF, truth_xyz, true_density_array, \\\n",
    "                      density_range, x_range, y_range, z_range, xv_range, yv_range, zv_range,\\\n",
    "                      measurement_array, times, stop_index, saveFig_bool, time_str, title_str, num_of_objects)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "indices = np.where(measurement_array[:, 1] == 1)[0]\n",
    "print(indices)\n",
    "indices = np.where(measurement_array[:, 1] == 2)[0]\n",
    "print(indices)\n",
    "indices = np.where(measurement_array[:, 1] == 3)[0]\n",
    "print(indices)\n",
    "indices = np.where(measurement_array[:, 1] == 4)[0]\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
