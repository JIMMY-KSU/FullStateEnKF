{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import sympy as sym\n",
    "from scipy.integrate import ode\n",
    "from scipy.io import loadmat\n",
    "import scipy\n",
    "from scipy import stats\n",
    "import pickle\n",
    "import copy\n",
    "import filter_functions\n",
    "from sympy.utilities.lambdify import lambdify\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.debugger import Tracer\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 6.0)\n",
    "plt.rcParams['xtick.labelsize'] = 14\n",
    "plt.rcParams['ytick.labelsize'] = 14\n",
    "np.set_printoptions(precision=15)\n",
    "sym.init_printing()\n",
    "from IPython.display import display\n",
    "\n",
    "#MSIS: https://github.com/DeepHorizons/Python-NRLMSISE-00\n",
    "#import time\n",
    "from nrlmsise_00_header import *\n",
    "from nrlmsise_00 import *\n",
    "#SUBROUTINE GTD7D -- d[5] is the \"effective total mass density\n",
    "#for drag\" and is the sum of the mass densities of all species\n",
    "#in this model, INCLUDING anomalous oxygen.\n",
    "\n",
    "#define constants\n",
    "r_earth_const = 6378136.3 * 1e-3 #km\n",
    "omega_const = 7.2921158553e-5 #rad/s, angular velocity of earth\n",
    "J_2_const = .00108262617385222\n",
    "J_3_const = -.00000253241051856772\n",
    "mu_earth = 3.986004415e14 * 1e-9 #km^3/s^2\n",
    "\n",
    "\n",
    "#Drag:\n",
    "A_const = 0.9551567 * 1e-6 #km^2; cross-sectional area of satellite\n",
    "m_const = 10 #kg; mass of satellite\n",
    "C_D_const = 2.0\n",
    "theta_dot_const = 7.2921158553e-5 #rad/sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHYAAAAPBAMAAADDpCYrAAAAMFBMVEX///8AAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAv3aB7AAAAD3RSTlMAVO8Qq5l2zWaJMt0i\nu0SCRuA9AAAACXBIWXMAAA7EAAAOxAGVKw4bAAABK0lEQVQoFaWTsU4CURBF7+YFBQTcP4CEjooY\nLYyFxMpYUZAQOhINtf4CnZVbqTEU/AExsbTwD7Cg5RsMWYXQOTNvw8y+Urc4xb1z8jazbwF+3OB+\nGgInzy8w0Bk2pETUBPZj9x4Cdzj7MNAZdqU87qXAGHgKUWygdKswNamSA3vkLoB6O0C1gyhVmBly\nJffuN/AVBzjsoLZSQGtyJRfX/ZB7ncfwvIvaWqEzQ3IlF7eyBWaTPJJ5gvJWoTMJuZJ7l46cTfJI\n5l12d6jsanE5//c78x7qvCsLWkfEu8pgZvyuopWciyXw0A5Q/UQxVZgZciX3Lt2NPn98C/r8Bb4b\nGUxNruTePYhdCwHwiqOpgdZuA58XLjaXcI839C/kgdPRFQy0xluW0wv89fkFArncYIWUTMoAAAAA\nSUVORK5CYII=\n",
      "text/latex": [
       "$$1000000000.0$$"
      ],
      "text/plain": [
       "1000000000.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1e-4/1e-13\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-try comparing to the true density array used to generate measurements instead of the density based on the estimated position of the object\n",
    "-go back and do same comparion with estimated state (ps and vel) with the true, make sure its not as different this time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(441, 5)\n",
      "(441, 6)\n",
      "1440.0\n",
      "7.37037037037\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ntruth_xyz = truth_xyz[0::20]\\nmeasurement_array = measurement_array[0::20]\\ntrue_density_array = true_density_array[0::20]\\nlat_lst_meas_array = lat_lst_meas_array[0::20]\\nprint(measurement_array[50,0]/(60))\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "num_of_X_ensembles = 450\n",
    "\n",
    "meas_type = 3\n",
    "\n",
    "if meas_type == 1:\n",
    "    meas_file = open('Data Files/meas_range_rangeRate.pkl', 'rb')\n",
    "elif meas_type == 2:\n",
    "    meas_file = open('Data Files/meas_az_el.pkl', 'rb')\n",
    "elif meas_type == 3:\n",
    "    meas_file = open('Data Files/meas_az_el_range.pkl', 'rb') #_10s_all_3stat.pkl\n",
    "    \n",
    "    \n",
    "    \n",
    "#Date of Simulation Details:\n",
    "#June 24th, 2017 at 6am (this is the date & time at the beginning of the simulation/orbit)\n",
    "year_init = 2017\n",
    "month_init = 6\n",
    "day_of_month_init = 24\n",
    "day_of_year_init = 175\n",
    "hour_init = 6\n",
    "boulder_UT_offset = 6 #Boulder time + 6 hours = UT time\n",
    "hour_init_UT = hour_init + boulder_UT_offset\n",
    "    \n",
    "    \n",
    "\n",
    "#Canbera Station (DSS 34)\n",
    "lat_dss34 = math.radians(-35.398333)\n",
    "lon_dss34 = math.radians(148.981944)\n",
    "alt_dss34 = 691.75 * 1e-3 #km\n",
    "\n",
    "r_ecef_dss34 = filter_functions.topo2ecef(lat_dss34, lon_dss34, alt_dss34, r_earth_const)\n",
    "#print(r_ecef_dss34)\n",
    "\n",
    "#Madrid Station (DSS 65) -- correct position of Madrid Station\n",
    "lat_dss65 = math.radians(40.427222)\n",
    "lon_dss65 = math.radians(355.749444)\n",
    "alt_dss65 = 834.539 * 1e-3 #km\n",
    "\n",
    "r_ecef_dss65 = filter_functions.topo2ecef(lat_dss65, lon_dss65, alt_dss65, r_earth_const)\n",
    "#print(r_ecef_dss65)\n",
    "\n",
    "#Goldstone Station (DSS 13) \n",
    "lat_dss13 = math.radians(35.247164)\n",
    "lon_dss13 = math.radians(200.205)\n",
    "alt_dss13 = 1071.14904 * 1e-3 #km\n",
    "\n",
    "r_ecef_dss13 = filter_functions.topo2ecef(lat_dss13, lon_dss13, alt_dss13, r_earth_const)\n",
    "#print(r_ecef_dss13)\n",
    "\n",
    "\n",
    "\n",
    "# read python dict containing measurements\n",
    "mydict2 = pickle.load(meas_file)\n",
    "meas_file.close()\n",
    "measurement_array = mydict2['measurement_array']\n",
    "truth_xyz = mydict2['truth_pos_vel']\n",
    "true_density_array = mydict2['true_density']*1e9\n",
    "lat_lst_meas_array = mydict2['lat_lst_array']\n",
    "print(np.shape(measurement_array))\n",
    "print(np.shape(truth_xyz))\n",
    "\n",
    "#convert to km\n",
    "truth_xyz = truth_xyz * 1e-3\n",
    "measurement_array[:, -1] = measurement_array[:, -1] * 1e-3\n",
    "print(measurement_array[-1,0]/(60))\n",
    "print(measurement_array[200,0]/(60)/90)\n",
    "\n",
    "\n",
    "\n",
    "#sample measurements so only using every nth measurement\n",
    "\"\"\"\n",
    "truth_xyz = truth_xyz[0::20]\n",
    "measurement_array = measurement_array[0::20]\n",
    "true_density_array = true_density_array[0::20]\n",
    "lat_lst_meas_array = lat_lst_meas_array[0::20]\n",
    "print(measurement_array[50,0]/(60))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Vector of State/density Ensembles: (450, 37, 73)\n",
      "0.000479211906198\n",
      "0.000674552463898\n"
     ]
    }
   ],
   "source": [
    "#read in files necessary for ensemble & density est. portion\n",
    "\n",
    "\n",
    "# read python dict containing densities\n",
    "ensemble_file = open('Data Files/ensemble_density_grids.pkl', 'rb')\n",
    "mydict2 = pickle.load(ensemble_file)\n",
    "ensemble_file.close()\n",
    "\n",
    "#shape: ensembles (# of combos) by lat by lon\n",
    "ensembles_of_density_grid = mydict2['ensemble_of_density_grids'] \n",
    "print('Shape of Vector of State/density Ensembles:', np.shape(ensembles_of_density_grid))\n",
    "(num_of_ensembles, num_of_lat, num_of_lon) = np.shape(ensembles_of_density_grid)\n",
    "\n",
    "#convert from kg/m**3 to kg/km**3 -> 1/(1e-3)**3 = 1/(1e-9) = 1e9\n",
    "ensembles_of_density_grid = ensembles_of_density_grid * 1e9\n",
    "\n",
    "latitude_grid = mydict2['latitudes'] \n",
    "longitude_grid = mydict2['longitudes'] \n",
    "lat_res = latitude_grid[1] - latitude_grid[0] #spacing between each latitude \"tick\"\n",
    "lon_res = longitude_grid[1] - longitude_grid[0] #spacing between each longitude \"tick\"\n",
    "\n",
    "\n",
    "\n",
    "#add noise w/ standard deviation = 1e-4 (used as initialization of density covariance, as well)\n",
    "\n",
    "print(np.sum(ensembles_of_density_grid[:, 0, 0])/num_of_ensembles)\n",
    "ensemble_noise = np.random.randn(num_of_ensembles, num_of_lat, num_of_lon) * 1e-2 #1e-4\n",
    "ensembles_of_density_grid = ensembles_of_density_grid + ensemble_noise\n",
    "print(np.sum(ensembles_of_density_grid[:, 0, 0])/num_of_ensembles) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Propogate reference trajectory and S.T.M.\n",
    "def orbitpropogator_EnKF(t, X_vector, density):\n",
    "    \n",
    "    ensemble_member = X_vector\n",
    "\n",
    "    #find X acceleration via the F(X) lambdified equation\n",
    "    state_acc = X_dot_sol_fcn(ensemble_member[0], ensemble_member[1], ensemble_member[2], \\\n",
    "                                  ensemble_member[3], ensemble_member[4], ensemble_member[5], density)\n",
    "        \n",
    "    dx = state_acc.flatten()\n",
    "    return dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#EnKF specific functionality\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def gen_ensemble(X_0, Bsqrt_cov, ensemble_size):\n",
    "    \n",
    "    X_0 = X_0.reshape(len(X_0), 1)\n",
    "    \n",
    "    ensemble = np.zeros((len(X_0), ensemble_size))\n",
    "    \n",
    "    for ii in range(ensemble_size):\n",
    "        \n",
    "        member = X_0 + np.dot(Bsqrt_cov, np.random.randn(len(X_0), 1))\n",
    "\n",
    "        ensemble[:, ii] = member.reshape(len(X_0))\n",
    "    \n",
    "    return ensemble\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#two body motion force\n",
    "# **Setup force equations/acceleration/U\n",
    "\n",
    "#Force equations with J_2\n",
    "x, y, z, J_2, r_earth, mu, r, J_3 = sym.symbols('x y z J_2 r_earth mu r J_3')\n",
    "\n",
    "\n",
    "two_body_J2_string = 'mu/r * ( 1 - J_2*(r_earth/r)**2 * (3/2 * (z/r)**2 - 1/2) )' #potential\n",
    "two_body_J2 = sym.sympify(two_body_J2_string)\n",
    "two_body_J2 = two_body_J2.subs([(r, sym.sqrt(x**2+y**2+z**2))])\n",
    "two_body_J2_acc_x = two_body_J2.diff(x)\n",
    "two_body_J2_acc_y = two_body_J2.diff(y)\n",
    "two_body_J2_acc_z = two_body_J2.diff(z)\n",
    "\n",
    "\n",
    "two_body_J2_acc_x = two_body_J2_acc_x.subs([(r_earth, r_earth_const), (mu, mu_earth), \\\n",
    "                              (J_2, J_2_const)])\n",
    "two_body_J2_acc_y = two_body_J2_acc_y.subs([(r_earth, r_earth_const), (mu, mu_earth), \\\n",
    "                              (J_2, J_2_const)])\n",
    "two_body_J2_acc_z = two_body_J2_acc_z.subs([(r_earth, r_earth_const), (mu, mu_earth), \\\n",
    "                              (J_2, J_2_const)])\n",
    "#print('2 body & J2', two_body_J2_acc_x)\n",
    "\n",
    "x_acc = two_body_J2_acc_x\n",
    "y_acc = two_body_J2_acc_y\n",
    "z_acc = two_body_J2_acc_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Add drag to J_2 force equations\n",
    "\n",
    "x_dot, y_dot, z_dot, x_double_dot, y_double_dot, z_double_dot = \\\n",
    "    sym.symbols('x_dot, y_dot, z_dot, x_double_dot, y_double_dot, z_double_dot')\n",
    "    \n",
    "C_D, A, m, density, theta_dot, val, val_dot = \\\n",
    "    sym.symbols('C_D A m density theta_dot val, val_dot')\n",
    "\n",
    "drag_str = ('-(1/2)*C_D*(A/m)*density*'\n",
    "                'sqrt((x_dot+theta_dot*y)**2 + (y_dot-theta_dot*x)**2 +'\n",
    "                'z_dot**2)*(val_dot+theta_dot*val)')\n",
    "drag_symp = sym.sympify(drag_str)\n",
    "\n",
    "drag_symp = drag_symp.subs([(A, A_const), (m, m_const), (C_D, C_D_const),\\\n",
    "                        (theta_dot, theta_dot_const)])\n",
    "\n",
    "\n",
    "x_drag_symp = drag_symp.subs([(r, sym.sqrt(x**2+y**2+z**2)), (val, y), (val_dot, x_dot)])\n",
    "x_acc = x_acc + x_drag_symp\n",
    "\n",
    "y_drag_symp = drag_symp.subs([(r, sym.sqrt(x**2+y**2+z**2)), (val, x), (val_dot, y_dot)])\n",
    "y_acc = y_acc + y_drag_symp\n",
    "\n",
    "z_drag_symp = drag_symp.subs([(r, sym.sqrt(x**2+y**2+z**2)), (val, z), (val_dot, z_dot)])\n",
    "z_acc = z_acc + z_drag_symp\n",
    "    \n",
    "\n",
    "\n",
    "x_acc_sol_fcn = lambdify((x, y, z, x_dot, y_dot, z_dot, density), x_acc)\n",
    "y_acc_sol_fcn = lambdify((x, y, z, x_dot, y_dot, z_dot, density), y_acc)\n",
    "z_acc_sol_fcn = lambdify((x, y, z, x_dot, y_dot, z_dot, density), z_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if (meas_type == 1) or (meas_type == 3):\n",
    "    \n",
    "    x_s, y_s, z_s, x_sf, y_sf, z_sf, theta, theta_dot, t, x_dot, y_dot, z_dot = \\\n",
    "        sym.symbols('x_s, y_s, z_s, x_sf, y_sf, z_sf, theta, theta_dot, t, x_dot, y_dot, z_dot')\n",
    "\n",
    "    #define symbolic rho equation\n",
    "    rho = ('sqrt((x - x_s)**2 + (y - y_s)**2 + (z - z_s)**2)')\n",
    "    rho = sym.sympify(rho)\n",
    "    #sub rotation equation of ecef for eci\n",
    "    rho = rho.subs(x_s, x_sf*sym.cos(omega_const*t) - y_sf*sym.sin(omega_const*t))\n",
    "    rho = rho.subs(y_s, x_sf*sym.sin(omega_const*t) + y_sf*sym.cos(omega_const*t))\n",
    "    rho = rho.subs(z_s, z_sf)\n",
    "\n",
    "    #define symbolic rho dot equation\n",
    "    rho_dot = ('(x*x_dot + y*y_dot + z*z_dot - (x_dot*x_s+y_dot*y_s)*cos(theta) + \\\n",
    "               theta_dot*(x*x_s + y*y_s)*sin(theta) + (x_dot*y_s - y_dot*x_s)*sin(theta) +\\\n",
    "               theta_dot*(x*y_s - y*x_s)*cos(theta) - z_dot*z_s)/ rho')\n",
    "    rho_dot = sym.sympify(rho_dot)\n",
    "    #execute substitutions for rho_dot\n",
    "    rho_dot = rho_dot.subs(x_s, x_sf) \n",
    "    rho_dot = rho_dot.subs(y_s, y_sf) \n",
    "    rho_dot = rho_dot.subs(z_s, z_sf) \n",
    "    rho_dot = rho_dot.subs('rho', rho)\n",
    "    rho_dot = rho_dot.subs(theta, omega_const*t)    \n",
    "    rho_dot = rho_dot.subs(theta_dot, omega_const)\n",
    "\n",
    "    rho_fcn = lambdify(((x, y, z, x_sf, y_sf, z_sf, t)), rho)\n",
    "    rho_dot_fcn = lambdify(((x, y, z, x_dot, y_dot, z_dot, x_sf, y_sf, z_sf, t)), rho_dot)\n",
    "\n",
    "\n",
    "if (meas_type == 2) or (meas_type == 3):\n",
    "    \n",
    "    #x_sf, etc. is the sensor pos in ecef\n",
    "    #x, y, z is the satellite eci\n",
    "    x_s, y_s, z_s, x_sf, y_sf, z_sf, theta, x, y, z, t = \\\n",
    "        sym.symbols('x_s, y_s, z_s, x_sf, y_sf, z_sf, theta, x, y, z, t')\n",
    "    x_L, y_L, z_L, X_L_norm, x_range, y_range, z_range, lon, lat, \\\n",
    "        x_sat_ecef, y_sat_ecef, z_sat_ecef, sen_ecef_norm, omega  = \\\n",
    "        sym.symbols('x_L, y_L, z_L, X_L_norm, x_range, y_range, z_range, lon, lat, \\\n",
    "        x_sat_ecef, y_sat_ecef, z_sat_ecef, sen_ecef_norm, omega')\n",
    "        \n",
    "\n",
    "    #define symbolic rho equation\n",
    "    azimuth = ('atan2(x_L, y_L)') #step 4\n",
    "    azimuth = sym.sympify(azimuth)\n",
    "    \n",
    "    elevation = ('asin(z_L/X_L_norm)') #step 4\n",
    "    elevation = sym.sympify(elevation)\n",
    "    elevation = elevation.subs(X_L_norm, sym.sqrt(x_L**2 + y_L**2 + z_L**2))\n",
    "    \n",
    "    #step 3\n",
    "    azimuth = azimuth.subs([(x_L, -x_range*sym.sin(lon) + y_range*sym.cos(lon)), \\\n",
    "            (y_L, -x_range*sym.sin(lat)*sym.cos(lon) - y_range*sym.sin(lat)*sym.sin(lon) + z_range*sym.cos(lat))])\n",
    "    elevation = elevation.subs([(x_L, -x_range*sym.sin(lon) + y_range*sym.cos(lon)), \\\n",
    "            (y_L, -x_range*sym.sin(lat)*sym.cos(lon) - y_range*sym.sin(lat)*sym.sin(lon) + z_range*sym.cos(lat)), \\\n",
    "            (z_L, x_range*sym.cos(lat)*sym.cos(lon) + y_range*sym.cos(lat)*sym.sin(lon) + z_range*sym.sin(lat))])\n",
    "    \n",
    "    #step 2\n",
    "    azimuth = azimuth.subs([(x_range, x_sat_ecef - x_sf), (y_range, y_sat_ecef - y_sf), \\\n",
    "            (z_range, z_sat_ecef - z_sf), (lat, sym.asin(z_sf/sen_ecef_norm)), (lon, sym.atan2(y_sf, x_sf))])\n",
    "    elevation = elevation.subs([(x_range, x_sat_ecef - x_sf), (y_range, y_sat_ecef - y_sf), \\\n",
    "            (z_range, z_sat_ecef - z_sf), (lat, sym.asin(z_sf/sen_ecef_norm)), (lon, sym.atan2(y_sf, x_sf))])\n",
    "    \n",
    "    #step 1\n",
    "    azimuth = azimuth.subs([(x_sat_ecef, x*sym.cos(theta) + y*sym.sin(theta)), \\\n",
    "                        (y_sat_ecef, -x*sym.sin(theta) + y*sym.cos(theta)), (z_sat_ecef, z), \\\n",
    "                        (sen_ecef_norm, sym.sqrt(x_sf**2 + y_sf**2 + z_sf**2))])\n",
    "    elevation = elevation.subs([(x_sat_ecef, x*sym.cos(theta) + y*sym.sin(theta)), \\\n",
    "                        (y_sat_ecef, -x*sym.sin(theta) + y*sym.cos(theta)), (z_sat_ecef, z), \\\n",
    "                        (sen_ecef_norm, sym.sqrt(x_sf**2 + y_sf**2 + z_sf**2))])\n",
    "    \n",
    "    \n",
    "    azimuth = azimuth.subs([(theta, omega_const*t), (omega, omega_const)])\n",
    "    elevation = elevation.subs([(theta, omega_const*t), (omega, omega_const)])\n",
    "    \n",
    "    azimuth_fcn = lambdify(((x, y, z, x_sf, y_sf, z_sf, t)), azimuth)\n",
    "    elevation_fcn = lambdify(((x, y, z, x_sf, y_sf, z_sf, t)), elevation)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#State and A matrix\n",
    "\n",
    "\n",
    "#define the symbolic state matrix\n",
    "X = sym.Matrix([x, y, z, x_dot, y_dot, z_dot])\n",
    "X_dot = sym.Matrix([x_dot, y_dot, z_dot, x_acc, y_acc, z_acc])\n",
    "    \n",
    "\n",
    "#partial of the force model (x dot) WRT the state vector\n",
    "A_mat = X_dot.jacobian(X)\n",
    "#print(A_mat)\n",
    "\n",
    "A_sol_fcn = lambdify((x, y, z, x_dot, y_dot, z_dot, density), A_mat)\n",
    "#print(A_sol_fcn(1,2,3,4,5,6,7,8))\n",
    "\n",
    "#print(X_dot)\n",
    "X_dot_sol_fcn = lambdify((x, y, z, x_dot, y_dot, z_dot, density), X_dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define observation model (G) and H_tilde\n",
    "\n",
    "\n",
    "if meas_type == 1:\n",
    "    G = sym.Matrix([rho, rho_dot])\n",
    "    \n",
    "    \n",
    "elif meas_type == 2:\n",
    "    G = sym.Matrix([azimuth, elevation])\n",
    "    \n",
    "elif meas_type == 3:\n",
    "    G = sym.Matrix([azimuth, elevation, rho])\n",
    "\n",
    "#print(G)\n",
    "G_sol_fcn = lambdify((x, y, z, x_dot, y_dot, z_dot, x_sf, y_sf, z_sf, t), G)\n",
    "\n",
    "#partial derivitive of observation model WRT the state vector\n",
    "X_full = X = sym.Matrix([x, y, z, x_dot, y_dot, z_dot, density])\n",
    "H_tilde = G.jacobian(X_full)\n",
    "#print(H_tilde)\n",
    "H_tilde_sol_fcn = lambdify((x, y, z, x_dot, y_dot, z_dot, x_sf, y_sf, z_sf, t), H_tilde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def calc_P_TU(X_mean, ensemble):\n",
    "    \n",
    "    P_bar_sum = np.zeros((fullState_dimension, fullState_dimension))\n",
    "    \n",
    "    for ii in range(num_of_X_ensembles):\n",
    "\n",
    "        X = ensemble[:, ii].reshape(fullState_dimension, 1)\n",
    "        diff_X = X - X_mean\n",
    "        P_bar_sum = P_bar_sum + np.dot(diff_X, diff_X.T)\n",
    "\n",
    "    P_bar = P_bar_sum/(num_of_X_ensembles-1)\n",
    "\n",
    "    return P_bar\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 450)\n",
      "[  6.778236300000000e+03   1.000000000000000e-01   1.000000000000000e-01\n",
      "   1.000000000000000e-04   5.874556678131549e+00   4.929354431987092e+00]\n"
     ]
    }
   ],
   "source": [
    "#define reference state at epoch, covariance at epoch, and R (using measurement noise)\n",
    "\n",
    "pos_perturbation = 100 * 1e-3 #km\n",
    "vel_perturbation = .1 * 1e-3 #km/s\n",
    "\n",
    "density_dimension = num_of_lat * num_of_lon\n",
    "prob_dimension = 6\n",
    "fullState_dimension = prob_dimension + density_dimension\n",
    "\n",
    "X_ref = np.array([truth_xyz[0,0] + pos_perturbation, truth_xyz[0,1] + pos_perturbation, truth_xyz[0,2] \\\n",
    "            + pos_perturbation, truth_xyz[0,3] + vel_perturbation, truth_xyz[0,4] + vel_perturbation, \\\n",
    "            truth_xyz[0,5] + vel_perturbation])\n",
    "\n",
    "                  \n",
    "P_sigma_pos = 100 * 1e-3 #km\n",
    "P_sigma_vel = .1 * 1e-3 #km/s\n",
    "P_bar_0 = np.diag([P_sigma_pos**2, P_sigma_pos**2, P_sigma_pos**2, \\\n",
    "                   P_sigma_vel**2, P_sigma_vel**2, P_sigma_vel**2, (1e-4)**2]) #(1e-4)**2\n",
    "\n",
    "\n",
    "(L, V) = np.linalg.eig(P_bar_0[:prob_dimension, :prob_dimension]) #eigenvalues, eigenvectors\n",
    "Bsqrt_cov = V * np.sqrt(L)\n",
    "X_ensemble = gen_ensemble(X_ref, Bsqrt_cov, num_of_X_ensembles)\n",
    "print(np.shape(X_ensemble))\n",
    "\n",
    "       \n",
    "                  \n",
    "#Define Measurement Noise\n",
    "if meas_type == 1:\n",
    "    sigma_rho = .1 * 1e-3 #km\n",
    "    sigma_rho_dot = .01 * 1e-3 #km/s\n",
    "    W = np.array([[1/sigma_rho**2, 0], [0, 1/sigma_rho_dot**2]])\n",
    "    R = np.linalg.inv(W)\n",
    "    \n",
    "elif meas_type == 2:\n",
    "    arcsec_per_rad = 206264.806247 #arcsec/rad\n",
    "    noise_rad = (1/arcsec_per_rad)  * 5  #5 arcsec noise (suggested by Moriba for a telescope)\n",
    "    sigma_az = noise_rad\n",
    "    sigma_el = noise_rad\n",
    "    W = np.array([[1/sigma_az**2, 0], [0, 1/sigma_el**2]])\n",
    "    R = np.linalg.inv(W)\n",
    "    \n",
    "elif meas_type == 3:\n",
    "    arcsec_per_rad = 206264.806247 #arcsec/rad\n",
    "    noise_rad = (1/arcsec_per_rad)  * 5  #5 arcsec noise (suggested by Moriba for a telescope)\n",
    "    sigma_az = noise_rad\n",
    "    sigma_el = noise_rad\n",
    "    sigma_rho = .1 * 1e-3 #km\n",
    "    W = np.array([[1/sigma_az**2, 0, 0], [0, 1/sigma_el**2, 0], [0, 0, 1/sigma_rho**2]])\n",
    "    R = np.linalg.inv(W)\n",
    "    \n",
    "    meas_indices = np.array([0, 1, 2])\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "print(X_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#KALMAN\n",
    "\n",
    "def execute_enkf(obs_data, X_ensemble, P, R, density_state_ensemble,\\\n",
    "                         prob_dimension, density_dimension, stop_index):\n",
    "    \n",
    "    #initializations\n",
    "    Q_ECI = np.zeros((6,6))\n",
    "    \n",
    "    num_of_meas = np.shape(R)[0]\n",
    "    X_mean_updated = np.sum(X_ensemble, axis=1)/num_of_X_ensembles\n",
    "\n",
    "    post_fit_list = np.zeros((stop_index, num_of_meas))\n",
    "\n",
    "    P_list = np.zeros((stop_index, prob_dimension+1, prob_dimension+1))\n",
    "    X_mean_updated_list = np.zeros((stop_index, prob_dimension+1))\n",
    "    \n",
    "    density_MSIS_array = np.zeros(stop_index)\n",
    "    est_density_array = np.zeros(stop_index)\n",
    "    est_density_grid_array = np.zeros((stop_index, num_of_lat, num_of_lon))\n",
    "\n",
    "    \n",
    "    X_distribution = np.zeros((prob_dimension, num_of_X_ensembles, stop_index))\n",
    "    density_distribution = np.zeros((num_of_ensembles, stop_index))\n",
    "    lat_lst_array = np.zeros((2, stop_index))\n",
    "    X_density_ensemble = np.zeros((fullState_dimension, num_of_X_ensembles))\n",
    "\n",
    "    \n",
    "\n",
    "    for obsnum in range(len(obs_data[:stop_index])):\n",
    "        \n",
    "        print(obsnum)\n",
    "        \n",
    "        time = obs_data[obsnum, 0]\n",
    "        t_init = obs_data[obsnum-1, 0] \n",
    "        if obsnum == 0:\n",
    "            t_init = obs_data[obsnum, 0]\n",
    "            \n",
    "        #save true MSIS density for this time step (which is density at beginning of time used to prop for this step)\n",
    "        density_MSIS = filter_functions.calc_MSIS_density(time, X_mean_updated, day_of_year_init, day_of_month_init, \\\n",
    "                                                hour_init_UT, month_init, year_init, omega_const, r_earth_const)\n",
    "        density_MSIS = density_MSIS * 1e9  #convert from kg/m**3 to kg/km**3\n",
    "        density_MSIS_array[obsnum] = density_MSIS\n",
    "        \n",
    "        \n",
    "        #determine density to be used in propagation\n",
    "        (lat_grid_ticks, lst_grid_ticks) = filter_functions.calc_lat_lst_indices(t_init, \\\n",
    "                            X_mean_updated[:3].reshape(3,1), day_of_month_init, hour_init_UT, month_init, \\\n",
    "                                   year_init, omega_const, r_earth_const, lat_res, lon_res)\n",
    "        \n",
    "        \n",
    "        densities = density_state_ensemble[:, lat_grid_ticks, lst_grid_ticks]\n",
    "\n",
    "\n",
    "        result = X_ensemble\n",
    "\n",
    "        if (obs_data[obsnum, 0] != obs_data[obsnum-1, 0]) and (obsnum != 0):\n",
    "            \n",
    "            result = np.zeros((prob_dimension, num_of_X_ensembles))\n",
    "\n",
    "            for ii in range(num_of_X_ensembles):\n",
    "                #set the initial values for the propogator:\n",
    "                y0 = X_ensemble[:, ii]\n",
    "                density = densities[ii]\n",
    "            \n",
    "                integrator = ode(orbitpropogator_EnKF)\n",
    "                integrator.set_integrator('dopri5', nsteps=1e6, rtol=1e-12, atol=1e-12)\n",
    "                integrator.set_f_params(density)\n",
    "                integrator.set_initial_value(y0, t_init)\n",
    "                integrator.integrate(time)\n",
    "                result[:, ii] = integrator.y\n",
    "\n",
    "        X_ensemble = result.reshape(prob_dimension, num_of_X_ensembles)\n",
    "        \n",
    "        \n",
    "        #create array that holds X state ensemble & density state ensemble combined\n",
    "        density_state = density_state_ensemble[:, lat_grid_ticks, lst_grid_ticks]\n",
    "        X_density_ensemble[:6, :] = X_ensemble\n",
    "        X_density_ensemble[6:, :] = density_state_ensemble.reshape(num_of_ensembles, density_dimension).T \n",
    "        #****density_state\n",
    "        \n",
    "        #mean of X state & density state over ensemble\n",
    "        X_bar = np.sum(X_density_ensemble, axis=1).reshape(fullState_dimension,1)/num_of_X_ensembles\n",
    "        \n",
    "        #calculate Time Updated P\n",
    "        P_TU = calc_P_TU(X_bar, X_density_ensemble)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #determine station coordinates for observation eq.\n",
    "        if int(obs_data[obsnum, 1]) == 1:\n",
    "            #print('1')\n",
    "            station_index = 0\n",
    "            X_s = r_ecef_dss34\n",
    "        if int(obs_data[obsnum, 1]) == 2:\n",
    "            #print('2')\n",
    "            station_index = 1\n",
    "            X_s = r_ecef_dss65\n",
    "        if int(obs_data[obsnum, 1]) == 3:\n",
    "            #print('3')\n",
    "            station_index = 2\n",
    "            X_s = r_ecef_dss13\n",
    "\n",
    "        \n",
    "        #-------------------Measurement Update--------------------------------------------------------\n",
    "        \n",
    "        #-----calculate Hx_bar (predicted measurement for each member)-----\n",
    "        Hx_array = np.zeros((num_of_meas, num_of_X_ensembles))\n",
    "        Hx = np.zeros(3)\n",
    "\n",
    "        for ii in range(num_of_X_ensembles):\n",
    "            \n",
    "            X = X_ensemble[:, ii]\n",
    "\n",
    "            Hx = G_sol_fcn(*X, *X_s, time)\n",
    "            \n",
    "            if (meas_type != 1) and (Hx[0] < 0): #if az less than 0, add 2*pi\n",
    "                Hx[0] = Hx[0] + 2*math.pi\n",
    "\n",
    "            Hx_array[:, ii] = Hx.reshape(num_of_meas)\n",
    "            \n",
    "        \n",
    "        Hx_bar = np.sum(Hx_array, axis=1).reshape(num_of_meas,1)/num_of_X_ensembles #mean of Hx over ensemble\n",
    "         \n",
    "        \n",
    "        \n",
    "        #------------------calculate Pyy & Pxy-------------------\n",
    "        Pyy_sum = np.zeros((num_of_meas, num_of_meas))\n",
    "        Pxy_sum = np.zeros((fullState_dimension, num_of_meas))\n",
    "        \n",
    "        for ii in range(num_of_X_ensembles):\n",
    "        \n",
    "            #Pyy\n",
    "            diff_Hx = Hx_array[:, ii].reshape(num_of_meas, 1) - Hx_bar\n",
    "            Pyy_sum = Pyy_sum + np.dot(diff_Hx, diff_Hx.T)\n",
    "\n",
    "            #Pxy\n",
    "            X = X_density_ensemble[:, ii].reshape(fullState_dimension, 1)\n",
    "            diff_X = X - X_bar\n",
    "            Pxy_sum = Pxy_sum + np.dot(diff_X, diff_Hx.T)\n",
    "          \n",
    "       \n",
    "        Pyy_term = Pyy_sum/(num_of_X_ensembles-1)\n",
    "        Pyy = Pyy_term + R\n",
    "        Pxy = Pxy_sum/(num_of_X_ensembles-1)\n",
    "\n",
    "        \n",
    "        #--------calculate Kalman gain & apply measurement for posterior estimate------\n",
    "        K = np.dot(Pxy, np.linalg.inv(Pyy))\n",
    "        \n",
    "        #get actual observation\n",
    "        y_observed = obs_data[obsnum, 2:(2+num_of_meas)].reshape(num_of_meas,1)\n",
    "\n",
    "        X_density_ensemble_updated = np.zeros((fullState_dimension, num_of_X_ensembles))\n",
    "        #**correction_array = np.zeros((1, num_of_X_ensembles))\n",
    "        for ii in range(num_of_X_ensembles):\n",
    "            \n",
    "            e = np.sqrt(np.diag(R)).reshape(num_of_meas, 1) * np.random.randn(num_of_meas, 1)\n",
    "            \n",
    "            correction = np.dot(K, (y_observed + e - Hx_array[:, ii].reshape(num_of_meas, 1)))\n",
    "            \n",
    "            X_member_updated = X_density_ensemble[:, ii].reshape(fullState_dimension, 1) + correction\n",
    "                \n",
    "            X_density_ensemble_updated[:, ii] = X_member_updated.reshape(fullState_dimension)\n",
    "            #**correction_array[:, ii] = correction[-1,0] #save density correction\n",
    "        \n",
    "        \n",
    "        #update overall density grid ensemble with this lat/lon's updated density values \n",
    "        updated_density_portion = X_density_ensemble_updated[6:,:].T.reshape(num_of_ensembles, num_of_lat, num_of_lon)\n",
    "        density_state_ensemble = copy.deepcopy(updated_density_portion) #**X_density_ensemble_updated[-1,:]\n",
    "        #**density_state_ensemble[:, lat_grid_ticks, lst_grid_ticks]\n",
    "        est_density_grid_array[obsnum] = np.mean(density_state_ensemble, axis=0)\n",
    "        \n",
    "        \n",
    "        #apply update to local lat/lst combos via localization\n",
    "        #**density_state_ensemble = localization(lat_grid_ticks, lst_grid_ticks, \\\n",
    "                #correction_array, density_state_ensemble)\n",
    "\n",
    "        \n",
    "        est_density_array[obsnum] = np.sum(updated_density_portion[:, lat_grid_ticks, lst_grid_ticks])\\\n",
    "                                                                                            /num_of_ensembles\n",
    "        #**np.sum(X_density_ensemble_updated[-1,:])/num_of_ensembles\n",
    "            \n",
    "        #save values\n",
    "        X_mean_updated = np.zeros((7))\n",
    "        X_mean_updated[:6] = np.sum(X_density_ensemble_updated[:6,:], axis=1)/num_of_X_ensembles\n",
    "        X_mean_updated[6:] = est_density_array[obsnum]\n",
    "        #**X_mean_updated = np.sum(X_density_ensemble_updated, axis=1)/num_of_X_ensembles\n",
    "        X_mean_updated_list[obsnum, :] = X_mean_updated\n",
    "        \n",
    "        #save values for analysis of distributions and such\n",
    "        density_distribution[:, obsnum] = updated_density_portion[:, lat_grid_ticks, lst_grid_ticks] \n",
    "        #**X_density_ensemble_updated[-1,:]\n",
    "        X_distribution[:, :, obsnum] = X_density_ensemble_updated[:6, :]\n",
    "        lat_lst_array[:, obsnum] = np.array([latitude_grid[lat_grid_ticks], longitude_grid[lst_grid_ticks]]).reshape(2)\n",
    "\n",
    "        \n",
    "        #book:\n",
    "        #P = P_TU - np.dot(K, np.dot(Pyy, K.T)) \n",
    "        #Tracer() ()\n",
    "        #if obsnum > 5:\n",
    "        #print('cor:', np.diag(np.dot(K, np.dot(Pyy, K.T)))[:3])\n",
    "        #print('P:', np.diag(P)[:3])\n",
    "        \n",
    "        \n",
    "        #Review, eq. 9:\n",
    "        #pad H with zeros for density portion of state b/c partial of meas WRT to density = 0\n",
    "        H = np.zeros((num_of_meas, fullState_dimension))\n",
    "        H[:num_of_meas, :prob_dimension+1] = H_tilde_sol_fcn(*X_mean_updated[:6], *X_s, time)\n",
    "        part1 = np.identity(fullState_dimension) - np.dot(K, H)\n",
    "        P = np.dot(part1, np.dot(P_TU, part1.T)) + np.dot(K, np.dot(R, K.T))\n",
    "        \n",
    "        #Review, eq. 10:\n",
    "        #P = np.dot((np.identity(prob_dimension+1) - np.dot(K, H)), P_TU)\n",
    "        \n",
    "        P_list[obsnum, :6, :6] = P[:6, :6]\n",
    "        P_list[obsnum, -1, -1] = P[0,6:].reshape(num_of_lat, num_of_lon)[lat_grid_ticks, lst_grid_ticks]\n",
    "\n",
    "\n",
    "        #-------------RMS / Residuals-------------\n",
    "        Hx = G_sol_fcn(*X_mean_updated[:6], *X_s, time)\n",
    "        if Hx[0] < 0:\n",
    "            Hx[0] = Hx[0] + 2*math.pi\n",
    "        post_fit_resid = y_observed - Hx.reshape(num_of_meas, 1)\n",
    "        post_fit_list[obsnum, :] = post_fit_resid.reshape(num_of_meas) \n",
    "        \n",
    "\n",
    "        #Prep for next time step\n",
    "        X_ensemble = X_density_ensemble_updated[:6, :]\n",
    "\n",
    "    \n",
    "    \n",
    "    return (X_mean_updated_list, P_list, post_fit_list, density_MSIS_array, est_density_array, X_distribution, \\\n",
    "            density_distribution, lat_lst_array, updated_density_portion, X_ensemble, est_density_grid_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n"
     ]
    }
   ],
   "source": [
    "#Call EnKF\n",
    "\n",
    "\n",
    "stop_index = 175 #558 * 3 #92.56 minutes #len(measurement_array)/2\n",
    "\n",
    "(X_mean_updated_list_EnKF, P_list_EnKF, post_fit_list_EnKF, density_MSIS_array, est_density_array,\\\n",
    "X_distribution, density_distribution, lat_lst_array, updated_density_portion, X_ensemble, est_density_grid_array) = \\\n",
    "                execute_enkf(measurement_array, X_ensemble, P_bar_0, R, ensembles_of_density_grid,\\\n",
    "                         prob_dimension, density_dimension, stop_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n#generate density grid for final time in order to compare entire final density grid estimate to truth\\nalt = np.linalg.norm(X_mean_updated_list_EnKF[-1,:3]) - r_earth_const\\nprint(alt)\\nfinal_density_grid_truth = filter_functions.gen_one_ensemble(latitude_grid, longitude_grid, alt,                                                    day_of_year_init, measurement_array[stop_index-1,0]) #final time\\n\\nfinal_density_grid_truth_timeSeries = np.zeros((stop_index, num_of_lat, num_of_lon))\\n\\nfor ii in range(stop_index):\\n    \\n    #generate density grid for final time in order to compare entire final density grid estimate to truth\\n    alt = np.linalg.norm(X_mean_updated_list_EnKF[ii,:3]) - r_earth_const\\n    final_density_grid_truth_timeSeries[ii] = filter_functions.gen_one_ensemble(latitude_grid, longitude_grid, alt,                                                day_of_year_init, measurement_array[ii,0]) #final time\\n    \\n\\n\\n\\n#PKL file\\n\\nmydict = {'X_mean_updated_list_EnKF': X_mean_updated_list_EnKF, 'P_list_EnKF': P_list_EnKF,           'post_fit_list_EnKF': post_fit_list_EnKF, 'density_MSIS_array': density_MSIS_array,           'est_density_array': est_density_array, 'X_distribution': X_distribution,           'density_distribution': density_distribution, 'lat_lst_array': lat_lst_array,\\n         'final_density_ensemble_est': updated_density_portion, 'final_X_ensemble': X_ensemble,        'true_density_array': true_density_array*1e9,           'final_density_grid_truth_timeSeries': final_density_grid_truth_timeSeries*1e9,\\n         'est_density_grid_array': est_density_grid_array}\\n\\noutput = open('Figures/Results.pkl', 'wb')\\npickle.dump(mydict, output)\\noutput.close()\\n\\n\\n\\n#MATLAB file\\n\\nfilename = 'Data Files/7Period_Results.mat'\\n\\nimport scipy.io\\nscipy.io.savemat(filename, mdict={'X_mean_updated_list_EnKF': X_mean_updated_list_EnKF, 'P_list_EnKF': P_list_EnKF,           'post_fit_list_EnKF': post_fit_list_EnKF, 'density_MSIS_array': density_MSIS_array,           'est_density_array': est_density_array, 'X_distribution': X_distribution,           'density_distribution': density_distribution, 'lat_lst_array': lat_lst_array,\\n         'final_density_ensemble_est': updated_density_portion, 'final_X_ensemble': final_X_ensemble,\\n        'true_density_array': true_density_array*1e9, \\n        'final_density_grid_truth_timeSeries': final_density_grid_truth_timeSeries*1e9,\\n        'est_density_grid_array': est_density_grid_array})\\n\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Save Results\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#generate density grid for final time in order to compare entire final density grid estimate to truth\n",
    "alt = np.linalg.norm(X_mean_updated_list_EnKF[-1,:3]) - r_earth_const\n",
    "print(alt)\n",
    "final_density_grid_truth = filter_functions.gen_one_ensemble(latitude_grid, longitude_grid, alt,\\\n",
    "                                                    day_of_year_init, measurement_array[stop_index-1,0]) #final time\n",
    "\n",
    "final_density_grid_truth_timeSeries = np.zeros((stop_index, num_of_lat, num_of_lon))\n",
    "\n",
    "for ii in range(stop_index):\n",
    "    \n",
    "    #generate density grid for final time in order to compare entire final density grid estimate to truth\n",
    "    alt = np.linalg.norm(X_mean_updated_list_EnKF[ii,:3]) - r_earth_const\n",
    "    final_density_grid_truth_timeSeries[ii] = filter_functions.gen_one_ensemble(latitude_grid, longitude_grid, alt,\\\n",
    "                                                day_of_year_init, measurement_array[ii,0]) #final time\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#PKL file\n",
    "\n",
    "mydict = {'X_mean_updated_list_EnKF': X_mean_updated_list_EnKF, 'P_list_EnKF': P_list_EnKF, \\\n",
    "          'post_fit_list_EnKF': post_fit_list_EnKF, 'density_MSIS_array': density_MSIS_array, \\\n",
    "          'est_density_array': est_density_array, 'X_distribution': X_distribution, \\\n",
    "          'density_distribution': density_distribution, 'lat_lst_array': lat_lst_array,\n",
    "         'final_density_ensemble_est': updated_density_portion, 'final_X_ensemble': X_ensemble,\\\n",
    "        'true_density_array': true_density_array*1e9, \\\n",
    "          'final_density_grid_truth_timeSeries': final_density_grid_truth_timeSeries*1e9,\n",
    "         'est_density_grid_array': est_density_grid_array}\n",
    "\n",
    "output = open('Figures/Results.pkl', 'wb')\n",
    "pickle.dump(mydict, output)\n",
    "output.close()\n",
    "\n",
    "\n",
    "\n",
    "#MATLAB file\n",
    "\n",
    "filename = 'Data Files/7Period_Results.mat'\n",
    "\n",
    "import scipy.io\n",
    "scipy.io.savemat(filename, mdict={'X_mean_updated_list_EnKF': X_mean_updated_list_EnKF, 'P_list_EnKF': P_list_EnKF, \\\n",
    "          'post_fit_list_EnKF': post_fit_list_EnKF, 'density_MSIS_array': density_MSIS_array, \\\n",
    "          'est_density_array': est_density_array, 'X_distribution': X_distribution, \\\n",
    "          'density_distribution': density_distribution, 'lat_lst_array': lat_lst_array,\n",
    "         'final_density_ensemble_est': updated_density_portion, 'final_X_ensemble': final_X_ensemble,\n",
    "        'true_density_array': true_density_array*1e9, \n",
    "        'final_density_grid_truth_timeSeries': final_density_grid_truth_timeSeries*1e9,\n",
    "        'est_density_grid_array': est_density_grid_array})\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Figures/Results.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-323f27a50389>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# read python dict containing densities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mresults_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Figures/Results.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Results_1Period/Results_558.pkl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmydict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mresults_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Figures/Results.pkl'"
     ]
    }
   ],
   "source": [
    "#Read Data (from saved run) in to Generate Results\n",
    "\n",
    "\n",
    "# read python dict containing densities\n",
    "results_file = open('Figures/Results.pkl', 'rb') #Results_1Period/Results_558.pkl\n",
    "mydict = pickle.load(results_file)\n",
    "results_file.close()\n",
    "\n",
    "X_mean_updated_list_EnKF = mydict['X_mean_updated_list_EnKF'] \n",
    "P_list_EnKF = mydict['P_list_EnKF'] \n",
    "post_fit_list_EnKF = mydict['post_fit_list_EnKF'] \n",
    "density_MSIS_array = mydict['density_MSIS_array'] \n",
    "est_density_array = mydict['est_density_array'] \n",
    "X_distribution = mydict['X_distribution'] \n",
    "density_distribution = mydict['density_distribution'] \n",
    "lat_lst_array = mydict['lat_lst_array'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "times = measurement_array[:stop_index, 0]/(60)\n",
    "\n",
    "time_str = 'Time (minutes)'\n",
    "\n",
    "time_repeated_ensemble = np.repeat(times[:stop_index], num_of_ensembles)\n",
    "time_repeated_X = np.repeat(times[:stop_index], num_of_X_ensembles)\n",
    "\n",
    "#\"\"\"\n",
    "#calculate the normed density distribution by subtracting the mean of the ensemble from each ensemble   \n",
    "#density_distribution shape = num_of_ensembles x stop_index\n",
    "density_distribution_mean = np.mean(density_distribution[:,:stop_index], axis=0).reshape(1, stop_index)\n",
    "\n",
    "density_distribution_mean_tiled = np.tile(density_distribution_mean, (1, num_of_X_ensembles))\n",
    "density_distribution_mean_tiled = density_distribution_mean_tiled.reshape(num_of_X_ensembles, stop_index)\n",
    "\n",
    "density_distribution_normed = density_distribution[:, :stop_index] - density_distribution_mean_tiled\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.scatter(time_repeated_ensemble, density_distribution_normed.T.flatten())\n",
    "plt.ylabel(r'Distribution $(kg/km^3)$', fontsize=18)\n",
    "plt.xlabel(time_str, fontsize=18)\n",
    "plt.ylim(-.5e-3, .5e-3)\n",
    "plt.title('Density Ensemble Distribution (normed by mean)', fontsize=18)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#calculate the normed density distribution by subtracting the truth of the ensemble from each ensemble   \n",
    "#density_distribution shape = num_of_ensembles x stop_index\n",
    "density_distribution_truth = density_MSIS_array[:stop_index].reshape(1, stop_index)\n",
    "density_distribution_normed = density_distribution[:, :stop_index] - \\\n",
    "                    np.tile(density_distribution_truth, (num_of_ensembles, 1))\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.scatter(time_repeated_ensemble, density_distribution_normed.T.flatten())\n",
    "plt.ylabel(r'Distribution $(kg/km^3)$', fontsize=18)\n",
    "plt.xlabel(time_str, fontsize=18)\n",
    "plt.ylim(-1e-2, 1e-2)\n",
    "plt.title('Density Ensemble Distribution (normed by truth/MSIS)', fontsize=18)\n",
    "plt.show()\n",
    "#\"\"\"\n",
    "\n",
    "#calculate the normed X pos distribution by subtracting the mean of the ensemble from each ensemble   \n",
    "#X_distribution shape = 6 x num_of_X_ensembles x stop_index\n",
    "X_distribution_Xpos = X_distribution[0,:,:stop_index]\n",
    "X_distribution_Xpos_mean = np.mean(X_distribution_Xpos, axis=0).reshape(1, stop_index)\n",
    "X_distribution_Xpos_mean_tiled = np.tile(X_distribution_Xpos_mean, (1, num_of_X_ensembles))\n",
    "X_distribution_Xpos_mean_tiled = X_distribution_Xpos_mean_tiled.reshape(num_of_X_ensembles, stop_index)\n",
    "X_distribution_Xpos_diff = X_distribution_Xpos - X_distribution_Xpos_mean_tiled\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.scatter(time_repeated_X, X_distribution_Xpos_diff.T.flatten())\n",
    "plt.ylabel(r'Distribution (km)', fontsize=18)\n",
    "plt.xlabel(time_str, fontsize=18)\n",
    "#plt.ylim(-1e-4,1e-4)\n",
    "plt.title('State X Position Ensemble Distribution (normed by mean)', fontsize=18)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#calculate the normed X pos distribution by subtracting the truth of the ensemble from each ensemble   \n",
    "#X_distribution shape = 6 x num_of_X_ensembles x stop_index\n",
    "X_distribution_Xpos_truth = truth_xyz[:stop_index, 0].reshape(1, stop_index)\n",
    "X_distribution_Xpos_truth_normed = X_distribution_Xpos - np.tile(X_distribution_Xpos_truth, (num_of_ensembles, 1))\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.scatter(time_repeated_ensemble, X_distribution_Xpos_truth_normed.T.flatten())\n",
    "plt.ylabel(r'Distribution (km)', fontsize=18)\n",
    "plt.xlabel(time_str, fontsize=18)\n",
    "#plt.ylim(-1e-4, 1e-4)\n",
    "plt.title('State X Position Ensemble Distribution (normed by truth)', fontsize=18)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "times = measurement_array[:stop_index, 0]/(60)\n",
    "\n",
    "time_repeated_ensemble = np.repeat(times[:stop_index], num_of_ensembles)\n",
    "time_repeated_X = np.repeat(times[:stop_index], num_of_X_ensembles)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig_lat_meas = plt.figure()\n",
    "plt.plot(times, np.degrees(lat_lst_meas_array[:stop_index, 0]), 'g')\n",
    "plt.ylabel('Degrees', fontsize=18)\n",
    "plt.xlabel(time_str, fontsize=18)\n",
    "plt.title('Meas Gen Latitude', fontsize=18)\n",
    "plt.show()\n",
    "\n",
    "fig_lat = plt.figure()\n",
    "plt.plot(times, np.degrees(lat_lst_array[0,:stop_index]), 'g')\n",
    "plt.ylabel('Degrees', fontsize=18)\n",
    "plt.xlabel(time_str, fontsize=18)\n",
    "plt.title('Latitude', fontsize=18)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "fig_LST_meas = plt.figure()\n",
    "plt.plot(times, lat_lst_meas_array[:stop_index, 1], 'm')\n",
    "plt.ylabel('Hours', fontsize=18)\n",
    "plt.xlabel(time_str, fontsize=18)\n",
    "plt.title('Meas Gen LST', fontsize=18)\n",
    "plt.show()\n",
    "\n",
    "lst_rad = lat_lst_array[1,:stop_index]\n",
    "\n",
    "lst_hour = (lst_rad/np.radians(360)) * 24\n",
    "\n",
    "\n",
    "fig_LST = plt.figure()\n",
    "plt.plot(times, lst_hour, 'm')\n",
    "plt.ylabel('Hours', fontsize=18)\n",
    "plt.xlabel(time_str, fontsize=18)\n",
    "plt.title('LST', fontsize=18)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig_density_comparison = plt.figure()\n",
    "plt.plot(times, density_MSIS_array[:stop_index], 'g')\n",
    "plt.plot(times, true_density_array[:stop_index], 'm')\n",
    "#plt.ylim([-1e-11,1e-11])\n",
    "plt.ylabel(r'$kg/km^3$', fontsize=18)\n",
    "plt.xlabel(time_str, fontsize=18)\n",
    "plt.title('MSIS/True Density v. Meas Gen Density', fontsize=18)\n",
    "legend_names = ['MSIS', 'Meas Truth']\n",
    "plt.legend(legend_names, fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig_density_comparison = plt.figure()\n",
    "plt.plot(times, density_MSIS_array[:stop_index], 'g')\n",
    "plt.plot(times, est_density_array[:stop_index], 'm')\n",
    "#plt.ylim([-1e-11,1e-11])\n",
    "plt.ylabel(r'$kg/km^3$', fontsize=18)\n",
    "plt.xlabel(time_str, fontsize=18)\n",
    "plt.title('Estimated v. MSIS/True Density', fontsize=18)\n",
    "legend_names = ['MSIS', 'Estimated']\n",
    "plt.legend(legend_names, fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig_density_comparison_meas = plt.figure()\n",
    "plt.plot(times, true_density_array[:stop_index], 'g')\n",
    "plt.plot(times, est_density_array[:stop_index], 'm')\n",
    "#plt.ylim([-1e-11,1e-11])\n",
    "plt.ylabel(r'$kg/km^3$', fontsize=18)\n",
    "plt.xlabel(time_str, fontsize=18)\n",
    "plt.title('Estimated v. Meas Gen True Density', fontsize=18)\n",
    "legend_names = ['Meas', 'Estimated']\n",
    "plt.legend(legend_names, fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "perc_error = 100 * np.absolute(est_density_array[:stop_index] - true_density_array[:stop_index])/true_density_array[:stop_index]\n",
    "\n",
    "fig_percent_meas = plt.figure()\n",
    "plt.plot(times, perc_error[:stop_index], 'g')\n",
    "#plt.ylim([-1e-11,1e-11])\n",
    "plt.ylabel('Percent', fontsize=18)\n",
    "plt.xlabel(time_str, fontsize=18)\n",
    "plt.title('Percent Error of Estimated Density (w/ meas gen truth)', fontsize=18)\n",
    "legend_names = ['MSIS', 'Estimated']\n",
    "#plt.legend(legend_names, fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "perc_error = 100 * np.absolute(est_density_array[:stop_index] - density_MSIS_array[:stop_index])/density_MSIS_array[:stop_index]\n",
    "\n",
    "fig_percent = plt.figure()\n",
    "plt.plot(times, perc_error[:stop_index], 'g')\n",
    "#plt.ylim([-1e-11,1e-11])\n",
    "plt.ylabel('Percent', fontsize=18)\n",
    "plt.xlabel(time_str, fontsize=18)\n",
    "plt.title('Percent Error of Estimated Density', fontsize=18)\n",
    "legend_names = ['MSIS', 'Estimated']\n",
    "#plt.legend(legend_names, fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig_msis = plt.figure()\n",
    "plt.plot(times, density_MSIS_array[:stop_index], 'g')\n",
    "plt.ylabel(r'$kg/km^3$', fontsize=18)\n",
    "plt.xlabel(time_str, fontsize=18)\n",
    "plt.title('MSIS/True Density', fontsize=18)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "mean = np.mean(est_density_array[:stop_index])\n",
    "est_density_array_normalized = est_density_array[:stop_index] - mean\n",
    "\n",
    "fig_est = plt.figure()\n",
    "plt.plot(times, est_density_array_normalized[:stop_index], 'm')\n",
    "plt.ylabel(r'$kg/km^3$', fontsize=18)\n",
    "plt.xlabel(time_str, fontsize=18)\n",
    "plt.title('Estimated Density', fontsize=18)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#\"\"\"\n",
    "fig_lat.savefig('Figures/latitude.png')\n",
    "fig_LST.savefig('Figures/LST.png')\n",
    "fig_density_comparison.savefig('Figures/density_comparison.png')\n",
    "fig_percent.savefig('Figures/percent.png')\n",
    "fig_msis.savefig('Figures/msis.png')\n",
    "fig_est.savefig('Figures/est_density.png')\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Rank Histograms\n",
    "\n",
    "\n",
    "#Density\n",
    "\n",
    "rank_array = np.zeros((stop_index, 1))\n",
    "\n",
    "for ii in range(stop_index):\n",
    "    \n",
    "    new_array = np.append(density_distribution[:, ii], density_MSIS_array[ii])\n",
    "    \n",
    "    rank = scipy.stats.rankdata(new_array, method='min')[-1]\n",
    "    \n",
    "    rank_array[ii] = rank\n",
    "    \n",
    "indices = np.where(rank_array != 450)[0]\n",
    "indices1 = np.where(rank_array == 450)[0]\n",
    "print(times[indices1])\n",
    "\n",
    "\n",
    "fig_rank_hist = plt.figure()\n",
    "plt.hist(rank_array, bins=451)\n",
    "plt.ylabel('Frequency', fontsize=18)\n",
    "plt.xlabel('Rank', fontsize=18)\n",
    "plt.title('Density Ensemble Rank Histogram', fontsize=18)\n",
    "plt.xlim([0,451])\n",
    "plt.show()\n",
    "\n",
    "#fig_rank_hist.savefig('Figures/rank_hist.png')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#Generate Plots for Analysis\n",
    "\n",
    "x_range = .005\n",
    "y_range = .05\n",
    "z_range = .05\n",
    "\n",
    "xv_range = 1e-4\n",
    "yv_range = 1e-4\n",
    "zv_range = 1e-4\n",
    "\n",
    "times = measurement_array[:, 0]\n",
    "\n",
    "\n",
    "calc_display_results(post_fit_list_EnKF, measurement_array, R, meas_type, stop_index)\n",
    "\n",
    "\n",
    "plot_error_covar_xref(P_list_EnKF, X_mean_updated_list_EnKF, \\\n",
    "                      truth_xyz, density_MSIS_array, x_range, y_range, z_range, xv_range, yv_range, zv_range,\\\n",
    "                      measurement_array, times, stop_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_display_results(post_fit_list, measurement_array, R, meas_type, stop_index):\n",
    "    \n",
    "    \n",
    "    rms_1 = 'Range ='\n",
    "    unit_1 = 'km'\n",
    "    ylabel_1 = 'Range Residuals (km)'\n",
    "    title_1 = 'Range Post-fit Residuals'\n",
    "    save_fig_1 = 'postfit_range.png'\n",
    "    rms_2 = 'Range Rate ='\n",
    "    unit_2 = 'km/s'\n",
    "    ylabel_2 = 'Range Rate Residuals (km/s)'\n",
    "    title_2 = 'Range Rate Post-fit Residuals'\n",
    "    save_fig_2 = 'postfit_rangeRate.png'\n",
    "    post_fit_list_new = copy.deepcopy(post_fit_list)\n",
    "         \n",
    "    if (meas_type == 2) or (meas_type == 3):\n",
    "        rms_1 = 'Azimuth ='\n",
    "        unit_1 = 'degrees'\n",
    "        rms_2 = 'Elevation ='\n",
    "        unit_2 = 'degrees'\n",
    "        post_fit_list_new[:, 0] = np.degrees(post_fit_list[:, 0])\n",
    "        post_fit_list_new[:, 1] = np.degrees(post_fit_list[:, 1])\n",
    "        ylabel_1 = 'Azimuth Residuals (degrees)'\n",
    "        title_1 = 'Azimuth Post-fit Residuals'\n",
    "        save_fig_1 = 'postfit_az.png'\n",
    "        ylabel_2 = 'Elevation Residuals (degrees)'\n",
    "        title_2 = 'Elevation Post-fit Residuals'\n",
    "        save_fig_2 = 'postfit_el_rate.png'\n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "    times = measurement_array[:stop_index,0]/(60)\n",
    "    \n",
    "    indices_1 = np.where(measurement_array[:stop_index, 1] == 1)[0]\n",
    "    indices_2 = np.where(measurement_array[:stop_index, 1] == 2)[0]\n",
    "    indices_3 = np.where(measurement_array[:stop_index, 1] == 3)[0]\n",
    "\n",
    "    \n",
    "    \n",
    "    #Post-fit\n",
    "    print('Post-fit RMS:')\n",
    "    post_fit_1_list_4RMS = post_fit_list_new[:, 0]\n",
    "    postfit_1_rms = np.sqrt(np.mean(np.square(post_fit_1_list_4RMS)))\n",
    "    print(rms_1, postfit_1_rms, unit_1)\n",
    "\n",
    "    post_fit_2_list_4RMS = post_fit_list_new[:, 1]\n",
    "    postfit_2_rms = np.sqrt(np.mean(np.square(post_fit_2_list_4RMS)))\n",
    "    print(rms_2, postfit_2_rms, unit_2)\n",
    "    \n",
    "    if meas_type == 3:\n",
    "        post_fit_3_list_4RMS = post_fit_list_new[:, 2]\n",
    "        postfit_3_rms = np.sqrt(np.mean(np.square(post_fit_3_list_4RMS)))\n",
    "        print('Range =', postfit_3_rms, 'km')\n",
    "    \n",
    "    \n",
    "    covar_env_upper1 = np.ones((stop_index)) * np.degrees(np.sqrt(abs(R[0, 0])))*3\n",
    "    covar_env_upper2 = np.ones((stop_index)) * np.degrees(np.sqrt(abs(R[1, 1])))*3\n",
    "\n",
    "   \n",
    "    #Post-fit Residuals\n",
    "    fig_postFit_az = plt.figure()\n",
    "    plt.plot(times, covar_env_upper1, label='_nolegend_', c='g')\n",
    "    plt.plot(times, -covar_env_upper1, label='_nolegend_', c='g')\n",
    "    plt.scatter(times[indices_1], post_fit_list_new[indices_1, 0], s=70, c='m', marker='4')\n",
    "    plt.scatter(times[indices_2], post_fit_list_new[indices_2, 0], s=70, c='b', marker='+')\n",
    "    #plt.scatter(times[indices_3], post_fit_list_new[indices_3, 0], s=70, c='m', marker='4')\n",
    "    #plt.scatter(times, post_fit_list[:, 0])\n",
    "    plt.ylabel(ylabel_1, fontsize=18)\n",
    "    plt.xlabel(time_str, fontsize=18)\n",
    "    plt.title(title_1, fontsize=18)\n",
    "    legend_names = ['Station 1', 'Station 2']\n",
    "    plt.legend(legend_names, fontsize=16)\n",
    "    plt.xlim([0,times[-1]])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    #fig.savefig(save_fig_1)\n",
    "\n",
    "    fig_postFit_el = plt.figure()\n",
    "    plt.plot(times, covar_env_upper2, label='_nolegend_', c='g')\n",
    "    plt.plot(times, -covar_env_upper2, label='_nolegend_', c='g')\n",
    "    plt.scatter(times[indices_1], post_fit_list_new[indices_1, 1], s=70, c='m', marker='4')\n",
    "    plt.scatter(times[indices_2], post_fit_list_new[indices_2, 1], s=70, c='b', marker='+')\n",
    "    #plt.scatter(times[indices_3], post_fit_list_new[indices_3, 1], s=70, c='m', marker='4')\n",
    "    #plt.scatter(times, post_fit_list[:, 1])\n",
    "    plt.ylabel(ylabel_2, fontsize=18)\n",
    "    plt.xlabel(time_str, fontsize=18)\n",
    "    plt.title(title_2, fontsize=18)\n",
    "    plt.legend(legend_names, fontsize=16)\n",
    "    #plt.ylim([-.01,.01])\n",
    "    plt.xlim([0,times[-1]])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    #fig.savefig(save_fig_2)\n",
    "    \n",
    "    if meas_type == 3:\n",
    "        \n",
    "        covar_env_upper3 = np.ones((stop_index)) * np.sqrt(abs(R[2, 2]))*3\n",
    "        \n",
    "        fig_postFit_range = plt.figure()\n",
    "        plt.plot(times, covar_env_upper3, label='_nolegend_', c='g')\n",
    "        plt.plot(times, -covar_env_upper3, label='_nolegend_', c='g')\n",
    "        plt.scatter(times[indices_1], post_fit_list_new[indices_1, 2], s=70, c='m', marker='4')\n",
    "        plt.scatter(times[indices_2], post_fit_list_new[indices_2, 2], s=70, c='b', marker='+')\n",
    "        #plt.scatter(times[indices_3], post_fit_list_new[indices_3, 2], s=70, c='m', marker='4')\n",
    "        #plt.scatter(times, post_fit_list[:, 1])\n",
    "        plt.ylabel('Range Residuals (km)', fontsize=18)\n",
    "        plt.xlabel(time_str, fontsize=18)\n",
    "        plt.title('Range Post-fit Residuals', fontsize=18)\n",
    "        legend_names = ['Station 1', 'Station 2', 'Station 3']\n",
    "        plt.legend(legend_names, fontsize=16)\n",
    "        #plt.ylim([-.01,.01])\n",
    "        plt.xlim([0,times[-1]])\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        #fig.savefig('postfit_range.png')\n",
    "    \n",
    "    \n",
    "    fig_postFit_az.savefig('Figures/postFit_az.png')\n",
    "    fig_postFit_el.savefig('Figures/postFit_el.png')\n",
    "    fig_postFit_range.savefig('Figures/postFit_range.png')\n",
    "    \n",
    "    \n",
    "def plot_error_covar_xref(P_list, x_ref_updated_list, obs_data_truth, density_truth, x_range, \\\n",
    "                          y_range, z_range, xv_range, yv_range, zv_range, measurement_array, time, stop_index):\n",
    "    \n",
    "    #Compare to the Truth Data : Estimation Errors------\n",
    "\n",
    "    times = time[:stop_index]/(60)\n",
    "    \n",
    "    indices_1 = np.where(measurement_array[:stop_index, 1] == 1)[0]\n",
    "    indices_2 = np.where(measurement_array[:stop_index, 1] == 2)[0]\n",
    "    indices_3 = np.where(measurement_array[:stop_index, 1] == 3)[0]\n",
    "    \n",
    "    density_covar_env_upper = np.sqrt(abs(P_list[:stop_index, -1, -1]))*3\n",
    "    density_covar_env_lower = -density_covar_env_upper\n",
    "    density_error = x_ref_updated_list[:stop_index,-1] - density_truth[:stop_index]\n",
    "    \n",
    "    x_covar_env_upper = np.sqrt(abs(P_list[:stop_index, 0, 0]))*3\n",
    "    x_covar_env_lower = -x_covar_env_upper\n",
    "    x_error = x_ref_updated_list[:stop_index,0] - obs_data_truth[:stop_index, 0]\n",
    "    \n",
    "    y_covar_env_upper = np.sqrt(abs(P_list[:stop_index, 1, 1]))*3\n",
    "    y_covar_env_lower = -y_covar_env_upper\n",
    "    y_error = x_ref_updated_list[:stop_index,1] - obs_data_truth[:stop_index, 1]\n",
    "    \n",
    "    z_covar_env_upper = np.sqrt(abs(P_list[:stop_index, 2, 2]))*3\n",
    "    z_covar_env_lower = -z_covar_env_upper\n",
    "    z_error = x_ref_updated_list[:stop_index,2] - obs_data_truth[:stop_index, 2]\n",
    "    \n",
    "    #error_pos_norm = np.sqrt(x_error**2 + y_error**2 + z_error**2)\n",
    "    error_density_rms_3D = np.sqrt(np.mean(np.square(density_error)))\n",
    "    print('Density RMS =', error_density_rms_3D, r'$(kg/km^3)$')\n",
    "    \n",
    "    print('Position RMS:')\n",
    "    error_x_pos_rms_3D = np.sqrt(np.mean(np.square(x_error)))\n",
    "    print('X =', error_x_pos_rms_3D, 'km')\n",
    "    \n",
    "    error_y_pos_rms_3D = np.sqrt(np.mean(np.square(y_error)))\n",
    "    print('Y =', error_y_pos_rms_3D, 'km')\n",
    "    \n",
    "    error_z_pos_rms_3D = np.sqrt(np.mean(np.square(z_error)))\n",
    "    print('Z =', error_z_pos_rms_3D, 'km')\n",
    "    \n",
    "    pos_rms = np.sqrt(error_x_pos_rms_3D**2 + error_y_pos_rms_3D**2 + error_z_pos_rms_3D**2)\n",
    "    print('Overall =', pos_rms, 'km')\n",
    "    \n",
    "    \n",
    "    #Density\n",
    "    fig_dens = plt.figure()\n",
    "    plt.plot(times, density_covar_env_upper, label='_nolegend_', c='g')\n",
    "    plt.plot(times, density_covar_env_lower, label='_nolegend_', c='g')\n",
    "    plt.scatter(times[indices_1], density_error[indices_1], s=70, c='m', marker='4')\n",
    "    plt.scatter(times[indices_2], density_error[indices_2], s=70, c='b', marker='+')\n",
    "    #plt.scatter(times[indices_3], density_error[indices_3], s=70, c='m', marker='4')\n",
    "    #plt.scatter(times, x_error)\n",
    "    plt.ylabel(r'$(kg/km^3)$', fontsize=18)\n",
    "    plt.xlabel(time_str, fontsize=18)\n",
    "    legend_names = ['Station 1', 'Station 2']\n",
    "    plt.legend(legend_names, fontsize=16)\n",
    "    plt.title('EnKF Density Error & Covariance Envelope', fontsize=18)\n",
    "    #plt.ylim([-x_range,x_range])\n",
    "    plt.xlim([0,times[-1]])\n",
    "    plt.show()\n",
    "    #fig.savefig('x_pos_error.png')\n",
    "    \n",
    "\n",
    "    #x Position\n",
    "    fig_xpos = plt.figure()\n",
    "    plt.plot(times, x_covar_env_upper, label='_nolegend_', c='g')\n",
    "    plt.plot(times, x_covar_env_lower, label='_nolegend_', c='g')\n",
    "    plt.scatter(times[indices_1], x_error[indices_1], s=70, c='m', marker='4')\n",
    "    plt.scatter(times[indices_2], x_error[indices_2], s=70, c='b', marker='+')\n",
    "    #plt.scatter(times[indices_3], x_error[indices_3], s=70, c='m', marker='4')\n",
    "    #plt.scatter(times, x_error)\n",
    "    plt.ylabel('km', fontsize=18)\n",
    "    plt.xlabel(time_str, fontsize=18)\n",
    "    legend_names = ['Station 1', 'Station 2', 'Station 3']\n",
    "    plt.legend(legend_names, fontsize=16)\n",
    "    plt.title('EnKF X Position Error & Covariance Envelope', fontsize=18)\n",
    "    plt.ylim([-x_range,x_range])\n",
    "    plt.xlim([0,times[-1]])\n",
    "    plt.show()\n",
    "    #fig.savefig('x_pos_error.png')\n",
    "\n",
    "    #y Position \n",
    "    fig_ypos = plt.figure()\n",
    "    plt.plot(times, y_covar_env_upper, label='_nolegend_', c='g')\n",
    "    plt.plot(times, y_covar_env_lower, label='_nolegend_', c='g')\n",
    "    plt.scatter(times[indices_1], y_error[indices_1], s=70, c='m', marker='4')#c='k', marker='x'\n",
    "    plt.scatter(times[indices_2], y_error[indices_2], s=70, c='b', marker='+')\n",
    "    #plt.scatter(times[indices_3], y_error[indices_3], s=70, c='m', marker='4')\n",
    "    #plt.scatter(times, y_error)\n",
    "    plt.ylabel('km', fontsize=18)\n",
    "    plt.xlabel(time_str, fontsize=18)\n",
    "    plt.legend(legend_names, fontsize=16)\n",
    "    plt.title('EnKF Y Position Error & Covariance Envelope', fontsize=18)\n",
    "    plt.ylim([-y_range,y_range])\n",
    "    plt.xlim([0,times[-1]])\n",
    "    plt.show()\n",
    "    #fig.savefig('y_pos_error.png')\n",
    "\n",
    "    #z Position\n",
    "    fig_zpos = plt.figure()\n",
    "    plt.plot(times, z_covar_env_upper, label='_nolegend_', c='g')\n",
    "    plt.plot(times, z_covar_env_lower, label='_nolegend_', c='g')\n",
    "    plt.scatter(times[indices_1], z_error[indices_1], s=70, c='m', marker='4')\n",
    "    plt.scatter(times[indices_2], z_error[indices_2], s=70, c='b', marker='+')\n",
    "    #plt.scatter(times[indices_3], z_error[indices_3], s=70, c='m', marker='4')\n",
    "    #plt.scatter(times, z_error)\n",
    "    plt.ylabel('km', fontsize=18)\n",
    "    plt.xlabel(time_str, fontsize=18)\n",
    "    plt.legend(legend_names, fontsize=16)\n",
    "    plt.title('EnKF Z Position Error & Covariance Envelope', fontsize=18)\n",
    "    plt.ylim([-z_range,z_range])\n",
    "    plt.xlim([0,times[-1]])\n",
    "    plt.show()\n",
    "    #fig.savefig('z_pos_error.png')\n",
    "    \n",
    "    #x Velocity\n",
    "    x_dot_covar_env_upper = np.sqrt(abs(P_list[:stop_index, 3, 3]))*3\n",
    "    x_dot_covar_env_lower = -np.sqrt(abs(P_list[:stop_index, 3, 3]))*3\n",
    "    x_vel_error = x_ref_updated_list[:stop_index,3] - obs_data_truth[:stop_index, 3]\n",
    "\n",
    "    fig_xvel = plt.figure()\n",
    "    plt.plot(times, x_dot_covar_env_upper, label='_nolegend_', c='g')\n",
    "    plt.plot(times, x_dot_covar_env_lower, label='_nolegend_', c='g')\n",
    "    plt.scatter(times[indices_1], x_vel_error[indices_1], s=70, c='m', marker='4')\n",
    "    plt.scatter(times[indices_2], x_vel_error[indices_2], s=70, c='b', marker='+')\n",
    "    #plt.scatter(times[indices_3], x_vel_error[indices_3], s=70, c='m', marker='4')\n",
    "    #plt.scatter(times, x_vel_error)\n",
    "    plt.ylabel('km/second', fontsize=18)\n",
    "    plt.xlabel(time_str, fontsize=18)\n",
    "    plt.legend(legend_names, fontsize=16)\n",
    "    plt.title('EnKF X Velocity Error & Covariance Envelope', fontsize=18)\n",
    "    plt.ylim([-xv_range,xv_range])\n",
    "    plt.xlim([0,times[-1]])\n",
    "    plt.show()\n",
    "    #fig.savefig('x_vel_error.png')\n",
    "\n",
    "    #y Velocity\n",
    "    y_dot_covar_env_upper = np.sqrt(abs(P_list[:stop_index, 4, 4]))*3\n",
    "    y_dot_covar_env_lower = -np.sqrt(abs(P_list[:stop_index, 4, 4]))*3\n",
    "    y_vel_error = x_ref_updated_list[:stop_index,4] - obs_data_truth[:stop_index, 4]\n",
    "\n",
    "    fig_yvel = plt.figure()\n",
    "    plt.plot(times, y_dot_covar_env_upper, label='_nolegend_', c='g')\n",
    "    plt.plot(times, y_dot_covar_env_lower, label='_nolegend_', c='g')\n",
    "    plt.scatter(times[indices_1], y_vel_error[indices_1], s=70, c='m', marker='4')\n",
    "    plt.scatter(times[indices_2], y_vel_error[indices_2], s=70, c='b', marker='+')\n",
    "    #plt.scatter(times[indices_3], y_vel_error[indices_3], s=70, c='m', marker='4')\n",
    "    #plt.scatter(times, y_vel_error)\n",
    "    plt.ylabel('km/second', fontsize=18)\n",
    "    plt.xlabel(time_str, fontsize=18)\n",
    "    plt.legend(legend_names, fontsize=16)\n",
    "    plt.title('EnKF Y Velocity Error & Covariance Envelope', fontsize=18)\n",
    "    plt.ylim([-yv_range,yv_range])\n",
    "    plt.xlim([0,times[-1]])\n",
    "    plt.show()\n",
    "    #fig.savefig('y_vel_error.png')\n",
    "\n",
    "    #z Velocity\n",
    "    z_dot_covar_env_upper = np.sqrt(abs(P_list[:stop_index, 5, 5]))*3\n",
    "    z_dot_covar_env_lower = -np.sqrt(abs(P_list[:stop_index, 5, 5]))*3\n",
    "    z_vel_error = x_ref_updated_list[:stop_index,5] - obs_data_truth[:stop_index, 5]\n",
    "\n",
    "    fig_zvel = plt.figure()\n",
    "    plt.plot(times, z_dot_covar_env_upper, label='_nolegend_', c='g')\n",
    "    plt.plot(times, z_dot_covar_env_lower, label='_nolegend_', c='g')\n",
    "    plt.scatter(times[indices_1], z_vel_error[indices_1], s=70, c='m', marker='4')\n",
    "    plt.scatter(times[indices_2], z_vel_error[indices_2], s=70, c='b', marker='+')\n",
    "    #plt.scatter(times[indices_3], z_vel_error[indices_3], s=70, c='m', marker='4')\n",
    "    #plt.scatter(times, z_vel_error)\n",
    "    plt.ylabel('km/second', fontsize=18)\n",
    "    plt.xlabel(time_str, fontsize=18)\n",
    "    plt.legend(legend_names, fontsize=16)\n",
    "    plt.title('EnKF Z Velocity Error & Covariance Envelope', fontsize=18)\n",
    "    plt.ylim([-zv_range,zv_range])\n",
    "    plt.xlim([0,times[-1]])\n",
    "    plt.show()\n",
    "    #fig.savefig('z_vel_error.png')\n",
    "    \n",
    "    \n",
    "    print('Velocity RMS:')\n",
    "    error_x_vel_rms_3D = np.sqrt(np.mean(np.square(x_vel_error)))\n",
    "    print('X =', error_x_vel_rms_3D, 'km/second')\n",
    "    \n",
    "    error_y_vel_rms_3D = np.sqrt(np.mean(np.square(y_vel_error)))\n",
    "    print('Y =', error_y_vel_rms_3D, 'km/second')\n",
    "    \n",
    "    error_z_vel_rms_3D = np.sqrt(np.mean(np.square(z_vel_error)))\n",
    "    print('Z =', error_z_vel_rms_3D, 'km/second')\n",
    "    \n",
    "    vel_rms = np.sqrt(error_x_vel_rms_3D**2 + error_y_vel_rms_3D**2 + error_z_vel_rms_3D**2)\n",
    "    print('Overall =', vel_rms, 'km/s')\n",
    "\n",
    "    \n",
    "    #\"\"\"\n",
    "    fig_dens.savefig('Figures/dens_error.png')\n",
    "    fig_xpos.savefig('Figures/x_pos_error.png')\n",
    "    fig_ypos.savefig('Figures/y_pos_error.png')\n",
    "    fig_zpos.savefig('Figures/z_pos_error.png')\n",
    "    fig_xvel.savefig('Figures/x_vel_error.png')\n",
    "    fig_yvel.savefig('Figures/y_vel_error.png')\n",
    "    fig_zvel.savefig('Figures/z_vel_error.png')\n",
    "    #\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
